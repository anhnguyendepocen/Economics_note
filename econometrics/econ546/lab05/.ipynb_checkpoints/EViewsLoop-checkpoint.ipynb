{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as spst\n",
    "from scipy.optimize import leastsq\n",
    "import pandas as pd\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mizon57= pd.read_csv(\"Mizon57.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mizon57[\"L\"]= (mizon57[\"LF\"]-mizon57[\"U\"])*mizon57[\"H\"]/100\n",
    "mizon57[\"L100\"]= (mizon57[\"LF\"]-mizon57[\"U\"])*mizon57[\"H\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mizon57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_ces(params, X_data,Y_data):\n",
    "    \"\"\"only error output for least squared\n",
    "    Q = \\gamma[\\delta K^{-\\rho} +(1-\\delta) L^{-\\rho}  ]^{-\\frac{\\nu}{\\rho}\n",
    "    error = Q_est - Q\n",
    "    \"\"\"\n",
    "    Q = Y_data[:,0]\n",
    "    K = X_data[:,0]\n",
    "    L = X_data[:,1]    \n",
    "    gamma = params[0]\n",
    "    delta = params[1]\n",
    "    rho = params[2]\n",
    "    nu = params[3]\n",
    "    Q_est =  gamma*(delta* K**(-rho) + (1-delta)* L**(-rho))**(-nu/rho)\n",
    "    return  np.array(Q - Q_est)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## http://www.scipy-lectures.org/intro/summary-exercises/optimize-fit.html\n",
    "\n",
    "p0 = np.array([0.1, 0.1, -0.1, 1.0])\n",
    "\n",
    "estimates = leastsq(error_ces, p0, \n",
    "                    args = ( np.array(mizon57.ix[:,[\"K\",\"L\"]]), \n",
    "                            np.array(mizon57.ix[:,[\"Q\"]]) ), full_output=1)\n",
    "\n",
    "estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.array([0.1, 0.1, -0.1, 1.0])\n",
    "\n",
    "## alternative using minimization routine.\n",
    "def sse_ces(params, X_data,Y_data):\n",
    "    \"\"\"Sum of squared error\n",
    "    Q = \\gamma[\\delta K^{-\\rho} +(1-\\delta) L^{-\\rho}  ]^{-\\frac{\\nu}{\\rho}\n",
    "    error = Q_est - Q\n",
    "    sse = sum(error**2)\n",
    "    \"\"\"\n",
    "    return np.sum(error_ces(params, X_data,Y_data)**2)\n",
    "\n",
    "res = optimize.minimize(sse_ces, p0, method='L-BFGS-B', \n",
    "                        args=( np.array(mizon57.ix[:,[\"K\",\"L\"]]), \n",
    "                              np.array(mizon57.ix[:,[\"Q\"]]) ))\n",
    "print( res.x)\n",
    "print( res.nfev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## curve fit NOT working\n",
    "# p0 = np.array([0.1, 0.1, -0.1, 1.0])\n",
    "\n",
    "# def ces(X_data, params):\n",
    "#     \"\"\"ces\n",
    "#     Q = \\gamma[\\delta K^{-\\rho} +(1-\\delta) L^{-\\rho}  ]^{-\\frac{\\nu}{\\rho}\n",
    "#     error = Q_est - Q\n",
    "#     \"\"\"    \n",
    "#     K = X_data[:,0]\n",
    "#     L = X_data[:,1]    \n",
    "#     gamma = params[0]\n",
    "#     delta = params[1]\n",
    "#     rho = params[2]\n",
    "#     nu = params[3]        \n",
    "#     return gamma*(delta* K**(-rho) + (1-delta)* L**(-rho))**(-nu/rho)\n",
    "\n",
    "# popt, pcov = optimize.curve_fit(ces,\n",
    "#                                 np.array(mizon57.ix[:,[\"K\",\"L\"]]),\n",
    "#                                 np.array(mizon57.ix[:,[\"Q\"]]),\n",
    "#                                 p0=p0\n",
    "#                                )\n",
    "# print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "func = sse_ces\n",
    "name = \"CES production function\" \n",
    "                 \n",
    "#p0= {\"gamma\": 0.04746368, \"delta\":0.43092672,\"rho\": -0.34408368, \"nu\": 1.020762290}\n",
    "#p0= {\"gamma\": 0.18, \"delta\":0.1,\"rho\": -0.1, \"nu\": 1.01}\n",
    "\n",
    "\n",
    "\n",
    "xdata= np.array(mizon57.ix[:,[\"K\",\"L100\"]])\n",
    "ydata= np.array(mizon57.ix[:,[\"Q\"]])\n",
    "inits = list(p0.values()) \n",
    "\n",
    "#https://stackoverflow.com/questions/9219094/order-of-values-when-extracting-python-dict-to-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [\"gamma\", \"delta\",\"rho\", \"nu\"]\n",
    "#p0 =np.array([0.04746368, 0.43092672,-0.34408368,  1.020762290])\n",
    "p0 =np.array([0.1, 0.1,-0.1,  1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "mod1 = leastsq(sse_ces, p0, args = (xdata, ydata), full_output=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate output report like R  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters\n",
    "parmEsts = np.round( mod1[0], 4 )\n",
    "\n",
    "# Get the Error variance and standard deviation\n",
    "RSS = np.sum( mod1[2]['fvec']**2 )\n",
    "df = nobs - nparm\n",
    "MSE = RSS / df\n",
    "RMSE = np.sqrt( MSE )\n",
    "\n",
    "# Get the covariance matrix\n",
    "cov = MSE * mod1[1]\n",
    "\n",
    "# Get parameter standard errors\n",
    "parmSE = np.diag( np.sqrt( cov ) )\n",
    "\n",
    "# Calculate the t-values\n",
    "tvals = parmEsts/parmSE\n",
    "\n",
    "# Get p-values\n",
    "pvals = (1 - spst.t.cdf( np.abs(tvals), df))*2\n",
    "\n",
    "# Get biased variance (MLE) and calculate log-likehood\n",
    "s2b = RSS / nobs\n",
    "logLik = -nobs/2 * np.log(2*np.pi) - nobs/2 * np.log(s2b) - 1/(2*s2b) * RSS\n",
    "\n",
    "#del(mod1)\n",
    "#del(s2b)\n",
    "#del(inits)\n",
    "\n",
    "# Get AIC. Add 1 to the df to account for estimation of standard error\n",
    "def AIC(self, k=2):\n",
    "    return -2*logLik + k*(nparm + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary\n",
    "def summary():\n",
    "    print()\n",
    "    print( 'Non-linear least squares')\n",
    "    print( 'Model: ' + name)\n",
    "    print( 'Parameters:')\n",
    "    print( \" Estimate Std. Error t-value P(>|t|)\")\n",
    "    for i in range( len(parmNames) ):\n",
    "            print( \"% -5s % 5.4f % 5.4f % 5.4f % 5.4f\" % tuple( [parmNames[i], \n",
    "                                                                parmEsts[i], parmSE[i], tvals[i], pvals[i]] ))\n",
    "    print()\n",
    "    print( 'Residual Standard Error: % 5.4f' % RMSE)\n",
    "    print( 'Df: %i' % df)\n",
    "summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very sensitive for initial value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified to fit python3.5\n",
    "# # #  https://www.r-bloggers.com/r-vs-python-practical-data-analysis-nonlinear-regression/\n",
    "\n",
    "\n",
    "class NLS:\n",
    "    ''' This provides a wrapper for scipy.optimize.leastsq to get the relevant output for nonlinear least squares.\n",
    "    Although scipy provides curve_fit for that reason, curve_fit only returns parameter estimates and covariances. \n",
    "    This wrapper returns numerous statistics and diagnostics'''\n",
    " \n",
    "    import numpy as np\n",
    "    import scipy.stats as spst\n",
    "    from scipy.optimize import leastsq\n",
    " \n",
    "    def __init__(self,func,name, params ,p0, xdata, ydata):\n",
    "        \"\"\"\n",
    "        func: error function y^ - y\n",
    "        name: model name\n",
    "        params: names of parameters, list\n",
    "        p0: starting value for params(the same order), np.array\n",
    "        xdata: np.array \n",
    "        ydata: np.array\n",
    "        \n",
    "        \"\"\"\n",
    "        # Check the data     \n",
    "        \n",
    "        if len(xdata) != len(ydata):\n",
    "            msg = 'The number of observations does not match the number of rows for the predictors'\n",
    "            raise ValueError(msg)\n",
    " \n",
    "\n",
    "            \n",
    "        self.name = name \n",
    "        self.func = func\n",
    "\n",
    "        self.xdata = xdata\n",
    "        self.ydata = ydata\n",
    "        \n",
    "        self.nobs = len( ydata )\n",
    "\n",
    "        # Check parameter estimates\n",
    "#         if type(p0) != dict:\n",
    "#             msg = \"Initial parameter estimates (p0) must be a dictionry of form p0={'a':1, 'b':2, etc}\"\n",
    "#             raise ValueError(msg)\n",
    "        \n",
    "        self.inits = p0 \n",
    "        self.nparm= len( self.inits )\n",
    "        self.parmNames = params\n",
    "        \n",
    "        # clean parameters' names \n",
    "        for i in range( len(self.parmNames) ):\n",
    "            if len(self.parmNames[i]) > 5:\n",
    "                self.parmNames[i] = self.parmNames[i][0:4]\n",
    " \n",
    "        # Run the model\n",
    "        self.mod1 = leastsq(self.func, np.array(self.inits), args = (self.xdata, self.ydata), full_output=1)\n",
    " \n",
    "        # Get the parameters\n",
    "        self.parmEsts = np.round( self.mod1[0], 4 )\n",
    " \n",
    "        # Get the Error variance and standard deviation\n",
    "        self.RSS = np.sum( self.mod1[2]['fvec']**2 )\n",
    "        self.df = self.nobs - self.nparm\n",
    "        self.MSE = self.RSS / self.df\n",
    "        self.RMSE = np.sqrt( self.MSE )\n",
    " \n",
    "        # Get the covariance matrix\n",
    "        self.cov = self.MSE * self.mod1[1]\n",
    " \n",
    "        # Get parameter standard errors\n",
    "        self.parmSE = np.diag( np.sqrt( self.cov ) )\n",
    " \n",
    "        # Calculate the t-values\n",
    "        self.tvals = self.parmEsts/self.parmSE\n",
    " \n",
    "        # Get p-values\n",
    "        self.pvals = (1 - spst.t.cdf( np.abs(self.tvals), self.df))*2\n",
    " \n",
    "        # Get biased variance (MLE) and calculate log-likehood\n",
    "        self.s2b = self.RSS / self.nobs\n",
    "        self.logLik = -self.nobs/2 * np.log(2*np.pi) - self.nobs/2 * np.log(self.s2b) - 1/(2*self.s2b) * self.RSS\n",
    " \n",
    "        #del(self.mod1)\n",
    "        #del(self.s2b)\n",
    "        #del(self.inits)\n",
    " \n",
    "    # Get AIC. Add 1 to the df to account for estimation of standard error\n",
    "    def AIC(self, k=2):\n",
    "        return -2*self.logLik + k*(self.nparm + 1)\n",
    " \n",
    "    del(np)\n",
    "    del(leastsq)\n",
    " \n",
    "    # Print the summary\n",
    "    def summary(self):\n",
    "        print()\n",
    "        print( 'Non-linear least squares')\n",
    "        print( 'Model: ' + self.name)\n",
    "        print( 'Parameters:')\n",
    "        print( \" Estimate Std. Error t-value P(>|t|)\")\n",
    "        for i in range( len(self.parmNames) ):\n",
    "                print( \" % -5s % 5.4f % 5.4f % 5.4f % 5.4f\" % tuple( [self.parmNames[i], \n",
    "                                                                    self.parmEsts[i], self.parmSE[i], self.tvals[i], self.pvals[i]] ))\n",
    "        print()\n",
    "        print( 'Residual Standard Error: % 5.4f' % self.RMSE)\n",
    "        print( 'Df: %i' % self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\"gamma\", \"delta\",\"rho\", \"nu\"]\n",
    "#p0 =np.array([0.04746368, 0.43092672,-0.34408368,  1.020762290])\n",
    "p0 =np.array([0.1, 0.1,-0.1,  1.1])\n",
    "\n",
    "nls_mizon57 = NLS(func = sse_ces,name = \"CES production function\", params = params,\n",
    "                  p0=p0, xdata= np.array(mizon57.ix[:,[\"K\",\"L100\"]]), ydata= np.array(mizon57.ix[:,[\"Q\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nls_mizon57.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out there are many ways to do the NLS in Python since there are couples of way to do the minimization/optimization in scipy\n",
    "\n",
    "ref:\n",
    "### scipy.optimize.least_squares\n",
    "\n",
    "Solve a nonlinear least-squares problem with bounds on the variables.\n",
    "\n",
    "Given the residuals $f(x)$ (an m-dimensional real function of n real variables) and the loss function $rho(s)$ (a scalar function), least_squares finds a local minimum of the cost function $F(x)$:\n",
    "\n",
    "minimize $F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)$\n",
    "\n",
    "subject to $lb <= x <= ub$\n",
    "\n",
    "The purpose of the loss function $rho(s)$ is to reduce the influence of outliers on the solution.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy-0.19.1/reference/generated/scipy.optimize.least_squares.html\n",
    "\n",
    "### Optimization (scipy.optimize)\n",
    "\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html\n",
    "\n",
    "http://blog.mmast.net/least-squares-fitting-numpy-scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.poly1d([-5, 1, 3])\n",
    "x = np.linspace(0, 2, 20)\n",
    "y = f(x) + 1.5*np.random.normal(size=len(x))\n",
    "xn = np.linspace(0, 2, 200)\n",
    "\n",
    "plt.plot(x, y, 'or')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b, c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "def residual(p, x, y):\n",
    "    return y - f(x, *p)\n",
    "\n",
    "p0 = [1., 1., 1.]\n",
    "\n",
    "popt, pcov, info, mesg, ler = optimize.leastsq(residual, p0, \n",
    "                                               args=(x, y), full_output=True)\n",
    "# popt, pcov = optimize.leastsq(residual, p0, args=(x, y))\n",
    "\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should use non-linear least squares if the dimensionality of the output vector is larger than the number of parameters to optimize. Here, we can see the number of function evaluations of our last estimation of the coeffients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( info['nfev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yn = f(xn, *popt)\n",
    "\n",
    "plt.plot(x, y, 'or')\n",
    "plt.plot(xn, yn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using as a example, a L-BFGS minimization we will achieve the minimization in more cost function evaluations:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_residual(p, x, y):\n",
    "    return sum(residual(p, x, y)**2)\n",
    "\n",
    "res = optimize.minimize(min_residual, p0, method='L-BFGS-B', args=(x, y))\n",
    "print( res.x)\n",
    "print( res.nfev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = optimize.curve_fit(f, x, y, p0=p0)\n",
    "print( popt)\n",
    "\n",
    "plt.plot(x, y, 'or')\n",
    "plt.plot(xn, f(xn, *popt))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stnd    = input(r'P ($\\%$) and $\\theta$ of pol. standard? (as tuple)')\n",
    "# p       = stnd[0]/100.\n",
    "# ang     = np.radians(stnd[1])  \n",
    "# x,y     = sympy.symbols('x y')  \n",
    "# stndqu  = sympy.solve([sympy.sqrt(x**2+y**2)-p,(0.5*sympy.atan(y/x))-ang],[x,y])[1] \n",
    "\n",
    "# stndqun = np.array([sympy.N(i) for i in stndqu],dtype=float) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
