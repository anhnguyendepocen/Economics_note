{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of gradient descent and Newton's method\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Newton_optimization_vs_grad_descent.svg/440px-Newton_optimization_vs_grad_descent.svg.png)\n",
    "\n",
    "\n",
    "A comparison of gradient descent (green) and Newton's method (red) for minimizing a function (with small step sizes). \n",
    "\n",
    "Newton's method uses curvature information to take a more direct route.\n",
    "\n",
    "Newton's method is faster than gradient descent.\n",
    "\n",
    "----\n",
    "\n",
    "### Tow different Newton's Method\n",
    "- In calculus, Newton's method is an iterative method for finding the roots of a differentiable function $f$ (i.e. solutions to the equation $f(x)=0$). \n",
    "\n",
    "\n",
    "\n",
    "- In optimization, Newton's method is applied to the derivative $f ′$ of a twice-differentiable function $f$ to find the roots of the derivative (solutions to $f ′(x)=0$), also known as the stationary points of $f$.\n",
    "\n",
    "\n",
    "\n",
    "### Method\n",
    "\n",
    "In the one-dimensional problem, Newton's method to find the roots attempts to construct a sequence xn from an initial guess $x_0$ that converges towards some value $x^*$ satisfying $f ′(x^*)=0$. This $x^*$ is a **stationary point** of $f$.\n",
    "\n",
    "The second order Taylor expansion $f_T(x)$ of f around xn is:\n",
    "\n",
    "$${\\displaystyle f_{T}(x)=f_{T}(x_{n}+\\Delta x)\\approx f(x_{n})+f'(x_{n})\\Delta x+{\\frac {1}{2}}f''(x_{n})\\Delta x^{2}} .$$\n",
    "\n",
    "\n",
    "We want to find $Δx$ such that $x_n + Δx$ is a **stationary point**. We seek to solve the equation that sets the derivative of this last expression with respect to $Δx$ equal to zero:\n",
    "\n",
    "$${\\displaystyle \\displaystyle 0={\\frac {\\rm {d}}{\\rm {d\\Delta x}}}\\left(f(x_{n})+f'(x_{n})\\Delta x+{\\frac {1}{2}}f''(x_{n})\\Delta x^{2}\\right)=f'(x_{n})+f''(x_{n})\\Delta x} .$$\n",
    "\n",
    "\n",
    "For the value of $Δx = −f ′(x_n) / f ″(x_n)$, which is the solution of this equation, it can be hoped that \n",
    "\n",
    "$$x_{n+1} = x_n + Δx = x_n − f ′(x_n) / f ″(x_n)$$ \n",
    "\n",
    "will be closer to a **stationary point** $x^*$. \n",
    "\n",
    "Provided that $f$ is a **twice-differentiable** function and other technical conditions are satisfied, the sequence $x_1, x_2, …$ will converge to a point $x^*$ satisfying $f ′(x*) = 0$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Geometric interpretation\n",
    "\n",
    "The geometric interpretation of Newton's method is that at each iteration one approximates $f(x)$ by a quadratic function around $x_n$, and then takes a step towards the maximum/minimum of that quadratic function (in higher dimensions, this may also be a saddle point).\n",
    "\n",
    "Note that if $f(x)$ happens to be a quadratic function, then the exact extremum is found in one step.\n",
    "\n",
    "### Higher dimensions\n",
    "\n",
    "The above iterative scheme can be generalized to several dimensions by replacing the derivative with the gradient, $∇f(x$), and the reciprocal of the second derivative with the inverse of the Hessian matrix, $H f(x)$. One obtains the iterative scheme\n",
    "\n",
    "$${\\displaystyle \\mathbf {x} _{n+1}=\\mathbf {x} _{n}-[\\mathbf {H} f(\\mathbf {x} _{n})]^{-1}\\nabla f(\\mathbf {x} _{n}),\\ n\\geq 0.} .$$\n",
    "\n",
    "\n",
    "\n",
    "Often Newton's method is modified to include a small **step size** $\\alpha ∈ (0,1)$ instead of $\\alpha = 1$\n",
    "\n",
    "$${\\displaystyle \\mathbf {x} _{n+1}=\\mathbf {x} _{n}-\\alpha [\\mathbf {H} f(\\mathbf {x} _{n})]^{-1}\\nabla f(\\mathbf {x} _{n}).} .$$\n",
    "\n",
    "\n",
    "\n",
    "This is often done to ensure that the Wolfe conditions are satisfied at each step $x_n → x_{n+1}$ of the iteration. For **step sizes** other than 1, the method is often referred to as the relaxed Newton's method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Chinese lecture from Dr. Lu at UVic\n",
    "\n",
    "https://www.bilibili.com/video/av5400941/index_4.html?t=1764\n",
    "\n",
    "#### Newton's method\n",
    "\n",
    "start at 47min: SOTS\n",
    "\n",
    "\n",
    "50min: comparison of Gradient and Newton's\n",
    "\n",
    "\n",
    "\n",
    "$$\\delta = - I^{-1} g(x)$$\n",
    "\n",
    "Gradient descent is slow but simple/cheap and easy to compute. \n",
    "\n",
    "$$\\delta = - H^{-1} g(x)$$\n",
    "\n",
    "Newton's fast but expensivie and hard to compute.\n",
    "\n",
    "\n",
    "In an united way，\n",
    "\n",
    "$$\\delta = - S g(x)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$S= \\begin{cases}\n",
    "I,  & \\text{Gradien} \\\\\n",
    "H^{-1}, & \\text{Newton's}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "#### Fix Hessian with negative Eigenvalues\n",
    "https://www.bilibili.com/video/av5400941/index_4.html?t=1764#page=5\n",
    "\n",
    "20min: descent algorithm always descent\n",
    "\n",
    "Newton's method only descent if the angle of $-g$ and $-H^{-1}g$ is less than 90\n",
    "\n",
    "$$(-g^T)  (-H^{-1}  g)     >0$$\n",
    "\n",
    "$$g^T  H^{-1}  g  >0$$\n",
    "\n",
    "and $H^{-1} > 0$ or $H> 0$ \n",
    "\n",
    "\n",
    "$H$ is symmetric matix, so the Eigenvalues are real. \n",
    "\n",
    "If $min(eig(H)) > 0$, $H$ is positive definite.\n",
    "\n",
    "If one of the Eigenvalues is negative, we need to modify the $H$.\n",
    "\n",
    "$$\\hat H = H + min(eig(H)) I$$\n",
    "\n",
    "$$\\hat H = U \\Lambda U^T + \\lambda_n  U \\Lambda U^T $$\n",
    "\n",
    "where $\\Lambda$ is Eigenvalue matrix, and $U$ is the normalized/standardized Eigenvector matrix\n",
    "\n",
    "make sure $\\hat H>0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:02.873239Z",
     "start_time": "2017-11-05T05:10:00.214684Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%precision 4\n",
    "plt.style.use('ggplot')\n",
    "import sympy as sym\n",
    "sym.init_printing()\n",
    "x1,x2,alpha = sym.symbols('x_1 x_2 alpha ', positive = True)\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:02.877689Z",
     "start_time": "2017-11-05T05:10:02.874680Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sympy.plotting import plot_parametric\n",
    "from sympy.plotting import plot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:03.743389Z",
     "start_time": "2017-11-05T05:10:02.881700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sympy import series\n",
    "x, xbar = sym.symbols(\"x,xbar\")\n",
    "f = sym.Function(\"f\")\n",
    "\n",
    "sym.series(f(x), x, x0=xbar, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:04.133452Z",
     "start_time": "2017-11-05T05:10:03.744392Z"
    }
   },
   "outputs": [],
   "source": [
    "x = sym.Matrix([x1,x2])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:04.138463Z",
     "start_time": "2017-11-05T05:10:04.135456Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F=sym.Function(\"F\")(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:04.549877Z",
     "start_time": "2017-11-05T05:10:04.139969Z"
    }
   },
   "outputs": [],
   "source": [
    "F.diff(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:04.840607Z",
     "start_time": "2017-11-05T05:10:04.551380Z"
    }
   },
   "outputs": [],
   "source": [
    "g =sym.Matrix([F]).jacobian(x).T\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:05.136894Z",
     "start_time": "2017-11-05T05:10:04.842111Z"
    }
   },
   "outputs": [],
   "source": [
    "H = sym.hessian(F,x)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent\n",
    "\n",
    "\n",
    "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent\n",
    "\n",
    "https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/jiahao/1561144\n",
    "\n",
    "\n",
    "MatrixCalculus provides matrix calculus for everyone. It is an online tool that computes vector and matrix derivatives (matrix calculus).\n",
    "http://www.matrixcalculus.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton’s method(牛顿法)\n",
    "\n",
    "\n",
    "Similar to the gradient method, the algorithm of Newton’s method for optimization reads:\n",
    "\n",
    "1. Initialization: find a initial value of $x_0$ in the feasible set\n",
    "\n",
    "1. Convergence Test: calculate the gradient and Hessian matrix of current value $∇f(x)$, and know if it satisfies the convergent criteria.\n",
    "\n",
    "1. Update: if not converge, update$ x^{k+1}=x^k- \\alpha H^{-1} ∇f(x^k)$, where $\\alpha$ is a small number to control the step size and $H$ is the Hessian matrix.\n",
    "\n",
    "\n",
    "ref:\n",
    "\n",
    "http://www.scipy-lectures.org/advanced/mathematical_optimization/index.html#newton-and-quasi-newton-methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen decomposition for $H^{-1}$\n",
    "\n",
    "### Eigenvectors and Eigenvalues\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/0/06/Eigenvectors.gif)\n",
    "\n",
    "\n",
    "First recall that an eigenvector of a matrix A is a non-zero vector v such that\n",
    "\n",
    "$$Av=λv$$\n",
    "\n",
    "for some scalar $λ$\n",
    "\n",
    "The value $λ$ is called an eigenvalue of A.\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/ad/Eigenvectors-extended.gif)\n",
    "\n",
    "\n",
    "If an n×n matrix A has nn linearly independent eigenvectors, then A may be decomposed in the following manner:\n",
    "\n",
    "$$A=QΛQ^{-1}$$\n",
    "\n",
    "where $Λ$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$ and the columns of $Q$ are the corresponding eigenvectors of A.\n",
    "\n",
    "### Calculating Eigenvalues\n",
    "\n",
    "It is easy to see from the definition that if v is an eigenvector of an n×n matrix A with eigenvalue $λ$, then\n",
    "\n",
    "$$Av−λI=0$$\n",
    "\n",
    "where I is the identity matrix of dimension nn and 0 is an n-dimensional zero vector. Therefore, the eigenvalues of A satisfy:\n",
    "\n",
    "$$det(A−λI)=0$$\n",
    "\n",
    "The left-hand side above is a polynomial in $λ$, and is called the characteristic polynomial of A. Thus, to find the eigenvalues of A, we find the roots of the characteristic polynomial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:17.289750Z",
     "start_time": "2017-11-05T05:10:16.991943Z"
    }
   },
   "outputs": [],
   "source": [
    "f = 2*x1**2- 2*x1*x2 +x2**2 +2*x1-2*x2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:18.307122Z",
     "start_time": "2017-11-05T05:10:17.933381Z"
    }
   },
   "outputs": [],
   "source": [
    "g =sym.Matrix([f]).jacobian(x).T\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:23.421065Z",
     "start_time": "2017-11-05T05:10:23.034028Z"
    }
   },
   "outputs": [],
   "source": [
    "g.jacobian(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:10:24.891947Z",
     "start_time": "2017-11-05T05:10:24.610198Z"
    }
   },
   "outputs": [],
   "source": [
    "H = sym.hessian(f,x)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:01:56.302662Z",
     "start_time": "2017-11-04T23:01:55.904617Z"
    }
   },
   "outputs": [],
   "source": [
    "#If all you want is the characteristic polynomial, use charpoly. This is more efficient than eigenvals, because sometimes symbolic roots can be expensive to calculate.\n",
    "lamda = sym.symbols('lamda')\n",
    "p = H.charpoly(lamda)\n",
    "sym.factor(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:02:18.138697Z",
     "start_time": "2017-11-04T23:02:17.707065Z"
    }
   },
   "outputs": [],
   "source": [
    "sym.solve(sym.factor(p), lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Eighenvector and Eigenvalue\n",
    "\n",
    "http://setosa.io/ev/eigenvectors-and-eigenvalues/\n",
    "\n",
    "https://www.youtube.com/watch?v=PFDu9oVAE-g&t=704s\n",
    "\n",
    "https://www.youtube.com/watch?v=ue3yoeZvt8E&t=137s\n",
    "\n",
    "https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors\n",
    "\n",
    "\n",
    "### Find the Eigenvalues and Eigen vectors\n",
    "\n",
    "https://www.youtube.com/watch?v=IdsV0RaC9jM\n",
    "\n",
    "http://people.duke.edu/~ccc14/sta-663-2016/07_LinearAlgebra2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:05:50.890615Z",
     "start_time": "2017-11-04T23:05:50.845006Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:09:49.519308Z",
     "start_time": "2017-11-04T23:09:49.484717Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert sympy matrix to np array\n",
    "# If you skip the astype method, numpy will create a matrix of type 'object', which won't work with common array operations.\n",
    "w, v = LA.eig(np.array(H).astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:09:51.238540Z",
     "start_time": "2017-11-04T23:09:51.235043Z"
    }
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:09:52.270529Z",
     "start_time": "2017-11-04T23:09:52.266520Z"
    }
   },
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:39:24.855663Z",
     "start_time": "2017-11-04T22:39:24.106900Z"
    }
   },
   "outputs": [],
   "source": [
    "H.eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:21:50.538363Z",
     "start_time": "2017-11-04T22:21:50.155299Z"
    }
   },
   "outputs": [],
   "source": [
    "H.eigenvects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:59:16.922952Z",
     "start_time": "2017-11-04T22:59:16.890883Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P, D = H.diagonalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:59:18.296878Z",
     "start_time": "2017-11-04T22:59:18.015549Z"
    }
   },
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:59:20.111211Z",
     "start_time": "2017-11-04T22:59:19.831969Z"
    }
   },
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:59:21.411435Z",
     "start_time": "2017-11-04T22:59:21.032019Z"
    }
   },
   "outputs": [],
   "source": [
    "P.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:57:53.135667Z",
     "start_time": "2017-11-04T22:57:52.737368Z"
    }
   },
   "outputs": [],
   "source": [
    "P.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T22:59:06.132377Z",
     "start_time": "2017-11-04T22:59:05.770917Z"
    }
   },
   "outputs": [],
   "source": [
    "P**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:00:14.796461Z",
     "start_time": "2017-11-04T23:00:14.792451Z"
    }
   },
   "outputs": [],
   "source": [
    "P*D*P**-1 == H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def func(x):\n",
    "    return 100*np.square(np.square(x[0])-x[1])+np.square(x[0]-1)\n",
    "\n",
    "# first order derivatives of the function\n",
    "def dfunc(x):\n",
    "    df1 = 400*x[0]*(np.square(x[0])-x[1])+2*(x[0]-1)\n",
    "    df2 = -200*(np.square(x[0])-x[1])\n",
    "    return np.array([df1, df2])\n",
    "\n",
    "def invhess(x):\n",
    "    df11 = 1200*np.square(x[0])-400*x[1]+2\n",
    "    df12 = -400*x[0]\n",
    "    df21 = -400*x[0]\n",
    "    df22 = 200\n",
    "    hess = np.array([[df11, df12], [df21, df22]])\n",
    "    return inv(hess)\n",
    "\n",
    "def newton(x, max_int):\n",
    "    miter = 1\n",
    "    step = .5\n",
    "    vals = []\n",
    "    objectfs = []\n",
    "    # you can customize your own condition of convergence, here we limit the number of iterations\n",
    "    while miter <= max_int:\n",
    "        vals.append(x)\n",
    "        objectfs.append(func(x))\n",
    "        temp = x-step*(invhess(x).dot(dfunc(x)))\n",
    "        if np.abs(func(temp)-func(x))>0.01:\n",
    "            x = temp\n",
    "        else:\n",
    "            break\n",
    "        print(x, func(x), miter)\n",
    "        miter += 1\n",
    "    return vals, objectfs, miter\n",
    "\n",
    "start = [5, 5]\n",
    "val, objectf, iters = newton(start, 50)\n",
    "\n",
    "x = np.array([i[0] for i in val])\n",
    "y = np.array([i[1] for i in val])\n",
    "z = np.array(objectf)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(x, y, z, label='newton method')\n",
    "#plt.savefig('./res/newton.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Line serach for $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:57:05.288382Z",
     "start_time": "2017-11-04T23:57:04.650527Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.optimize\n",
    "def test_func(x):\n",
    "    return (x[0])**2+(x[1])**2\n",
    "\n",
    "def test_grad(x):\n",
    "    return [2*x[0],2*x[1]]\n",
    "\n",
    "import numpy as np\n",
    "sp.optimize.line_search(test_func,test_grad,np.array([1.8,1.7]),np.array([-1.,-1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton methods is efficient and fast, but if it is expensive to calculate the Hessian. \n",
    "\n",
    "And if the Eigenvalues of the Hessian are negative, we need to modify the Hessian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x, y = np.mgrid[-1:1:31j, -1:1:31j]\n",
    "z = x**2 - y**2\n",
    "ax.plot_surface(x, y, z, **{'rstride': 1, 'cstride': 1, 'cmap': \"Greens_r\"})\n",
    "ax.plot([0], [0], [0], 'ro')\n",
    "ax.view_init(azim=-60, elev=20)\n",
    "plt.xticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "ax.set_zticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Quasi-Newton methods\n",
    "\n",
    "In quasi-Newton methods Hessian matrix is approximated, so finite difference scheme used in the above task already gives a quasi-Newton method. However, there are serious problems with Hessian matrices that require to tweak them in a special way. First, Hessian may be singular, so no inverse H−1 may exist. Another problem is that Hessian may have negative curvature along some directions.\n",
    "\n",
    "Instead of descending like gradient descent would do, Newton methods may start approaching the saddle point. This happens because one of the eigenvalues is negative and there is negative curvature around the point. It is possible to switch to gradient descent in such situations.\n",
    "\n",
    "All eigenvalues of a symmetric positive-definite matrix are positive.\n",
    "\n",
    "\n",
    "####  BFGS\n",
    "BFGS (Broyden–Fletcher–Goldfarb–Shanno) is one of the classical methods that changes the Hessian matrix to fix the problem with singularities and saddle points while still maintaining some meaningful curvature information to move along valleys. The general idea is to approximate the inverse of Hessian with something like $(H+pI)^{−1}$ and find a way to avoid recomputing the inverse every time as it is expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Chinese lecture from Dr. Lu at UVic\n",
    "\n",
    "https://www.bilibili.com/video/av5400941/index_4.html?t=1764#page=5\n",
    "\n",
    "#### Quasi Newton's method\n",
    "\n",
    "start at 35min: \n",
    "\n",
    "using first order gradient to approximate the second order Hession.\n",
    "\n",
    "39min: DFP or BFGS\n",
    "\n",
    "$$\\delta_k = - S{-1}_k g(x)_k$$\n",
    "\n",
    "\n",
    "$$S{-1}_k  $$\n",
    "\n",
    "\n",
    "$S$ keeps the good properties such as symmetric, postitive definite, approximate the $H^{-1}$\n",
    "\n",
    "\n",
    "ref:\n",
    "\n",
    "http://aria42.com/blog/2014/12/understanding-lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conjugate gradient descent\n",
    "\n",
    "The gradient descent algorithms above are toys not to be used on real problems.\n",
    "\n",
    "As can be seen from the above experiments, one of the problems of the simple gradient descent algorithms, is that it tends to oscillate across a valley, each time following the direction of the gradient, that makes it cross the valley. \n",
    "\n",
    "The conjugate gradient solves this problem by adding a friction term: **each step depends on the two last values of the gradient and sharp turns are reduced**.\n",
    "\n",
    "ref:\n",
    "http://www.scipy-lectures.org/advanced/mathematical_optimization/index.html#conjugate-gradient-descent\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Newton-Conjugate Gradient algorithm is a modified Newton’s method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian [NW]. Newton’s method is based on fitting the function locally to a quadratic form:\n",
    "\n",
    "\n",
    "\n",
    "$$f(x)≈f(x_0)+∇f(x_0)⋅(x−x_0)+\\frac{1}{2}(x−x_0)^T H(x_0)(x−x_0).$$\n",
    "\n",
    "where $H(x_0$) is a matrix of second-derivatives (the Hessian). If the Hessian is positive definite then the local minimum of this function can be found by setting the gradient of the quadratic form to zero, resulting in\n",
    "\n",
    "$$x_{opt}= x0- H^{-1}∇f.$$\n",
    "\n",
    "The inverse of the Hessian is evaluated using the conjugate-gradient method. \n",
    "\n",
    "To take full advantage of the Newton-CG method, a function which computes the Hessian must be provided. The Hessian matrix itself does not need to be constructed, only a vector which is the product of the Hessian with an arbitrary vector needs to be available to the minimization routine.\n",
    "\n",
    "As a result, the user can provide either a function to compute the Hessian matrix, or a function to compute the product of the Hessian with an arbitrary vector.\n",
    "\n",
    "\n",
    "\n",
    "ref:\n",
    "https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition.\n",
    "\n",
    "We say that two non-zero vectors u and v are conjugate (with respect to A) if\n",
    "\n",
    "${\\displaystyle \\mathbf {u} ^{\\mathsf {T}}\\mathbf {A} \\mathbf {v} =0.} \\mathbf {u} ^{\\mathsf {T}}\\mathbf {A} \\mathbf {v} =0$\n",
    ".\n",
    "Since A is symmetric and positive definite, the left-hand side defines an inner product\n",
    "\n",
    "ref:\n",
    "https://en.wikipedia.org/wiki/Conjugate_gradient_method\n",
    "\n",
    "\n",
    "\n",
    "scipy provides scipy.optimize.minimize() to find the minimum of scalar functions of one or more variables. The simple conjugate gradient method can be used by setting the parameter method to CG\n",
    "\n",
    "Here, CG refers to the fact that an internal inversion of the Hessian is performed by conjugate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:57:20.886508Z",
     "start_time": "2017-11-04T23:57:20.853923Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):   # The rosenbrock function\n",
    "     return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "optimize.minimize(f, [2, -1], method=\"CG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient methods need the Jacobian (gradient) of the function. They can compute it numerically, but will perform better if you can pass them the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:58:19.909451Z",
     "start_time": "2017-11-04T23:58:19.902936Z"
    }
   },
   "outputs": [],
   "source": [
    "def jacobian(x):\n",
    "     return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "optimize.minimize(f, [2, 1], method=\"CG\", jac=jacobian)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that compared to a conjugate gradient (above), Newton’s method has required less function evaluations, but more gradient evaluations, as it uses it to approximate the Hessian. Let’s compute the Hessian and pass it to the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T23:59:51.943157Z",
     "start_time": "2017-11-04T23:59:51.925097Z"
    }
   },
   "outputs": [],
   "source": [
    "def hessian(x): # Computed with sympy\n",
    "     return np.array(((1 - 4*x[1] + 12*x[0]**2, -4*x[0]), (-4*x[0], 2)))\n",
    "optimize.minimize(f, [2,-1], method=\"Newton-CG\", jac=jacobian, hess=hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "```Matlab\n",
    "% Program: conj_gradient_linesearch.m\n",
    "% Title: Conjugate-gradient algorithm \n",
    "% Authors:  A. Antoniou, W.-S. Lu, A. Mehlenbacher, University of Victoria\n",
    "% Description: Implements the conjugate-gradient\n",
    "% Input:\n",
    "%   fname: objective function\n",
    "%   gname: gradient of the objective function\n",
    "%   x0: initial point\n",
    "%   epsi: termination tolerance\n",
    "% Output:\n",
    "%   xbar: solution point\n",
    "%   fbar: objective function evaluated at xbar.\n",
    "%   k: number of iterations at convergence\n",
    "% Example:\n",
    "%   [xbar,fbar,k] = conj_gradient_linesearch('f_quartic','g_quartic',[1 1]',1e-4)\n",
    "%   [xbar,fbar,k] = conj_gradient_linesearch('f_himm','g_himm',[5 5]',1e-4)\n",
    "% ==========================================================\n",
    "function [xbar,fbar,k]=conj_gradient_linesearch(fname,gname,x0,epsi)\n",
    "disp(' ')\n",
    "disp('Program conj_gradient_linesearch.m')\n",
    "%Initialize\n",
    "k = 0;\n",
    "xk = x0;\n",
    "xkdisp = ['k = ',num2str(k),'; xk = [',num2str(xk'),'];'];\n",
    "disp(xkdisp)\n",
    "gk = feval(gname,xk);\n",
    "dk = -gk;\n",
    "g2 = gk'*gk;\n",
    "ak = inex_lsearch_econ350(xk,dk,fname,gname); \n",
    "adk = ak*dk;\n",
    "err = norm(adk);\n",
    "%While length of descent vector > epsi\n",
    "while  err >= epsi\n",
    "    k = k + 1;\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %1:  New min point x\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   xk = xk + adk;\n",
    "   xkdisp = ['k = ',num2str(k),'; xk = [',num2str(xk'),'];'];\n",
    "   disp(xkdisp)\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %2:  Gradient \n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   gk = feval(gname,xk);\n",
    "   g2_new = gk'*gk;\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %3:  Beta and d\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   bk = g2_new/g2;\n",
    "   dk = -gk + bk*dk;\n",
    "   g2 = g2_new; \n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      \n",
    "   %4: Alpha using Line Search\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   ak = inex_lsearch_econ350(xk,dk,fname,gname); \n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %5:  Current error\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   adk = ak*dk;\n",
    "   err = norm(adk);\n",
    "end\n",
    "xbar = xk + adk;\n",
    "fbar = feval(fname,xbar);\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```Matlab\n",
    "% Program: conj_gradient_hessian.m\n",
    "% Title: Conjugate-gradient algorithm \n",
    "% Authors:  A. Antoniou, W.-S. Lu, A. Mehlenbacher, University of Victoria\n",
    "% Description: Implements the conjugate-gradient\n",
    "% Input:\n",
    "%   fname: objective function\n",
    "%   gname: gradient of the objective function\n",
    "%   hname: Hessian of the objective function\n",
    "%   x0: initial point\n",
    "%   epsi: termination tolerance\n",
    "% Output:\n",
    "%   xbar: solution point\n",
    "%   fbar: objective function evaluated at xbar.\n",
    "%   k: number of iterations at convergence\n",
    "% Example:\n",
    "%   [xbar,fbar,k] = conj_gradient_hessian('f_quartic','g_quartic','h_quartic',[1 1]',1e-4)\n",
    "%   [xbar,fbar,k] = conj_gradient_hessian('f_himm','g_himm','h_himm',[5 5]',1e-4)\n",
    "% ==========================================================\n",
    "function [xbar,fbar,k]=conj_gradient_hessian(fname,gname,hname,x0,epsi)\n",
    "disp(' ')\n",
    "disp('Program conj_gradient_econ350.m')\n",
    "%Initialize\n",
    "k = 0;\n",
    "xk = x0;\n",
    "xkdisp = ['k = ',num2str(k),'; xk = [',num2str(xk'),'];'];\n",
    "disp(xkdisp)\n",
    "gk = feval(gname,xk);\n",
    "Hk = feval(hname,xk);\n",
    "dk = -gk;\n",
    "g2 = gk'*gk;\n",
    "ak = g2/(dk'*Hk*dk);\n",
    "adk = ak*dk;\n",
    "err = norm(adk);\n",
    "%While length of descent vector > epsi\n",
    "while  err >= epsi\n",
    "   k = k + 1;\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %1:  New min point x\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   xk = xk + adk;\n",
    "   xkdisp = ['k = ',num2str(k),'; xk = [',num2str(xk'),'];'];\n",
    "   disp(xkdisp)\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %2:  Gradient and Hessian\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   gk = feval(gname,xk);\n",
    "   g2_new = gk'*gk;\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %3:  Beta and d\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   bk = g2_new/g2;\n",
    "   dk = -gk + bk*dk;\n",
    "   g2 = g2_new;\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %4: Alpha using Hessian\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   Hk = feval(hname,xk);\n",
    "   ak = g2/(dk'*Hk*dk);\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %5:  Current error\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   adk = ak*dk;\n",
    "   err = norm(adk);\n",
    "end\n",
    "xbar = xk + adk;\n",
    "fbar = feval(fname,xbar);\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_himmelblau(x):\n",
    "    x1= x[1]\n",
    "    x2 = x[2]\n",
    "    return (x1**2 + x2-11)**2 + (x1 + x2**2 -7)**2\n",
    "def g_himmelblau(x):\n",
    "    \"\"\" output g is a column vector\"\"\"\n",
    "    x1,x2= x\n",
    "    w1 = x1**2 + x2 -11\n",
    "    w2 = x1 + x2**2 -7\n",
    "    g1 = 4*w1 *x1 + 2*w2\n",
    "    g2 = 2*w1 + 4*w2*x2\n",
    "    return np.array([g1,g2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T06:29:20.745985Z",
     "start_time": "2017-11-05T06:29:20.406583Z"
    }
   },
   "outputs": [],
   "source": [
    "f = (x1**2 + x2-11)**2 + (x1 + x2**2 -7)**2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T06:29:23.915528Z",
     "start_time": "2017-11-05T06:29:23.526835Z"
    }
   },
   "outputs": [],
   "source": [
    "g =sym.Matrix([f]).jacobian(x).T\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T06:29:26.852096Z",
     "start_time": "2017-11-05T06:29:26.462547Z"
    }
   },
   "outputs": [],
   "source": [
    "g.jacobian(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T06:30:42.878018Z",
     "start_time": "2017-11-05T06:30:42.579256Z"
    }
   },
   "outputs": [],
   "source": [
    "H = sym.hessian(f,x)\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the gradient does not have close form, we can use numerical method to get an approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_himmelblau_num(x):\n",
    "    n = len()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''\n",
    "Pure Python implementation of some numerical optimizers\n",
    "Created on Jan 21, 2011\n",
    "@author Jiahao Chen\n",
    "'''\n",
    "\n",
    "from scipy import dot, eye, finfo, asarray_chkfinite, sqrt, \\\n",
    "    outer, zeros, isinf, arccos\n",
    "from scipy.linalg import inv, norm, pinv, solve, LinAlgError\n",
    "from copy import deepcopy\n",
    "\n",
    "_epsilon = sqrt(finfo(float).eps)\n",
    "\n",
    "#################\n",
    "# Line searches #\n",
    "#################\n",
    "def MinpackLineSearch(f, df, x, p, df_x = None, f_x = None,\n",
    "                args = (), c1 = 1e-4, c2 = 0.4, amax = 50, xtol = _epsilon):\n",
    "    \"Modified shamelessly from scipy.optimize\"\n",
    "\n",
    "    if df_x == None: df_x = df(x)\n",
    "    if f_x == None: f_x = f(x)\n",
    "    from scipy.optimize import line_search\n",
    "    stp, _, _, _, _, _ = line_search(f, df, x, p, df_x, f_x, f_x, args, c1, c2, amax)\n",
    "    if stp == None: #Fall back to backtracking linesearch\n",
    "        return BacktrackingLineSearch(f, df, x, p, df_x, f_x, args)\n",
    "    else:\n",
    "        return stp\n",
    "\n",
    "\n",
    "\n",
    "def MoreThuenteLinesearchIterate(delta, alpha_l, alpha_u, alpha_t, alpha_min,\n",
    "            alpha_max, f_l, g_l, f_t, g_t, f_u, g_u, isBracketed, Verbose):\n",
    "    #Section 4 of More and Thuente: Pick new trial alpha\n",
    "    if Verbose: print( 'Section 4 of More and Thuente')\n",
    "    assert not (isBracketed and not min(alpha_l, alpha_u) < alpha_t < max(alpha_l, alpha_u)), 'Bracketing fail'\n",
    "    #if g_t * (alpha_t - alpha_l) < 0.0:\n",
    "    #    if Verbose: print 'Going uphill; reverting to last good point'\n",
    "    #    return isBracketed, alpha_l, alpha_l, alpha_t\n",
    "\n",
    "    if alpha_min == alpha_max:\n",
    "        print( 'Squeezed!')\n",
    "        return isBracketed, alpha_min, alpha_min, alpha_min\n",
    "\n",
    "    assert alpha_min < alpha_max\n",
    "\n",
    "    sgnd = dot(g_t, g_l / abs(g_l))\n",
    "    isBounded = False\n",
    "    if f_t > f_l:\n",
    "        if Verbose:\n",
    "            print (\"\"\"Case 1: a higher function value. \\\n",
    "The minimum is bracketed by [a_l, a_t]. If the cubic step is closer to \\\n",
    "lower bound then take it, else take average of quadratic and cubic steps.\"\"\")\n",
    "        #Calculate cubic and quadratic steps\n",
    "        da = alpha_t - alpha_l\n",
    "        d1 = g_l + g_t + 3 * (f_l - f_t) / da\n",
    "        s = max(g_l, g_t, d1)\n",
    "        d2 = s * ((d1 / s) ** 2 - (g_l / s * g_t / s)) ** 0.5\n",
    "        if alpha_t < alpha_l:\n",
    "            d2 = -d2\n",
    "        cubp = d2 - g_l + d1\n",
    "        cubq = ((d2 - g_l) + d2) + g_t\n",
    "        if abs(cubq) > _epsilon:\n",
    "            r = cubp / cubq\n",
    "            #Minimizer of cubic that interpolates f_l, f_t, g_l, g_t\n",
    "            alpha_c = alpha_l + r * da\n",
    "            #Minimizer of quadratic that interpolates f_l, f_t, g_l\n",
    "            alpha_q = alpha_l + ((g_l / ((f_l - f_t) / da + g_l)) * 0.5 * da)\n",
    "        else:\n",
    "            #print 'WARNING: VERY SMALL INTERVAL'\n",
    "            alpha_c = alpha_q = 0.5 * (alpha_l + alpha_t) #Invalid, choose midpoint\n",
    "        #Calculate new trial\n",
    "        if Verbose:\n",
    "            print( 'Interpolating guesses:', alpha_c, alpha_q)\n",
    "        if abs(alpha_c - alpha_l) < abs(alpha_q - alpha_l):\n",
    "            alpha_tt = alpha_c\n",
    "        else:\n",
    "            alpha_tt = 0.5 * (alpha_c + alpha_q)\n",
    "        isBracketed = True\n",
    "        isBounded = True\n",
    "\n",
    "    elif sgnd < 0: #f_t <= f_l assumed by here\n",
    "        if Verbose:\n",
    "            print( \"\"\"Case 2: lower function value and derivatives are of opposite sign. \\\n",
    "The minimum is bracketed by [a_l, a_t]. If the cubic step is closer to lower \\\n",
    "bound then take it, else take quadratic step.\"\"\")\n",
    "        #Calculate cubic and quadratic steps\n",
    "        da = alpha_t - alpha_l\n",
    "        d1 = g_l + g_t + 3 * (f_l - f_t) / da\n",
    "        s = max(g_l, g_t, d1)\n",
    "        d2 = s * ((d1 / s) ** 2 - (g_l / s * g_t / s)) ** 0.5\n",
    "        if alpha_t < alpha_l:\n",
    "            d2 = -d2\n",
    "        cubp = d2 - g_t + d1\n",
    "        cubq = ((d2 - g_t) + d2) + g_l\n",
    "        if abs(cubq) > _epsilon:\n",
    "            r = cubp / cubq #Minimizer of cubic that interpolates f_l, f_t, g_l, g_t\n",
    "            alpha_c = alpha_t - r * da #Minimizer of quadratic that interpolates f_l, f_t, g_l\n",
    "            alpha_s = alpha_t - (g_l / (g_l - g_t)) * da\n",
    "        else:\n",
    "            #print 'WARNING: VERY SMALL INTERVAL'\n",
    "            alpha_s = alpha_c = 0.5 * (alpha_l + alpha_t) #Invalid, choose midpoint\n",
    "        #Calculate new trial\n",
    "        if Verbose:\n",
    "            print( 'Interpolating guesses:', alpha_c, alpha_s)\n",
    "        if abs(alpha_c - alpha_t) >= abs(alpha_s - alpha_t):\n",
    "            alpha_tt = alpha_c\n",
    "        else:\n",
    "            alpha_tt = alpha_s\n",
    "        isBracketed = True\n",
    "        isBounded = False\n",
    "    elif norm(g_t) <= norm(g_l): #g_t * g_l >= 0 and f_t <= f_l assumed by here\n",
    "        if Verbose:\n",
    "            print( \"\"\"Case 3: lower function value and derivatives are of same \\\n",
    "sign, and magnitude of derivative decreased.\\n Cubic step used only if it tends \\\n",
    "to infinity in direction of step or if minimum of cubic lies beyond step. \\\n",
    "Otherwise cubic step is defined to be either stpmin or stpmax. Quadratic step is \\\n",
    "taken if it is closer to alpha_t, else take the farthest step.\"\"\")\n",
    "        #Calculate cubic and quadratic steps\n",
    "        da = alpha_t - alpha_l\n",
    "        d1 = g_l + g_t + 3 * (f_l - f_t) / da\n",
    "        s = max(g_l, g_t, d1)\n",
    "        d2 = s * ((d1 / s) ** 2 - (g_l / s * g_t / s)) ** 0.5\n",
    "        if alpha_t < alpha_l:\n",
    "            d2 = -d2\n",
    "\n",
    "        cubp = d2 - g_t + d1\n",
    "        cubq = ((d2 - g_t) + d2) + g_l\n",
    "        r = cubp / cubq\n",
    "        #The case d2 == 0 arises only if cubic does not tend to infinity in\n",
    "        #direction of step\n",
    "        if (r < 0. and abs(d2) < _epsilon):\n",
    "            alpha_c = alpha_t - r * da\n",
    "        elif alpha_t > alpha_l:\n",
    "            alpha_c = alpha_max\n",
    "        else:\n",
    "            alpha_c = alpha_min\n",
    "\n",
    "        #Minimizer of quadratic that interpolates f_l, f_t, g_l\n",
    "        alpha_s = -alpha_t - (g_l / (g_l - g_t)) * da\n",
    "\n",
    "        if isBracketed:\n",
    "            if abs(alpha_c - alpha_t) < abs(alpha_s - alpha_t):\n",
    "                alpha_tt = alpha_c\n",
    "            else:\n",
    "                alpha_tt = alpha_s\n",
    "        else:\n",
    "            if abs(alpha_c - alpha_t) > abs(alpha_s - alpha_t):\n",
    "                alpha_tt = alpha_c\n",
    "            else:\n",
    "                alpha_tt = alpha_s\n",
    "    else:\n",
    "        if Verbose:\n",
    "            print (\"\"\"Case 4: lower function value and derivatives \\\n",
    "are of same sign, and magnitude of derivative did not decrease.\n",
    "The minimum is NOT NECESSARILY bracketed. \\\n",
    "If bracketed, take cubic step, otherwise take  alpha_min or alpha_max.\"\"\")\n",
    "        #Minimizer of cubic that interpolates f_u, f_t, g_u, g_t\n",
    "        #Formula from Nocedal and Wright 2/e, Eq. 3.59, p 59\n",
    "        if isBracketed:\n",
    "            da = alpha_t - alpha_u\n",
    "            d1 = g_u + g_t + 3 * (f_u - f_t) / da\n",
    "            s = max(g_l, g_t, d1)\n",
    "            d2 = s * ((d1 / s) ** 2 - (g_l / s * g_t / s)) ** 0.5\n",
    "            if alpha_t < alpha_u:\n",
    "                d2 = -d2\n",
    "            cubp = d2 - g_t + d1\n",
    "            cubq = ((d2 - g_t) + d2) + g_u\n",
    "            if abs(cubq) > _epsilon:\n",
    "                r = cubp / cubq\n",
    "                alpha_tt = alpha_t - r * da\n",
    "            else:\n",
    "                #print 'WARNING: VERY SMALL INTERVAL'\n",
    "                alpha_tt = 0.5 * (alpha_l + alpha_t) #Invalid, choose midpoint\n",
    "        else:\n",
    "            if alpha_t > alpha_l:\n",
    "                alpha_tt = alpha_max\n",
    "            else:\n",
    "                alpha_tt = alpha_min\n",
    "\n",
    "        isBounded = False #abs(g_t) > abs(g_l)\n",
    "\n",
    "    if Verbose: print( 'Now proceeding to update')\n",
    "\n",
    "    #Update interval of uncertainty\n",
    "    if f_t > f_l:\n",
    "        if Verbose: print( 'Update case 1')\n",
    "        alpha_u, f_u, g_u = alpha_t, f_t, g_t\n",
    "    else:\n",
    "        if sgnd < 0:\n",
    "            if Verbose: print ('Update case 2')\n",
    "            alpha_u, f_u, g_u = alpha_l, f_l, g_l\n",
    "        else:\n",
    "            if Verbose: print( 'Update case 3')\n",
    "        alpha_l, f_l, g_l = alpha_t, f_t, g_t\n",
    "\n",
    "\n",
    "    \"\"\"Refine trial value if it lies outside [alpha_t, alpha_u] or\n",
    "    is otherwise too close to alpha_u.\"\"\"\n",
    "    alpha_tt = min(alpha_max, alpha_tt)\n",
    "    alpha_tt = max(alpha_min, alpha_tt) #Update\n",
    "    alpha_t = alpha_tt\n",
    "    if isBracketed and isBounded:\n",
    "        if alpha_u > alpha_l:\n",
    "            alpha_t = min(alpha_l + delta * (alpha_u - alpha_l), alpha_t)\n",
    "        else:\n",
    "            alpha_t = max(alpha_l + delta * (alpha_u - alpha_l), alpha_t)\n",
    "\n",
    "    if Verbose: print( 'Refined trial value = ', alpha_t)\n",
    "\n",
    "    return isBracketed, alpha_t, alpha_l, alpha_u\n",
    "\n",
    "\n",
    "\n",
    "def MoreThuenteLineSearch(f, df, x, p, df_x = None, f_x = None, args = (),\n",
    "    mu = 0.01, eta = 0.5, ftol = _epsilon, gtol = _epsilon, rtol = _epsilon,\n",
    "    maxiter = 9, fmin = 0.0, alpha_min0 = 0.001, alpha_max0 = 100.0, alpha_t = 1.0,\n",
    "    p5 = 0.5, xtrapf = 4.0, delta = 0.66, Verbose = False):\n",
    "    \"\"\"Implements the line search algorithm of Mor\\'e and Thuente, ACM TOMS 1994\n",
    "    \"Line search algorithms with guaranteed sufficient decrease\"\n",
    "    doi: 10.1145/192115.192132\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    f\n",
    "    df\n",
    "    x\n",
    "    p\n",
    "    df_x = None\n",
    "    f_x = None\n",
    "    args = ()\n",
    "    mu = 0.01\n",
    "    mu is maximum required decrease\n",
    "     mu >= |f(x+dx) - f(x)| / |df(x).p|\n",
    "     mu in (0,1)\n",
    "    eta = 0.5\n",
    "    eta is maximum permissible value of |df(x+dx).p|/|df(x).p|\n",
    "    eta in (0,1)\n",
    "    ftol = _epsilon\n",
    "    gtol = _epsilon\n",
    "    rtol = _epsilon\n",
    "    rtol is relative tolerance for acceptable step\n",
    "    maxiter: Maximum number of iterations. Default: 9\n",
    "    fmin = 0.0\n",
    "    alpha_min0 = 0.001\n",
    "    alpha_max0 = 100.0\n",
    "    alpha_t = 1.0\n",
    "    p5 = 0.5\n",
    "    xtrapf: An extrapolation parameter. Recommended: between [1.1, 4.0].\n",
    "            Default: 4.0\n",
    "    delta = 0.66\n",
    "    Verbose = True\n",
    "    \"\"\"\n",
    "\n",
    "    ##################################\n",
    "    # Evaluate function and derivative\n",
    "    ##################################\n",
    "    if f_x == None: f_0 = f(x, *args)\n",
    "    else:           f_0 = f_x\n",
    "\n",
    "    if df_x == None: df_0 = df(x, *args)\n",
    "    else:            df_0 = df_x\n",
    "\n",
    "    g_0 = dot(df_0, p)\n",
    "\n",
    "    ############################################################\n",
    "    # Check that initial gradient in search direction is downhill\n",
    "    #############################################################\n",
    "    assert g_0.shape == (1, 1) or g_0.shape == (), 'Scipy screwed up the calculation.'\n",
    "    assert g_0 < 0, 'Attempted to linesearch uphill'\n",
    "\n",
    "    ############\n",
    "    # Initialize\n",
    "    ############\n",
    "    FoundGoodPoint = isBracketed = False\n",
    "    gtest = ftol * g_0\n",
    "    width = alpha_max0 - alpha_min0\n",
    "    width1 = width / p5\n",
    "\n",
    "    f_t = f(x + alpha_t * p, *args)\n",
    "    df_t = df(x + alpha_t * p, *args)\n",
    "    g_t = dot(df_t, p)\n",
    "\n",
    "    # alpha = step size\n",
    "    # f = function value\n",
    "    # g = gradient value in descent direction\n",
    "    # _l = best step so far\n",
    "    # _u = other endpoint of interval of uncertainty\n",
    "    # _t = current step iterate\n",
    "    alpha_l = alpha_u = 0.0\n",
    "    f_l = f_u = f_0\n",
    "    g_l = g_u = g_0\n",
    "\n",
    "    if alpha_l == alpha_min0: alpha_l += _epsilon\n",
    "    if alpha_u == alpha_max0: alpha_u -= _epsilon\n",
    "    assert alpha_min0 != alpha_max0\n",
    "\n",
    "    for k in range(maxiter):\n",
    "        if Verbose: print( 'Iteration', k, ':', alpha_t, 'in (', alpha_l, ',', alpha_u, ')'\n",
    ")\n",
    "        ############################################\n",
    "        # Initialize current interval of uncertainty\n",
    "        ############################################\n",
    "        if isBracketed:\n",
    "            alpha_min = min(alpha_l, alpha_u)\n",
    "            alpha_max = max(alpha_l, alpha_u)\n",
    "        else:\n",
    "            alpha_min = alpha_l\n",
    "            alpha_max = alpha_t + xtrapf * abs(alpha_t - alpha_l)\n",
    "            assert alpha_min <= alpha_max\n",
    "\n",
    "        ##################################################\n",
    "        # Safeguard trial step within pre-specified bounds\n",
    "        ##################################################\n",
    "        alpha_t = max(alpha_t, alpha_min0)\n",
    "        alpha_t = min(alpha_t, alpha_max0)\n",
    "\n",
    "        #If something funny happened, set it to lowest point obtained thus far\n",
    "        if ((isBracketed and (alpha_t <= alpha_min or alpha_t >= alpha_max)) \\\n",
    "         or (isBracketed and (alpha_max - alpha_min <= rtol * alpha_max)) \\\n",
    "            ):\n",
    "            if Verbose: print( 'Something funny happened')\n",
    "            return alpha_l\n",
    "\n",
    "\n",
    "        #Evaluate function and gradient at new point\n",
    "        f_t = f(x + alpha_t * p, *args)\n",
    "        df_t = df(x + alpha_t * p, *args)\n",
    "        g_t = dot(df_t, p)\n",
    "        ftest1 = f_0 + alpha_t * gtest\n",
    "\n",
    "        ######################\n",
    "        # Test for convergence\n",
    "        ######################\n",
    "\n",
    "        if (alpha_l == alpha_max0 and f_t <= ftest1 and g_t <= gtest):\n",
    "            if Verbose: print( 'Stuck on upper bound')\n",
    "            break\n",
    "        if (alpha_l == alpha_min0 and f_t > ftest1 and g_t >= gtest):\n",
    "            if Verbose: print( 'Stuck on lower bound')\n",
    "            break\n",
    "        #Check for a) sufficient decrease AND b) strong curvature criterion\n",
    "        if ((f_t <= f_0 + mu * g_0 * alpha_t) \\\n",
    "            and abs(g_t) <= eta * abs(g_0)):\n",
    "            if Verbose: print( 'Decrease criterion satisfied')\n",
    "            break\n",
    "\n",
    "        #Check for bad stuff\n",
    "        if isBracketed:\n",
    "            if (abs(alpha_max - alpha_min) >= delta * width1):\n",
    "                print ('Warning: Could not satisfy curvature conditions')\n",
    "                alpha_t = alpha_min + p5 * (alpha_max - alpha_min)\n",
    "            width1 = width\n",
    "            width = abs(alpha_max - alpha_min)\n",
    "\n",
    "        #If interval is bracketed to sufficient precision, break\n",
    "        if k > 0 and abs(alpha_l - alpha_u) < rtol:\n",
    "            if Verbose: print( 'Interval bracketed: alpha = ', alpha_t)\n",
    "            print ('Line search stuck. Emergency abort.')\n",
    "            break\n",
    "\n",
    "        ########################################################\n",
    "        # Nocedal's modification of the More-Thuente line search\n",
    "        ########################################################\n",
    "        ftest1 = f_0 + alpha_l * gtest\n",
    "\n",
    "        # Seek a step for which the modified function has a nonpositive value\n",
    "        # and a nonnegative derivative\n",
    "\n",
    "        if (not FoundGoodPoint) and f_t <= ftest1 and g_t >= g_0 * min(ftol, gtol):\n",
    "            FoundGoodPoint = True\n",
    "\n",
    "        # The modified function is used only if we don't have a step where the\n",
    "        # previous condition is attained, and if a lower function value has been\n",
    "        # obtained but it is not low enough\n",
    "\n",
    "        if (not FoundGoodPoint) and f_t <= f_l and f_t > ftest1:\n",
    "            if Verbose: print (\"Performing Nocedal's modification\")\n",
    "            f_mt = f_t - alpha_t * gtest\n",
    "            f_ml = f_l - alpha_l * gtest\n",
    "            f_mu = f_u - alpha_u * gtest\n",
    "            g_mt = g_t - gtest\n",
    "            g_ml = g_l - gtest\n",
    "            g_mu = g_u - gtest\n",
    "\n",
    "            isBracketed, alpha_t, alpha_l, alpha_u = MoreThuenteLinesearchIterate\\\n",
    "            (delta, alpha_l, alpha_u, alpha_t, alpha_min, alpha_max, f_ml, \\\n",
    "             g_ml, f_mt, g_mt, f_mu, g_mu, isBracketed, Verbose)\n",
    "\n",
    "            f_l = f_ml + alpha_l * gtest\n",
    "            f_u = f_mu + alpha_u * gtest\n",
    "            g_l = g_ml + gtest\n",
    "            g_u = g_mu + gtest\n",
    "\n",
    "        else:\n",
    "            if Verbose: print( 'Performing original More-Thuente line search')\n",
    "            isBracketed, alpha_t, alpha_l, alpha_u = MoreThuenteLinesearchIterate\\\n",
    "            (delta, alpha_l, alpha_u, alpha_t, alpha_min, alpha_max, f_l, g_l, \\\n",
    "             f_t, g_t, f_u, g_u, isBracketed, Verbose)\n",
    "\n",
    "        ###########################\n",
    "        # Force sufficient decrease\n",
    "        ###########################\n",
    "        if isBracketed:\n",
    "            if Verbose: print ('Force sufficient decrease')\n",
    "            if abs(alpha_u - alpha_l) >= delta * width1:\n",
    "                alpha_t = alpha_l + p5 * (alpha_u - alpha_l)\n",
    "            width1 = width\n",
    "            width = abs(alpha_u - alpha_l)\n",
    "\n",
    "\n",
    "    if Verbose: print( k + 1, 'iterations in More-Thuente line search')\n",
    "    return alpha_t\n",
    "\n",
    "\n",
    "\n",
    "def CubicLineSearch(f, df, x, p, df_x, f_x = None, args = (),\n",
    "        alpha = 0.0001, beta = 0.9, eps = 0.0001, Verbose = False):\n",
    "\n",
    "    if f_x is None:\n",
    "        f_x = f(x, *args)\n",
    "\n",
    "    assert df_x.T.shape == p.shape\n",
    "    assert 0 < alpha < 1, 'Invalid value of alpha in backtracking linesearch'\n",
    "    assert 0 < beta < 1, 'Invalid value of beta in backtracking linesearch'\n",
    "\n",
    "\n",
    "    phi = lambda c: f(x + (c * p), *args)\n",
    "\n",
    "    phi0 = f_x\n",
    "    derphi0 = dot(df_x, p)\n",
    "\n",
    "    assert derphi0.shape == (1, 1) or derphi0.shape == ()\n",
    "    assert derphi0 < 0, 'Attempted to linesearch uphill'\n",
    "\n",
    "    stp = 1.0\n",
    "\n",
    "    #Loop until Armijo condition is satisfied\n",
    "    while phi(stp) > phi0 + alpha * stp * derphi0:\n",
    "        #Quadratic interpolant\n",
    "        stp_q = -derphi0 * alpha ** 2 / (2 * phi(stp) - phi0 - derphi0 * stp)\n",
    "        if phi(stp_q) > f_x + alpha * stp_q * derphi0:\n",
    "            return stp_q\n",
    "\n",
    "        #Quadratic interpolant bad, use cubic interpolant\n",
    "        A = zeros((2, 2))\n",
    "        b = zeros((2,))\n",
    "\n",
    "        A[0, 0] = stp * stp\n",
    "        A[0, 1] = -stp_q ** 2\n",
    "        A[1, 0] = -stp ** 3\n",
    "        A[1, 1] = stp_q ** 3\n",
    "\n",
    "        b[0] = phi(stp_q) - phi0 - derphi0 * stp_q\n",
    "        b[1] = phi(stp) - phi0 - derphi0 * stp\n",
    "\n",
    "        xx = dot(A, b) / ((stp * stp_q) ** 2 * (stp_q - stp))\n",
    "\n",
    "        stp_c = (-xx[1] + (xx[1] ** 2 - 3 * xx[0] * derphi0) ** 0.5) / (3 * xx[0])\n",
    "\n",
    "        #Safeguard: if new alpha too close to predecessor or too small\n",
    "        if abs(stp_c - stp) < 0.001 or stp_c < 0.001:\n",
    "            stp *= beta\n",
    "        else:\n",
    "            stp = stp_c\n",
    "        #print stp\n",
    "\n",
    "    #print stp\n",
    "    return stp\n",
    "\n",
    "\n",
    "\n",
    "def BacktrackingLineSearch(f, df, x, p, df_x = None, f_x = None, args = (),\n",
    "        alpha = 0.0001, beta = 0.9, eps = _epsilon, Verbose = False):\n",
    "    \"\"\"\n",
    "    Backtracking linesearch\n",
    "    f: function\n",
    "    x: current point\n",
    "    p: direction of search\n",
    "    df_x: gradient at x\n",
    "    f_x = f(x) (Optional)\n",
    "    args: optional arguments to f (optional)\n",
    "    alpha, beta: backtracking parameters\n",
    "    eps: (Optional) quit if norm of step produced is less than this\n",
    "    Verbose: (Optional) Print lots of info about progress\n",
    "    \n",
    "    Reference: Nocedal and Wright 2/e (2006), p. 37\n",
    "    \n",
    "    Usage notes:\n",
    "    -----------\n",
    "    Recommended for Newton methods; less appropriate for quasi-Newton or conjugate gradients\n",
    "    \"\"\"\n",
    "\n",
    "    if f_x is None:\n",
    "        f_x = f(x, *args)\n",
    "    if df_x is None:\n",
    "        df_x = df(x, *args)\n",
    "\n",
    "    assert df_x.T.shape == p.shape\n",
    "    assert 0 < alpha < 1, 'Invalid value of alpha in backtracking linesearch'\n",
    "    assert 0 < beta < 1, 'Invalid value of beta in backtracking linesearch'\n",
    "\n",
    "    derphi = dot(df_x, p)\n",
    "\n",
    "    assert derphi.shape == (1, 1) or derphi.shape == ()\n",
    "    assert derphi < 0, 'Attempted to linesearch uphill'\n",
    "\n",
    "    stp = 1.0\n",
    "    fc = 0\n",
    "    len_p = norm(p)\n",
    "\n",
    "\n",
    "    #Loop until Armijo condition is satisfied\n",
    "    while f(x + stp * p, *args) > f_x + alpha * stp * derphi:\n",
    "        stp *= beta\n",
    "        fc += 1\n",
    "        if Verbose: print ('linesearch iteration', fc, ':', stp, f(x + stp * p, *args), f_x + alpha * stp * derphi)\n",
    "        if stp * len_p < eps:\n",
    "            print( 'Step is  too small, stop')\n",
    "            break\n",
    "    #if Verbose: print 'linesearch iteration 0 :', stp, f_x, f_x\n",
    "\n",
    "    if Verbose: print( 'linesearch done')\n",
    "    #print fc, 'iterations in linesearch'\n",
    "    return stp\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Trust region models #\n",
    "#######################\n",
    "\n",
    "class DogLeg:\n",
    "    def __init__(self, f, df, B, x, g, f_x = None, df_x = None, TrustRadius = 1.0):\n",
    "        self.TrustRadius = TrustRadius\n",
    "        self.f = f   #Function\n",
    "        self.df = df #Gradient function\n",
    "        self.B = B #Approximate Hessian\n",
    "        self.x = x #Current point\n",
    "        self.g = g #Search direction\n",
    "\n",
    "    def solve(self):\n",
    "        g = self.g\n",
    "        B = self.B\n",
    "        #Step 1: Find unconstrained solution\n",
    "        pB = g\n",
    "        #Step 2: Find Cauchy point\n",
    "        pU = -dot(g, g) / dot(g, dot(B, g)) * g\n",
    "        #INCOMPLETE\n",
    "\n",
    "\n",
    "################\n",
    "# Extrapolator #\n",
    "################\n",
    "\n",
    "class DIISExtrapolator:\n",
    "    def __init__(self, max = 300):\n",
    "        self.maxvectors = max\n",
    "        self.Reset()\n",
    "\n",
    "    def Reset(self):\n",
    "        self.errorvectors = []\n",
    "        self.coordvectors = []\n",
    "\n",
    "    def AddData(self, error, coord):\n",
    "        #Check max\n",
    "        while self.GetNumVectors() - self.maxvectors >= 0:\n",
    "            self.errorvectors.pop()\n",
    "            self.coordvectors.pop()\n",
    "\n",
    "        from numpy import asarray\n",
    "        #print asarray(error).flatten().shape, asarray(coord).flatten().shape\n",
    "\n",
    "        self.errorvectors.append(deepcopy(asarray(error).flatten()))\n",
    "        self.coordvectors.append(deepcopy(asarray(coord).flatten()))\n",
    "\n",
    "    def GetNumVectors(self):\n",
    "        assert len(self.errorvectors) == len(self.coordvectors)\n",
    "        return len(self.errorvectors)\n",
    "\n",
    "    def Extrapolate(self):\n",
    "        #Construct Overlap matrix (aka Gram matrix)\n",
    "        N = self.GetNumVectors()\n",
    "        if N == 0: return None\n",
    "\n",
    "        B = zeros((N + 1, N + 1))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                B[i, j] = dot(self.errorvectors[i], self.errorvectors[j])\n",
    "\n",
    "        B[N, :] = -1\n",
    "        B[:, N] = -1\n",
    "        B[N, N] = 0\n",
    "\n",
    "        v = zeros(N + 1)\n",
    "        v[N] = -1\n",
    "\n",
    "        #Solve for linear combination\n",
    "        xex = self.coordvectors[0] * 0\n",
    "        try:\n",
    "            c = solve(B, v)\n",
    "            assert c.shape == v.shape\n",
    "\n",
    "            #Generate interpolated coordinate\n",
    "            for i in range(N):\n",
    "                xex += c[i] * self.coordvectors[i]\n",
    "        except: #Linear dependency detected; trigger automatic reset\n",
    "            c = dot(pinv(B), v)\n",
    "            #Generate interpolated coordinate\n",
    "            for i in range(N):\n",
    "                xex += c[i] * self.coordvectors[i]\n",
    "            print('Linear dependency detected in DIIS; resetting')\n",
    "            self.Reset()\n",
    "\n",
    "        return xex\n",
    "\n",
    "\n",
    "#######################\n",
    "# Quasi-Newton driver #\n",
    "#######################\n",
    "\n",
    "\n",
    "class ApproxHessianBase:\n",
    "    def __init__(self, N = None):\n",
    "        if N is None:\n",
    "            self.M = None\n",
    "        else:\n",
    "            self.M = eye(N)\n",
    "\n",
    "    def Reset(self):\n",
    "        self.SetToIdentity()\n",
    "\n",
    "    def SetToIdentity(self):\n",
    "        self.M = eye(self.M.shape[0])\n",
    "\n",
    "    def SetToScaledIdentity(self, c = 1):\n",
    "        self.M = c * eye(self.M.shape[0])\n",
    "\n",
    "    def SetToMatrix(self, K):\n",
    "        self.M = K\n",
    "\n",
    "    def GetHessian(self):\n",
    "        return self.M\n",
    "\n",
    "    def Operate(self, g):\n",
    "        assert False, 'Should never run this'\n",
    "\n",
    "    def Update(self, *args):\n",
    "        assert False, 'Should never run this'\n",
    "\n",
    "    def __rshift__(self, g): #Overload >> operator\n",
    "        \"\"\"If self.B is defined, solves for x in B x = -g\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dx = self.Operate(g)\n",
    "        except ValueError as e:\n",
    "            print( e)\n",
    "            print( g.shape, self.M.shape)\n",
    "            exit()\n",
    "\n",
    "        try:\n",
    "            return asarray_chkfinite(dx)\n",
    "        except (LinAlgError, ValueError):\n",
    "            print( 'Restarting approximate Hessian\\n',\n",
    "            'Trigger restart, fall back to steepest descent')\n",
    "            self.SetToIdentity()\n",
    "            return g\n",
    "\n",
    "\n",
    "\n",
    "class ApproxHessian(ApproxHessianBase): \n",
    "    def __init__(self, N = None):\n",
    "        ApproxHessianBase.__init__(self, N)\n",
    "\n",
    "    def SetToNumericalHessian(self, df, x, step = 0.001, UseStencil = None):\n",
    "        if UseStencil is None:\n",
    "            import Stencil\n",
    "            UseStencil = Stencil.FirstOrderCentralDifferenceStencil()\n",
    "        #Initialize Hessian as numerical Hessian\n",
    "        self.M = UseStencil.ApplyToFunction(df, x, step)\n",
    "\n",
    "    def Operate(self, g):\n",
    "        \"\"\"If self.B is defined, solves for x in B x = -g\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dx = -solve(self.M, g)\n",
    "        except LinAlgError:\n",
    "            print( 'Warning, using pseudoinverse')\n",
    "            dx = -pinv(self.M, g)\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "class ApproxInvHessian(ApproxHessianBase):\n",
    "    def __init__(self, N = None):\n",
    "        ApproxHessianBase.__init__(self, N)\n",
    "\n",
    "    def SetToNumericalHessian(self, df, x, step = 0.001, UseStencil = None):\n",
    "        if UseStencil is None:\n",
    "            import Stencil\n",
    "            UseStencil = Stencil.FirstOrderCentralDifferenceStencil()\n",
    "        #Initialize Hessian as numerical Hessian\n",
    "        K = UseStencil.ApplyToFunction(df, x, step)\n",
    "        try:\n",
    "            self.M = inv(K)\n",
    "        except LinAlgError:\n",
    "            self.M = pinv(K)\n",
    "\n",
    "    def GetHessian(self):\n",
    "        try:\n",
    "            return inv(self.M)\n",
    "        except LinAlgError:\n",
    "            print( 'Warning, using pseudoinverse')\n",
    "            return pinv(self.M)\n",
    "\n",
    "    def Operate(self, g):\n",
    "        \"\"\"If self.H is defined, returns x = H g\n",
    "        \"\"\"\n",
    "        return - dot(self.M, g)\n",
    "\n",
    "\n",
    "\n",
    "class BFGSInvHessian(ApproxInvHessian):\n",
    "    def Update(self, s, y, s_dot_y = None):\n",
    "            if s_dot_y is None: s_dot_y = dot(s, y)\n",
    "            rhok = 1.0 / s_dot_y\n",
    "            I = eye(self.M.shape[0])\n",
    "            A1 = I - outer(s, y) * rhok\n",
    "            A2 = I - outer(y, s) * rhok\n",
    "            self.M = dot(A1, dot(self.M, A2))\n",
    "            self.M += +rhok * outer(s, s)\n",
    "\n",
    "\n",
    "class LBFGSInvHessian(ApproxInvHessian):\n",
    "    def __init__(self, M = 20):\n",
    "        self.maxnumvecs = int(M)\n",
    "        self.rho = []\n",
    "        self.gamma = 1.0\n",
    "        self.Reset()\n",
    "\n",
    "        print( 'Initialized L-BFGS Hessian approximation with M =', self.maxnumvecs)\n",
    "\n",
    "    def Reset(self):\n",
    "        self.s = []\n",
    "        self.y = []\n",
    "        self.rho = []\n",
    "\n",
    "    def SetToIdentity(self): #Reset\n",
    "        self.Reset()\n",
    "\n",
    "    def SetToScaledIdentity(self, c = None): #Reset\n",
    "        self.Reset()\n",
    "        if c is not None: self.gamma = c\n",
    "\n",
    "    def Update(self, s, y, s_dot_y = None):\n",
    "        if len(self.s) == self.maxnumvecs:\n",
    "            self.s.pop(0)\n",
    "            self.y.pop(0)\n",
    "            self.rho.pop(0)\n",
    "\n",
    "        if s_dot_y is None: s_dot_y = dot(s, y)\n",
    "        rho = 1.0 / s_dot_y\n",
    "        if isinf(rho):\n",
    "            print( 'Warning, s . y = 0; ignoring pair')\n",
    "        else:\n",
    "            self.s.append(s)\n",
    "            self.y.append(y)\n",
    "            self.rho.append(rho)\n",
    "\n",
    "    def GetNumVecs(self):\n",
    "        assert len(self.s) == len(self.y)\n",
    "        return len(self.s)\n",
    "\n",
    "    def __rshift__(self, g): #Overload >> as multiply -H g\n",
    "        q = g\n",
    "        m = self.GetNumVecs()\n",
    "        s = self.s\n",
    "        y = self.y\n",
    "        a = zeros(m)\n",
    "        for i in range(m - 1, -1, -1):\n",
    "            a[i] = self.rho[i] * dot(s[i], q)\n",
    "            q = q - a[i] * y[i]\n",
    "\n",
    "        #r = Hguess * q\n",
    "        gamma = self.gamma\n",
    "        if m > 0: gamma = dot(s[-1], y[-1]) / dot(y[-1], y[-1])\n",
    "        #if m > 0: gamma = 0.5 * gamma + 0.5 * dot(s[-1], y[-1]) / dot(y[-1], y[-1])\n",
    "        self.gamma = gamma\n",
    "        print( 'gamma=',gamma)\n",
    "        r = gamma * q\n",
    "        for i in range(m):\n",
    "            b = self.rho[i] * dot(y[i], r)\n",
    "            r = r + s[i] * (a[i] - b)\n",
    "\n",
    "        return - r\n",
    "\n",
    "\n",
    "\n",
    "def Optimizer(f, x0, df = None, xtol = 0.1*_epsilon**0.5,\n",
    "             gtol =  0.1*_epsilon**0.5, maxstep = 1, maxiter = None,\n",
    "             line_search = BacktrackingLineSearch, DIISTrigger = 0):\n",
    "\n",
    "    #Hijack code with call to L-BFGS-B\n",
    "    #from scipy.optimize import fmin_l_bfgs_b\n",
    "    #return fmin_l_bfgs_b(f, x0, df, m = len(x0), pgtol = gtol, factr = 1.0/xtol, iprint=1)[0]\n",
    "\n",
    "    #Do LBFGS\n",
    "    #If true, will fall back on BFGS automatically\n",
    "    DoLBFGS = True\n",
    "\n",
    "    DoLineSearch = True\n",
    "    TrustRadius = 0.5\n",
    "    MaxTrustRadius = 10.0\n",
    "\n",
    "    #Cosine squared of uphill angle\n",
    "    UphillAngleThresh = 0.0\n",
    "    UphillFThresh = 0.001\n",
    "\n",
    "    #Massage x0\n",
    "    x0 = asarray_chkfinite(x0).ravel()\n",
    "    if x0.ndim == 0: x0.shape = (1,)\n",
    "\n",
    "    #Set default number of maxiters\n",
    "    if maxiter is None: maxiter = max(20000, len(x0) * 4)\n",
    "\n",
    "    ###########\n",
    "    #Initialize\n",
    "    ###########\n",
    "    x = x0\n",
    "    f_x, df_x = f(x0), df(x0)\n",
    "    norm_g = norm(df_x)\n",
    "    N = len(x0)\n",
    "    if DoLBFGS:\n",
    "        #Dimension of 3 --- 20 is normal\n",
    "        #H = LBFGSInvHessian(2 + N ** 0.2)\n",
    "        H = LBFGSInvHessian(N)\n",
    "    else:\n",
    "        H = BFGSInvHessian(N)\n",
    "        H.SetToNumericalHessian(df, x)\n",
    "\n",
    "    if DIISTrigger != 0: DIIS = DIISExtrapolator()\n",
    "\n",
    "\n",
    "\n",
    "    for k in range(maxiter):\n",
    "        #######################################################################\n",
    "        # Solve for unconstrained minimization direction p such that H p = -x #\n",
    "        #######################################################################\n",
    "        p = H >> df_x #overloaded\n",
    "        AcceptStep = True\n",
    "\n",
    "        ##########################################################\n",
    "        # Safeguard: perform linesearch or trust-region limiting #\n",
    "        #            to determine actual step s to take          #\n",
    "        ##########################################################\n",
    "        if DoLineSearch:\n",
    "            ####################\n",
    "            # Perform linesearch\n",
    "            ####################\n",
    "            alpha = line_search(f, df, x, p, df_x, f_x)\n",
    "            try:\n",
    "                alpha = line_search(f, df, x, p, df_x, f_x)\n",
    "                assert alpha > 0\n",
    "            except AssertionError:\n",
    "                print( 'Line search failed; falling back to exact line search')\n",
    "                alpha = MinpackLineSearch(f, df, x, p, df_x, f_x)\n",
    "                assert alpha > 0\n",
    "            s = alpha * p\n",
    "            norm_s = norm(s)\n",
    "        else:\n",
    "            ##############\n",
    "            # Do dog-leg #\n",
    "            ##############\n",
    "            #First do line search along steepest descent direction\n",
    "            alpha = line_search(f, df, x, -df_x, df_x, f_x)\n",
    "            #Cauchy point is x - alpha * df_x\n",
    "            #Next, move toward unconstrained solution from Cauchy point\n",
    "            dogleg = p + alpha * df_x\n",
    "            alpha2 = line_search(f, df, x - alpha * df_x, dogleg)\n",
    "            s = -alpha * df_x + alpha2 * dogleg\n",
    "            #Stupidly enforce simple trust radius\n",
    "            norm_s = norm(s)\n",
    "            while norm_s > TrustRadius:\n",
    "                alpha2 *= 0.9\n",
    "                s = -alpha * df_x + alpha2 * dogleg\n",
    "                norm_s = norm(s)\n",
    "\n",
    "        ######################################\n",
    "        # Do DIIS Extrapolation if requested #\n",
    "        ######################################\n",
    "        DidDIIS = False\n",
    "        if DIISTrigger != 0:\n",
    "            data = zeros(len(df_x) + 1)\n",
    "            data[1:] = df_x\n",
    "            data[0] = f_x\n",
    "            DIIS.AddData(error = data, coord = x)\n",
    "            if DIIS.GetNumVectors() % DIISTrigger == 0:\n",
    "                mindf = min([norm(xx[1:]) for xx in DIIS.errorvectors])\n",
    "                xdnew = DIIS.Extrapolate()\n",
    "                norm_df_xdnew = norm(df(xdnew))\n",
    "                f_xdnew = f(xdnew)\n",
    "                if norm_df_xdnew < mindf and f_xdnew < f_x:\n",
    "                    #Accept only if there is actually a reduction in |df|\n",
    "                    print( 'Accepted DIIS extrapolation')\n",
    "                    DidDIIS = True\n",
    "                    s = xdnew - x\n",
    "                    #if DIISTrigger > 1: DIISTrigger -= 1\n",
    "                    #print 'Improvement ratios', (f_x - f_xdnew) / f_x, (mindf - norm_df_xdnew) / mindf\n",
    "                    if (f_x - f_xdnew) < 0.000001 * f_x and (mindf - norm_df_xdnew) < 0.00001 * mindf :\n",
    "                        print( 'DIIS reaching diminishing returns, resetting')\n",
    "                        DIIS.Reset()\n",
    "                        #DIISTrigger += 1\n",
    "                else:\n",
    "                    print( 'DIIS wanted to go to', f_xdnew, norm_df_xdnew)\n",
    "                    print( \"Rejected DIIS extrapolation\")\n",
    "                    DIIS.Reset()\n",
    "                    DIISTrigger += 1\n",
    "                    AcceptStep = False\n",
    "            if DIISTrigger > 50:\n",
    "                print( 'Turning off DIIS')\n",
    "                DIISTrigger = 0\n",
    "\n",
    "        ####################################################\n",
    "        # Take step; compute function value and derivative #\n",
    "        ####################################################\n",
    "        xnew = x + s\n",
    "        f_xnew = f(xnew)\n",
    "        df_xnew = df(xnew)\n",
    "        y = df_xnew - df_x\n",
    "        norm_y = norm(y)\n",
    "        norm_gnew = norm(df_xnew)\n",
    "        s_y = dot(s, y)\n",
    "\n",
    "        #############################################\n",
    "        # If doing line search, update trust radius #\n",
    "        #############################################\n",
    "        if True:\n",
    "            ReductionRatio = 2 * (f_xnew - f_x) / dot(s, df_x)\n",
    "            print( '\\nReduction ratio:', ReductionRatio)\n",
    "            if ReductionRatio <= 0:\n",
    "                print( 'Iterations are producing a WORSE answer!\\nBailing.')\n",
    "                print( 'Rejecting step and restarting BFGS')\n",
    "                AcceptStep = False\n",
    "                if DoLBFGS:\n",
    "                    H.Reset()\n",
    "                    H.Update(s, y)\n",
    "                else:\n",
    "                    gamma = s_y / norm_y ** 2\n",
    "                    H.SetToScaledIdentity(gamma)\n",
    "\n",
    "        if not DoLineSearch: #ie do trust region\n",
    "            #Compute model quality?\n",
    "            ReductionRatio = 2 * (f_xnew - f_x) / dot(s, df_x)\n",
    "            print( '\\nReduction ratio:', ReductionRatio)\n",
    "            if ReductionRatio > 0.75:\n",
    "                if norm(s) == TrustRadius:\n",
    "                    TrustRadius = min (2 * TrustRadius, MaxTrustRadius)\n",
    "            elif ReductionRatio < 0.25:\n",
    "                TrustRadius *= 0.25\n",
    "\n",
    "        ###################\n",
    "        # Check convergence\n",
    "        ###################\n",
    "        if norm_g < gtol:\n",
    "            break\n",
    "\n",
    "        #################################\n",
    "        # Make sure we are going downhill\n",
    "        #################################\n",
    "        NegCosDescentAngle = s_y / (norm_s * norm_y)\n",
    "        DescentAngle = arccos(-NegCosDescentAngle)\n",
    "        DescentRatio = (f_x - f_xnew) / f_x\n",
    "        isGoingUphill = NegCosDescentAngle < UphillAngleThresh\n",
    "        isIncreasing = -DescentRatio > UphillFThresh\n",
    "        if isGoingUphill or isIncreasing:\n",
    "            if isGoingUphill:\n",
    "                print( '\\nIteration %d: WARNING: Going uphill with angle %f' % \\\n",
    "                    (k, -DescentAngle))\n",
    "\n",
    "            if isIncreasing:\n",
    "                print( '\\nIteration %d: WARNING: function increased by ratio %f' % \\\n",
    "                    (k, -DescentRatio))\n",
    "\n",
    "            print( 'Rejecting step and restarting BFGS')\n",
    "            AcceptStep = False\n",
    "            if DoLBFGS:\n",
    "                H.Reset()\n",
    "                H.Update(s, y)\n",
    "            else:\n",
    "                gamma = s_y / norm_y ** 2\n",
    "                H.SetToScaledIdentity(gamma)\n",
    "\n",
    "        else:\n",
    "            #####################################\n",
    "            # Accept step, update quasi-Hessian #\n",
    "            #####################################\n",
    "\n",
    "            if not DidDIIS:\n",
    "                H.Update(s, y)\n",
    "\n",
    "        #####################################################\n",
    "        # EXPERIMENTAL: Heuristics for switching algorithms #\n",
    "        #####################################################\n",
    "        if False and DoLineSearch and  DescentRatio < 1e-2:\n",
    "            print( '\\nTurning off linesearch')\n",
    "            DoLineSearch = False\n",
    "        if False and DescentRatio < 1e-2 and DIISTrigger==0:\n",
    "            print( 'Do DIIS')\n",
    "            DIISTrigger = 2\n",
    "            DIIS = DIISExtrapolator()\n",
    "            #print 'Switching to BFGS'\n",
    "            #H = BFGSInvHessian(N)\n",
    "            #gamma = dot(s, y) / dot(y, y)\n",
    "            #H.SetToScaledIdentity(gamma)\n",
    "\n",
    "        ###################################################\n",
    "        # Accept step: update function value and gradient #\n",
    "        ###################################################\n",
    "        if AcceptStep:\n",
    "            x = xnew\n",
    "            df_x = df_xnew\n",
    "            f_x = f_xnew\n",
    "            norm_g = norm_gnew\n",
    "\n",
    "        print( \"Iteration %d: f(x) = %f (%fx), ||f'(x)|| = %f, || dx || = %f, descent angle = %f\" \\\n",
    "            % (k, f_x, DescentRatio, norm_g, norm_s, DescentAngle))\n",
    "\n",
    "\n",
    "    if k == maxiter - 1: print( 'Maximum number of iterations reached')\n",
    "    print( 'Quasi-Newton done')\n",
    "\n",
    "    \"\"\"\n",
    "    W = ApproxHessian()\n",
    "    W.SetToNumericalHessian(df, x)\n",
    "    Hess = W.GetHessian()\n",
    "    from numpy.linalg import svd\n",
    "    for eigval in svd(Hess)[1]:\n",
    "        print eigval\n",
    "    \"\"\"\n",
    "    \n",
    "    exit()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
