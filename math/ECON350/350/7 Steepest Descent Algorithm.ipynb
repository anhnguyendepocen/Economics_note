{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dot Products, Norms, and Angles Between Vectors\n",
    "\n",
    "Suppose we have two nn-dimensional vectors xx and yy as shown below:\n",
    "\n",
    "$$x=\\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\x_n \\end{array} \\right) \\textrm{ and } y=\\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n \\end{array} \\right)$$\n",
    "\n",
    "### Dot Products\n",
    "\n",
    "We define the dot product of these two vectors, denoted $x⋅y$ in the following way\n",
    "\n",
    "$$x \\cdot y = x_1 y_1 + x_2 y_2 + x_3 y_3 + \\cdots + x_n y_n$$\n",
    "\n",
    "\n",
    "### norm of a vector\n",
    "\n",
    "We also define the norm of a vector $x$, denoted by $|x|$, by\n",
    "\n",
    "$$||x|| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$$\n",
    "\n",
    "\n",
    "This is meant to be geometrically interpreted as the length of the vector, or equivalently, the distance between the points $(0,0,...,0)$ and $(x_1,x_2,...,x_n$.\n",
    "\n",
    "Interestingly, we note that this can be written in a much shorter way by invoking the dot product:\n",
    "\n",
    "$$||x|| = \\sqrt{x \\cdot x}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### the triangle that can be formed from the vectors $x$, $y$, and $x−y$.\n",
    "\n",
    "Recall that the Law of Cosines, a generalization on the Pythagorean Theorem, gives us the relationship between the side lengths of an arbitrary triangle. Specifically, if a triangle has side lengths aa, bb, and cc, then\n",
    "$$a^2 + b^2 - 2ab\\cos \\theta = c^2$$\n",
    "\n",
    "where $θ$ is the angle between the sides of length $a$ and $b$.\n",
    "\n",
    "![](http://www.oxfordmathcenter.com/images/notes/168-00.png)\n",
    "\n",
    "Applying the Law of Cosines to this triangle, we have\n",
    "\n",
    "\n",
    "$$||x||^2 +||y||^2 - 2||x|| \\, ||y||\\cos \\theta = ||x-y||^2$$\n",
    "\n",
    "\n",
    "But this implies, using our observations about the dot product made above, that\n",
    "\n",
    "\n",
    "$$\\begin{array}{rcl} \n",
    "(x \\cdot x) + (y \\cdot y) - 2||x|| \\, ||y||\\cos \\theta & = & (x-y) \\cdot (x-y)\\\\ \n",
    "& = & x \\cdot (x-y) - y \\cdot (x-y)\\\\ \n",
    "& = & (x \\cdot x) - (x \\cdot y) - (y \\cdot x) + (y \\cdot y)\\\\ \n",
    "& = & (x \\cdot x) - (x \\cdot y) - (x \\cdot y) + (y \\cdot y)\\\\ \n",
    "& = & (x \\cdot x) - 2(x \\cdot y) + (y \\cdot y)\\\\ \n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "Subtracting the common $(x⋅x)$ and $(y⋅y)$ from both sides, we find\n",
    "\n",
    "\n",
    "$$- 2||x|| \\, ||y||\\cos \\theta = - 2(x \\cdot y)$$\n",
    "\n",
    "\n",
    "Which, solving for $cosθ$ tells us\n",
    "\n",
    "$$\\cos \\theta = \\frac{x \\cdot y}{||x|| \\, ||y||}$$\n",
    "\n",
    "\n",
    "\n",
    "ref: \n",
    "http://www.oxfordmathcenter.com/drupal7/node/168\n",
    "\n",
    "test01 2017 summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent\n",
    "\n",
    "\n",
    "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent\n",
    "\n",
    "https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/jiahao/1561144\n",
    "\n",
    "\n",
    "MatrixCalculus provides matrix calculus for everyone. It is an online tool that computes vector and matrix derivatives (matrix calculus).\n",
    "http://www.matrixcalculus.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import sympy as sym\n",
    "sym.init_printing()\n",
    "x1,x2,alpha = sym.symbols('x_1 x_2 alpha ', positive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import series\n",
    "x, xbar = sym.symbols(\"x,xbar\")\n",
    "f = sym.Function(\"f\")\n",
    "\n",
    "sym.series(f(x), x, x0=xbar, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent method(梯度下降法)\n",
    "\n",
    "The algorithm of gradient descent method reads:\n",
    "\n",
    "1. Initialization: find a initial value of x0 in the feasible set\n",
    "\n",
    "2. Convergence Test: calculate the gradient of current value $g=∇f(x)$, and know if it satisfy the convergent criteria.\n",
    "\n",
    "\n",
    "$$x_{k+1} = x_k - \\alpha  d$$\n",
    "\n",
    "$$d = -g(x)$$\n",
    "\n",
    "$$g(x) = \\frac{\\partial f}{\\partial x}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Update: if not converge, update $x_{i+1}=x_i - \\alpha g$, where $\\alpha$ is a small number to control the step size.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\Delta x = - \\alpha  g$$\n",
    "\n",
    "at direction $d$, we find the best $\\alpha$ by line search\n",
    "$$f(x)  = f{\\left (\\bar{x} \\right )} + \\left(x - \\bar{x}\\right) g(\\bar{x}) + \\frac{1}{2} \\left(x - \\bar{x}\\right)^T H \\left(x - \\bar{x}\\right)) $$\n",
    "\n",
    "\n",
    "$$f(x)= f(\\bar x + \\Delta x) = f{\\left (\\bar{x} \\right )} + \\Delta x g(\\bar{x}) + \\frac{1}{2} \\Delta x^T H \\Delta x $$\n",
    "\n",
    "\n",
    "$$f(x)  = f{\\left (\\bar{x} \\right )}  - \\alpha  g \\cdot g(\\bar{x}) + \\frac{1}{2} (\\alpha  g(\\bar{x}))^T H \\alpha g(\\bar{x}) $$\n",
    "\n",
    "$$f(x)  = f{\\left (\\bar{x} \\right )}  - \\alpha  g \\cdot g(\\bar{x}) + \\frac{1}{2} \\alpha^2  g(\\bar{x})^T H g(\\bar{x}) $$\n",
    "\n",
    "F.O.C results in\n",
    "\n",
    "$$\\alpha = \\frac{g^T g}{g^T H g}$$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sym.Matrix([x1,x2])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2*x1**2- 2*x1*x2 +x2**2 +2*x1-2*x2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g =sym.Matrix([f]).jacobian(x).T\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = sym.hessian(f,x)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym.series(f, x, x0=xbar, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = (g.T*g)/(g.T*H*g)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = -alpha*g.T\n",
    "dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting point at (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.subs({x1:3,x2:3}).evalf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx.subs({x1:3,x2:3}).evalf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sympy symbolic function to numpy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.array([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_numpy = sym.lambdify(x,f,'numpy')\n",
    "\n",
    "# *x0 devide x0 to 2 items \n",
    "# generally in Python you can use the * operator to unpack array values.\n",
    "#  adding brackets around the variable in lambdify also works, such as: gradFunc = sym.lambdify([x], g, modules=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_numpy( x0[0],x0[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_numpy = sym.lambdify(x,g,'numpy')\n",
    "g_numpy(x0[0],x0[1] )\n",
    "#https://stackoverflow.com/questions/34664359/how-to-make-a-sympy-lambdifyed-function-accept-array-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_numpy = sym.lambdify(x,alpha,'numpy')\n",
    "alpha_numpy(x0[0],x0[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = [0,1]\n",
    "#x_solution\n",
    "\n",
    "# just for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(-1,3.5, 50), np.linspace(0,3.5, 50))\n",
    "#zz = 2*xx**2 - 2*xx*yy +yy**2 +2*xx-2*yy\n",
    "zz = f_numpy(xx,yy)\n",
    "x0 = np.array([3, 3])\n",
    "dx = - alpha_numpy(x0[0],x0[1])*g_numpy(x0[0],x0[1]).T\n",
    "dx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =x0\n",
    "x=x0 + dx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [x0]\n",
    "x = x0\n",
    "eps = 10e-5\n",
    "i =0\n",
    "while np.linalg.norm(dx[0])> eps:\n",
    "    dx = - alpha_numpy(*x)*g_numpy(*x).T\n",
    "    x = x + dx[0]\n",
    "    steps.append(x)\n",
    "    i+=1\n",
    "    print(\"x:\",x,\"f(x):\" ,f_numpy(x[0],x[1]),\"at iteration:\", i)\n",
    "\n",
    "# for i in range(50):\n",
    "#     dx = - alpha_numpy(*x)*g_numpy(*x).T\n",
    "#     x = x + dx[0]\n",
    "#     steps.append(x)\n",
    "#     print(\"x:\",x,\"f(x):\" ,f_numpy(x[0],x[1]),\"at iteration:\", i)\n",
    "steps = np.array(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(dx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.contourf(xx, yy, zz, np.logspace(-5, 3, 60), cmap=\"YlGn_r\");\n",
    "csf = ax.contourf(xx, yy, zz,16, alpha=.75);\n",
    "# Create a simple contour plot with labels using default colors.  The\n",
    "# inline argument to clabel will control whether the labels are draw\n",
    "# over the line segments of the contour, removing the lines beneath\n",
    "# the label\n",
    "cs = ax.contour(xx, yy, zz,16)\n",
    "plt.clabel(cs, inline=1, fontsize=10)\n",
    "ax.set_xlim(-1,3.5)\n",
    "ax.set_ylim(0,3.5)\n",
    "ax.plot(steps[:,0], steps[:,1], label = \"path\")\n",
    "ax.plot((x0[0]), (x0[1]), 'o', color='y', label = \"starting point\")\n",
    "ax.plot(solution[0], solution[1], 'o', color='r', label = \"solution\");\n",
    "plt.title('Steepest Descent Algorithm')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "# Make a colorbar for the ContourSet returned by the contourf call.\n",
    "fig.colorbar(csf,ax = ax)\n",
    "# Add the contour line levels to the colorbar\n",
    "#cbar.add_lines(cs)\n",
    "ax.legend( );\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line search\n",
    "\n",
    "https://en.wikipedia.org/wiki/Line_search\n",
    "\n",
    "In optimization, the line search strategy is one of two basic iterative approaches to find a local minimum ${\\displaystyle \\mathbf {x} ^{*}} $ of an objective function ${\\displaystyle f:\\mathbb {R} ^{n}\\to \\mathbb {R} } $ . \n",
    "\n",
    "\n",
    "Here is an example gradient method that uses a line search in step 4.\n",
    "\n",
    "Set iteration counter ${\\displaystyle \\displaystyle k=0} $, and make an initial guess ${\\displaystyle \\mathbf {x} _{0}} $ for the minimum\n",
    "\n",
    "Repeat:\n",
    "\n",
    "- Compute a descent direction ${\\displaystyle \\mathbf {p} _{k}} $\n",
    "\n",
    "- Choose ${\\displaystyle \\alpha _{k}} $ to 'loosely' minimize ${\\displaystyle h(\\alpha )=f(\\mathbf {x} _{k}+\\alpha \\mathbf {p} _{k})} $ over ${\\displaystyle \\alpha \\in \\mathbb {R} _{+}} $\n",
    "\n",
    "- Update ${\\displaystyle \\mathbf {x} _{k+1}=\\mathbf {x} _{k}+\\alpha _{k}\\mathbf {p} _{k}} $, and ${\\displaystyle k=k+1} $\n",
    "\n",
    "- Until ${\\displaystyle \\|\\nabla f(\\mathbf {x} _{k})\\|} < tolerance$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Optimization - Line Search Method\n",
    "\n",
    "https://nlperic.github.io/line-search/\n",
    "\n",
    "A simple unconstrained problem: how to find the minimum of $100(x_1^2-x_2)^2+(x_2-1)^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm  #color map\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = 100*np.square(np.square(x)-y)+np.square(x-1)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d') #gca get current ax\n",
    "surf = ax.plot_surface(x, y, z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=\\\n",
    "       0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "#plt.savefig('./res/surface.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic iterative methods for optimization problems: the line search method and the trust region method. The line search approach first finds a descent direction along which the objective function ff will be reduced and then computes the step size that determines how far xx should move along that direction.\n",
    "\n",
    "The updating rule of line search method is $x_{k+1}=x_k+\\alpha_k*d(x_k)$, we need to compute the direction $d(x_k)$ and step size $\\alpha_k$ seperately.\n",
    "\n",
    "- direction: gradient descent, Newton’s method, etc.\n",
    "\n",
    "- step size: Golden-section method, Armijo rule, Wolfe rule, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent method(梯度下降法)\n",
    "\n",
    "The algorithm of gradient descent method reads:\n",
    "\n",
    "1. Initialization: find a initial value of x0 in the feasible set\n",
    "\n",
    "1. Convergence Test: calculate the gradient of current value $∇f(x)$, and know if it satisfy the convergent criteria.\n",
    "\n",
    "1. Update: if not converge, update $xi+1=xi−r∇f(x$), where r is a small number to control the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def func(x):\n",
    "    return 100*np.square(np.square(x[0])-x[1])+np.square(x[0]-1)\n",
    "    \n",
    "# first order derivatives of the function\n",
    "def dfunc(x):\n",
    "    df1 = 400*x[0]*(np.square(x[0])-x[1])+2*(x[0]-1)\n",
    "    df2 = -200*(np.square(x[0])-x[1])\n",
    "    return np.array([df1, df2])\n",
    "    \n",
    "def grad(x, max_int):\n",
    "    miter = 1\n",
    "    step = .0001/miter\n",
    "    vals = []\n",
    "    objectfs = []\n",
    "    # you can customize your own condition of convergence, here we limit the number of iterations\n",
    "    while miter <= max_int:\n",
    "        vals.append(x)\n",
    "        objectfs.append(func(x))\n",
    "        temp = x-step*dfunc(x)\n",
    "        if np.abs(func(temp)-func(x))>0.01:\n",
    "            x = temp\n",
    "        else:\n",
    "            break\n",
    "        print(x, func(x), miter)\n",
    "        miter += 1\n",
    "    return vals, objectfs, miter\n",
    "\n",
    "start = [5, 5]\n",
    "val, objectf, iters = grad(start, 50)\n",
    "\n",
    "x = np.array([i[0] for i in val])\n",
    "y = np.array([i[1] for i in val])\n",
    "z = np.array(objectf)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.scatter(x, y, z, label='gradient descent method')\n",
    "surf = ax.plot_surface(x, y, z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=\\\n",
    "       0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "ax.legend()\n",
    "#plt.savefig('./res/g-d.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton’s method(牛顿法)\n",
    "\n",
    "Similar to the gradient method, the algorithm of Newton’s method for optimization reads:\n",
    "\n",
    "1. Initialization: find a initial value of $x_0$ in the feasible set\n",
    "\n",
    "1. Convergence Test: calculate the gradient and Hessian matrix of current value $∇f(x)$, and know if it satisfies the convergent criteria.\n",
    "\n",
    "1. Update: if not converge, update$ x_{i+1}=x_i−rH^{−1} ∇f(x)x$, where $r$ is a small number to control the step size and $H$ is the Hessian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def func(x):\n",
    "    return 100*np.square(np.square(x[0])-x[1])+np.square(x[0]-1)\n",
    "\n",
    "# first order derivatives of the function\n",
    "def dfunc(x):\n",
    "    df1 = 400*x[0]*(np.square(x[0])-x[1])+2*(x[0]-1)\n",
    "    df2 = -200*(np.square(x[0])-x[1])\n",
    "    return np.array([df1, df2])\n",
    "\n",
    "def invhess(x):\n",
    "    df11 = 1200*np.square(x[0])-400*x[1]+2\n",
    "    df12 = -400*x[0]\n",
    "    df21 = -400*x[0]\n",
    "    df22 = 200\n",
    "    hess = np.array([[df11, df12], [df21, df22]])\n",
    "    return inv(hess)\n",
    "\n",
    "def newton(x, max_int):\n",
    "    miter = 1\n",
    "    step = .5\n",
    "    vals = []\n",
    "    objectfs = []\n",
    "    # you can customize your own condition of convergence, here we limit the number of iterations\n",
    "    while miter <= max_int:\n",
    "        vals.append(x)\n",
    "        objectfs.append(func(x))\n",
    "        temp = x-step*(invhess(x).dot(dfunc(x)))\n",
    "        if np.abs(func(temp)-func(x))>0.01:\n",
    "            x = temp\n",
    "        else:\n",
    "            break\n",
    "        print(x, func(x), miter)\n",
    "        miter += 1\n",
    "    return vals, objectfs, miter\n",
    "\n",
    "start = [5, 5]\n",
    "val, objectf, iters = newton(start, 50)\n",
    "\n",
    "x = np.array([i[0] for i in val])\n",
    "y = np.array([i[1] for i in val])\n",
    "z = np.array(objectf)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(start[0], start[1], func(start), c = \"red\",label='start point')\n",
    "ax.scatter(x, y, z, label='newton method')\n",
    "#plt.savefig('./res/newton.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.optimize\n",
    "def test_func(x):\n",
    "    return (x[0])**2+(x[1])**2\n",
    "\n",
    "def test_grad(x):\n",
    "    return [2*x[0],2*x[1]]\n",
    "\n",
    "import numpy as np\n",
    "sp.optimize.line_search(test_func,test_grad,np.array([1.8,1.7]),np.array([-1.,-1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## conjugate gradient\n",
    "\n",
    "https://en.wikipedia.org/wiki/Conjugate_gradient_method\n",
    "\n",
    "\n",
    "\n",
    "The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition.\n",
    "\n",
    "We say that two non-zero vectors u and v are conjugate (with respect to A) if\n",
    "\n",
    "${\\displaystyle \\mathbf {u} ^{\\mathsf {T}}\\mathbf {A} \\mathbf {v} =0.} \\mathbf {u} ^{\\mathsf {T}}\\mathbf {A} \\mathbf {v} =0$\n",
    ".\n",
    "Since A is symmetric and positive definite, the left-hand side defines an inner product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''\n",
    "Pure Python implementation of some numerical optimizers\n",
    "Created on Jan 21, 2011\n",
    "@author Jiahao Chen\n",
    "'''\n",
    "\n",
    "from scipy import dot, eye, finfo, asarray_chkfinite, sqrt, \\\n",
    "    outer, zeros, isinf, arccos\n",
    "from scipy.linalg import inv, norm, pinv, solve, LinAlgError\n",
    "from copy import deepcopy\n",
    "\n",
    "_epsilon = sqrt(finfo(float).eps)\n",
    "\n",
    "#################\n",
    "# Line searches #\n",
    "#################\n",
    "def MinpackLineSearch(f, df, x, p, df_x = None, f_x = None,\n",
    "                args = (), c1 = 1e-4, c2 = 0.4, amax = 50, xtol = _epsilon):\n",
    "    \"Modified shamelessly from scipy.optimize\"\n",
    "\n",
    "    if df_x == None: df_x = df(x)\n",
    "    if f_x == None: f_x = f(x)\n",
    "    from scipy.optimize import line_search\n",
    "    stp, _, _, _, _, _ = line_search(f, df, x, p, df_x, f_x, f_x, args, c1, c2, amax)\n",
    "    if stp == None: #Fall back to backtracking linesearch\n",
    "        return BacktrackingLineSearch(f, df, x, p, df_x, f_x, args)\n",
    "    else:\n",
    "        return stp\n",
    "\n",
    "\n",
    "\n",
    "def MoreThuenteLinesearchIterate(delta, alpha_l, alpha_u, alpha_t, alpha_min,\n",
    "            alpha_max, f_l, g_l, f_t, g_t, f_u, g_u, isBracketed, Verbose):\n",
    "    #Section 4 of More and Thuente: Pick new trial alpha\n",
    "    if Verbose: print( 'Section 4 of More and Thuente')\n",
    "    assert not (isBracketed and not min(alpha_l, alpha_u) < alpha_t < max(alpha_l, alpha_u)), 'Bracketing fail'\n",
    "    #if g_t * (alpha_t - alpha_l) < 0.0:\n",
    "    #    if Verbose: print 'Going uphill; reverting to last good point'\n",
    "    #    return isBracketed, alpha_l, alpha_l, alpha_t\n",
    "\n",
    "    if alpha_min == alpha_max:\n",
    "        print( 'Squeezed!')\n",
    "        return isBracketed, alpha_min, alpha_min, alpha_min\n",
    "\n",
    "    assert alpha_min < alpha_max\n",
    "\n",
    "    sgnd = dot(g_t, g_l / abs(g_l))\n",
    "    isBounded = False\n",
    "    if f_t > f_l:\n",
    "        if Verbose:\n",
    "            print (\"\"\"Case 1: a higher function value. \\\n",
    "The minimum is bracketed by [a_l, a_t]. If the cubic step is closer to \\\n",
    "lower bound then take it, else take average of quadratic and cubic steps.\"\"\")\n",
    "        #Calculate cubic and quadratic steps\n",
    "        da = alpha_t - alpha_l\n",
    "        d1 = g_l + g_t + 3 * (f_l - f_t) / da\n",
    "        s = max(g_l, g_t, d1)\n",
    "        d2 = s * ((d1 / s) ** 2 - (g_l / s * g_t / s)) ** 0.5\n",
    "        if alpha_t < alpha_l:\n",
    "            d2 = -d2\n",
    "        cubp = d2 - g_l + d1\n",
    "        cubq = ((d2 - g_l) + d2) + g_t\n",
    "        if abs(cubq) > _epsilon:\n",
    "            r = cubp / cubq\n",
    "            #Minimizer of cubic that interpolates f_l, f_t, g_l, g_t\n",
    "            alpha_c = alpha_l + r * da\n",
    "            #Minimizer of quadratic that interpolates f_l, f_t, g_l\n",
    "            alpha_q = alpha_l + ((g_l / ((f_l - f_t) / da + g_l)) * 0.5 * da)\n",
    "        else:\n",
    "            #print 'WARNING: VERY SMALL INTERVAL'\n",
    "            alpha_c = alpha_q = 0.5 * (alpha_l + alpha_t) #Invalid, choose midpoint\n",
    "        #Calculate new trial\n",
    "        if Verbose:\n",
    "            print( 'Interpolating guesses:', alpha_c, alpha_q)\n",
    "        if abs(alpha_c - alpha_l) < abs(alpha_q - alpha_l):\n",
    "            alpha_tt = alpha_c\n",
    "        else:\n",
    "            alpha_tt = 0.5 * (alpha_c + alpha_q)\n",
    "        isBracketed = True\n",
    "        isBounded = True\n",
    "\n",
    "    elif sgnd < 0: #f_t <= f_l assumed by here\n",
    "        if Verbose:\n",
    "            print( \"\"\"Case 2: lower function value and derivatives are of opposite sign. \\\n",
    "The minimum is bracketed by [a_l, a_t]. If the cubic step is closer to lower \\\n",
    "bound then take it, else take quadratic step.\"\"\")\n",
    "        #Calculate cubic and quadratic steps\n",
    "        da = alpha_t - alpha_l\n",
    "        d1 = g_l + g_t + 3 * (f_l - f_t) / da\n",
    "        s = max(g_l, g_t, d1)\n",
    "        d2 = s * ((d1 / s) ** 2 - (g_l / s * g_t / s)) ** 0.5\n",
    "        if alpha_t < alpha_l:\n",
    "            d2 = -d2\n",
    "        cubp = d2 - g_t + d1\n",
    "        cubq = ((d2 - g_t) + d2) + g_l\n",
    "        if abs(cubq) > _epsilon:\n",
    "            r = cubp / cubq #Minimizer of cubic that interpolates f_l, f_t, g_l, g_t\n",
    "            alpha_c = alpha_t - r * da #Minimizer of quadratic that interpolates f_l, f_t, g_l\n",
    "            alpha_s = alpha_t - (g_l / (g_l - g_t)) * da\n",
    "        else:\n",
    "            #print 'WARNING: VERY SMALL INTERVAL'\n",
    "            alpha_s = alpha_c = 0.5 * (alpha_l + alpha_t) #Invalid, choose midpoint\n",
    "        #Calculate new trial\n",
    "        if Verbose:\n",
    "            print( 'Interpolating guesses:', alpha_c, alpha_s)\n",
    "        if abs(alpha_c - alpha_t) >= abs(alpha_s - alpha_t):\n",
    "            alpha_tt = alpha_c\n",
    "        else:\n",
    "            alpha_tt = alpha_s\n",
    "        isBracketed = True\n",
    "        isBounded = False\n",
    "    elif norm(g_t) <= norm(g_l): #g_t * g_l >= 0 and f_t <= f_l assumed by here\n",
    "        if Verbose:\n",
    "            print( \"\"\"Case 3: lower function value and derivatives are of same \\\n",
    "sign, and magnitude of derivative decreased.\\n Cubic step used only if it tends \\\n",
    "to infinity in direction of step or if minimum of cubic lies beyond step. \\\n",
    "Otherwise cubic step is defined to be either stpmin or stpmax. Quadratic step is \\\n",
    "taken if it is closer to alpha_t, else take the farthest step.\"\"\")\n",
    "        #Calculate cubic and quadratic steps\n",
    "        da = alpha_t - alpha_l\n",
    "        d1 = g_l + g_t + 3 * (f_l - f_t) / da\n",
    "        s = max(g_l, g_t, d1)\n",
    "        d2 = s * ((d1 / s) ** 2 - (g_l / s * g_t / s)) ** 0.5\n",
    "        if alpha_t < alpha_l:\n",
    "            d2 = -d2\n",
    "\n",
    "        cubp = d2 - g_t + d1\n",
    "        cubq = ((d2 - g_t) + d2) + g_l\n",
    "        r = cubp / cubq\n",
    "        #The case d2 == 0 arises only if cubic does not tend to infinity in\n",
    "        #direction of step\n",
    "        if (r < 0. and abs(d2) < _epsilon):\n",
    "            alpha_c = alpha_t - r * da\n",
    "        elif alpha_t > alpha_l:\n",
    "            alpha_c = alpha_max\n",
    "        else:\n",
    "            alpha_c = alpha_min\n",
    "\n",
    "        #Minimizer of quadratic that interpolates f_l, f_t, g_l\n",
    "        alpha_s = -alpha_t - (g_l / (g_l - g_t)) * da\n",
    "\n",
    "        if isBracketed:\n",
    "            if abs(alpha_c - alpha_t) < abs(alpha_s - alpha_t):\n",
    "                alpha_tt = alpha_c\n",
    "            else:\n",
    "                alpha_tt = alpha_s\n",
    "        else:\n",
    "            if abs(alpha_c - alpha_t) > abs(alpha_s - alpha_t):\n",
    "                alpha_tt = alpha_c\n",
    "            else:\n",
    "                alpha_tt = alpha_s\n",
    "    else:\n",
    "        if Verbose:\n",
    "            print (\"\"\"Case 4: lower function value and derivatives \\\n",
    "are of same sign, and magnitude of derivative did not decrease.\n",
    "The minimum is NOT NECESSARILY bracketed. \\\n",
    "If bracketed, take cubic step, otherwise take  alpha_min or alpha_max.\"\"\")\n",
    "        #Minimizer of cubic that interpolates f_u, f_t, g_u, g_t\n",
    "        #Formula from Nocedal and Wright 2/e, Eq. 3.59, p 59\n",
    "        if isBracketed:\n",
    "            da = alpha_t - alpha_u\n",
    "            d1 = g_u + g_t + 3 * (f_u - f_t) / da\n",
    "            s = max(g_l, g_t, d1)\n",
    "            d2 = s * ((d1 / s) ** 2 - (g_l / s * g_t / s)) ** 0.5\n",
    "            if alpha_t < alpha_u:\n",
    "                d2 = -d2\n",
    "            cubp = d2 - g_t + d1\n",
    "            cubq = ((d2 - g_t) + d2) + g_u\n",
    "            if abs(cubq) > _epsilon:\n",
    "                r = cubp / cubq\n",
    "                alpha_tt = alpha_t - r * da\n",
    "            else:\n",
    "                #print 'WARNING: VERY SMALL INTERVAL'\n",
    "                alpha_tt = 0.5 * (alpha_l + alpha_t) #Invalid, choose midpoint\n",
    "        else:\n",
    "            if alpha_t > alpha_l:\n",
    "                alpha_tt = alpha_max\n",
    "            else:\n",
    "                alpha_tt = alpha_min\n",
    "\n",
    "        isBounded = False #abs(g_t) > abs(g_l)\n",
    "\n",
    "    if Verbose: print( 'Now proceeding to update')\n",
    "\n",
    "    #Update interval of uncertainty\n",
    "    if f_t > f_l:\n",
    "        if Verbose: print( 'Update case 1')\n",
    "        alpha_u, f_u, g_u = alpha_t, f_t, g_t\n",
    "    else:\n",
    "        if sgnd < 0:\n",
    "            if Verbose: print ('Update case 2')\n",
    "            alpha_u, f_u, g_u = alpha_l, f_l, g_l\n",
    "        else:\n",
    "            if Verbose: print( 'Update case 3')\n",
    "        alpha_l, f_l, g_l = alpha_t, f_t, g_t\n",
    "\n",
    "\n",
    "    \"\"\"Refine trial value if it lies outside [alpha_t, alpha_u] or\n",
    "    is otherwise too close to alpha_u.\"\"\"\n",
    "    alpha_tt = min(alpha_max, alpha_tt)\n",
    "    alpha_tt = max(alpha_min, alpha_tt) #Update\n",
    "    alpha_t = alpha_tt\n",
    "    if isBracketed and isBounded:\n",
    "        if alpha_u > alpha_l:\n",
    "            alpha_t = min(alpha_l + delta * (alpha_u - alpha_l), alpha_t)\n",
    "        else:\n",
    "            alpha_t = max(alpha_l + delta * (alpha_u - alpha_l), alpha_t)\n",
    "\n",
    "    if Verbose: print( 'Refined trial value = ', alpha_t)\n",
    "\n",
    "    return isBracketed, alpha_t, alpha_l, alpha_u\n",
    "\n",
    "\n",
    "\n",
    "def MoreThuenteLineSearch(f, df, x, p, df_x = None, f_x = None, args = (),\n",
    "    mu = 0.01, eta = 0.5, ftol = _epsilon, gtol = _epsilon, rtol = _epsilon,\n",
    "    maxiter = 9, fmin = 0.0, alpha_min0 = 0.001, alpha_max0 = 100.0, alpha_t = 1.0,\n",
    "    p5 = 0.5, xtrapf = 4.0, delta = 0.66, Verbose = False):\n",
    "    \"\"\"Implements the line search algorithm of Mor\\'e and Thuente, ACM TOMS 1994\n",
    "    \"Line search algorithms with guaranteed sufficient decrease\"\n",
    "    doi: 10.1145/192115.192132\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    f\n",
    "    df\n",
    "    x\n",
    "    p\n",
    "    df_x = None\n",
    "    f_x = None\n",
    "    args = ()\n",
    "    mu = 0.01\n",
    "    mu is maximum required decrease\n",
    "     mu >= |f(x+dx) - f(x)| / |df(x).p|\n",
    "     mu in (0,1)\n",
    "    eta = 0.5\n",
    "    eta is maximum permissible value of |df(x+dx).p|/|df(x).p|\n",
    "    eta in (0,1)\n",
    "    ftol = _epsilon\n",
    "    gtol = _epsilon\n",
    "    rtol = _epsilon\n",
    "    rtol is relative tolerance for acceptable step\n",
    "    maxiter: Maximum number of iterations. Default: 9\n",
    "    fmin = 0.0\n",
    "    alpha_min0 = 0.001\n",
    "    alpha_max0 = 100.0\n",
    "    alpha_t = 1.0\n",
    "    p5 = 0.5\n",
    "    xtrapf: An extrapolation parameter. Recommended: between [1.1, 4.0].\n",
    "            Default: 4.0\n",
    "    delta = 0.66\n",
    "    Verbose = True\n",
    "    \"\"\"\n",
    "\n",
    "    ##################################\n",
    "    # Evaluate function and derivative\n",
    "    ##################################\n",
    "    if f_x == None: f_0 = f(x, *args)\n",
    "    else:           f_0 = f_x\n",
    "\n",
    "    if df_x == None: df_0 = df(x, *args)\n",
    "    else:            df_0 = df_x\n",
    "\n",
    "    g_0 = dot(df_0, p)\n",
    "\n",
    "    ############################################################\n",
    "    # Check that initial gradient in search direction is downhill\n",
    "    #############################################################\n",
    "    assert g_0.shape == (1, 1) or g_0.shape == (), 'Scipy screwed up the calculation.'\n",
    "    assert g_0 < 0, 'Attempted to linesearch uphill'\n",
    "\n",
    "    ############\n",
    "    # Initialize\n",
    "    ############\n",
    "    FoundGoodPoint = isBracketed = False\n",
    "    gtest = ftol * g_0\n",
    "    width = alpha_max0 - alpha_min0\n",
    "    width1 = width / p5\n",
    "\n",
    "    f_t = f(x + alpha_t * p, *args)\n",
    "    df_t = df(x + alpha_t * p, *args)\n",
    "    g_t = dot(df_t, p)\n",
    "\n",
    "    # alpha = step size\n",
    "    # f = function value\n",
    "    # g = gradient value in descent direction\n",
    "    # _l = best step so far\n",
    "    # _u = other endpoint of interval of uncertainty\n",
    "    # _t = current step iterate\n",
    "    alpha_l = alpha_u = 0.0\n",
    "    f_l = f_u = f_0\n",
    "    g_l = g_u = g_0\n",
    "\n",
    "    if alpha_l == alpha_min0: alpha_l += _epsilon\n",
    "    if alpha_u == alpha_max0: alpha_u -= _epsilon\n",
    "    assert alpha_min0 != alpha_max0\n",
    "\n",
    "    for k in range(maxiter):\n",
    "        if Verbose: print( 'Iteration', k, ':', alpha_t, 'in (', alpha_l, ',', alpha_u, ')'\n",
    ")\n",
    "        ############################################\n",
    "        # Initialize current interval of uncertainty\n",
    "        ############################################\n",
    "        if isBracketed:\n",
    "            alpha_min = min(alpha_l, alpha_u)\n",
    "            alpha_max = max(alpha_l, alpha_u)\n",
    "        else:\n",
    "            alpha_min = alpha_l\n",
    "            alpha_max = alpha_t + xtrapf * abs(alpha_t - alpha_l)\n",
    "            assert alpha_min <= alpha_max\n",
    "\n",
    "        ##################################################\n",
    "        # Safeguard trial step within pre-specified bounds\n",
    "        ##################################################\n",
    "        alpha_t = max(alpha_t, alpha_min0)\n",
    "        alpha_t = min(alpha_t, alpha_max0)\n",
    "\n",
    "        #If something funny happened, set it to lowest point obtained thus far\n",
    "        if ((isBracketed and (alpha_t <= alpha_min or alpha_t >= alpha_max)) \\\n",
    "         or (isBracketed and (alpha_max - alpha_min <= rtol * alpha_max)) \\\n",
    "            ):\n",
    "            if Verbose: print( 'Something funny happened')\n",
    "            return alpha_l\n",
    "\n",
    "\n",
    "        #Evaluate function and gradient at new point\n",
    "        f_t = f(x + alpha_t * p, *args)\n",
    "        df_t = df(x + alpha_t * p, *args)\n",
    "        g_t = dot(df_t, p)\n",
    "        ftest1 = f_0 + alpha_t * gtest\n",
    "\n",
    "        ######################\n",
    "        # Test for convergence\n",
    "        ######################\n",
    "\n",
    "        if (alpha_l == alpha_max0 and f_t <= ftest1 and g_t <= gtest):\n",
    "            if Verbose: print( 'Stuck on upper bound')\n",
    "            break\n",
    "        if (alpha_l == alpha_min0 and f_t > ftest1 and g_t >= gtest):\n",
    "            if Verbose: print( 'Stuck on lower bound')\n",
    "            break\n",
    "        #Check for a) sufficient decrease AND b) strong curvature criterion\n",
    "        if ((f_t <= f_0 + mu * g_0 * alpha_t) \\\n",
    "            and abs(g_t) <= eta * abs(g_0)):\n",
    "            if Verbose: print( 'Decrease criterion satisfied')\n",
    "            break\n",
    "\n",
    "        #Check for bad stuff\n",
    "        if isBracketed:\n",
    "            if (abs(alpha_max - alpha_min) >= delta * width1):\n",
    "                print ('Warning: Could not satisfy curvature conditions')\n",
    "                alpha_t = alpha_min + p5 * (alpha_max - alpha_min)\n",
    "            width1 = width\n",
    "            width = abs(alpha_max - alpha_min)\n",
    "\n",
    "        #If interval is bracketed to sufficient precision, break\n",
    "        if k > 0 and abs(alpha_l - alpha_u) < rtol:\n",
    "            if Verbose: print( 'Interval bracketed: alpha = ', alpha_t)\n",
    "            print ('Line search stuck. Emergency abort.')\n",
    "            break\n",
    "\n",
    "        ########################################################\n",
    "        # Nocedal's modification of the More-Thuente line search\n",
    "        ########################################################\n",
    "        ftest1 = f_0 + alpha_l * gtest\n",
    "\n",
    "        # Seek a step for which the modified function has a nonpositive value\n",
    "        # and a nonnegative derivative\n",
    "\n",
    "        if (not FoundGoodPoint) and f_t <= ftest1 and g_t >= g_0 * min(ftol, gtol):\n",
    "            FoundGoodPoint = True\n",
    "\n",
    "        # The modified function is used only if we don't have a step where the\n",
    "        # previous condition is attained, and if a lower function value has been\n",
    "        # obtained but it is not low enough\n",
    "\n",
    "        if (not FoundGoodPoint) and f_t <= f_l and f_t > ftest1:\n",
    "            if Verbose: print (\"Performing Nocedal's modification\")\n",
    "            f_mt = f_t - alpha_t * gtest\n",
    "            f_ml = f_l - alpha_l * gtest\n",
    "            f_mu = f_u - alpha_u * gtest\n",
    "            g_mt = g_t - gtest\n",
    "            g_ml = g_l - gtest\n",
    "            g_mu = g_u - gtest\n",
    "\n",
    "            isBracketed, alpha_t, alpha_l, alpha_u = MoreThuenteLinesearchIterate\\\n",
    "            (delta, alpha_l, alpha_u, alpha_t, alpha_min, alpha_max, f_ml, \\\n",
    "             g_ml, f_mt, g_mt, f_mu, g_mu, isBracketed, Verbose)\n",
    "\n",
    "            f_l = f_ml + alpha_l * gtest\n",
    "            f_u = f_mu + alpha_u * gtest\n",
    "            g_l = g_ml + gtest\n",
    "            g_u = g_mu + gtest\n",
    "\n",
    "        else:\n",
    "            if Verbose: print( 'Performing original More-Thuente line search')\n",
    "            isBracketed, alpha_t, alpha_l, alpha_u = MoreThuenteLinesearchIterate\\\n",
    "            (delta, alpha_l, alpha_u, alpha_t, alpha_min, alpha_max, f_l, g_l, \\\n",
    "             f_t, g_t, f_u, g_u, isBracketed, Verbose)\n",
    "\n",
    "        ###########################\n",
    "        # Force sufficient decrease\n",
    "        ###########################\n",
    "        if isBracketed:\n",
    "            if Verbose: print ('Force sufficient decrease')\n",
    "            if abs(alpha_u - alpha_l) >= delta * width1:\n",
    "                alpha_t = alpha_l + p5 * (alpha_u - alpha_l)\n",
    "            width1 = width\n",
    "            width = abs(alpha_u - alpha_l)\n",
    "\n",
    "\n",
    "    if Verbose: print( k + 1, 'iterations in More-Thuente line search')\n",
    "    return alpha_t\n",
    "\n",
    "\n",
    "\n",
    "def CubicLineSearch(f, df, x, p, df_x, f_x = None, args = (),\n",
    "        alpha = 0.0001, beta = 0.9, eps = 0.0001, Verbose = False):\n",
    "\n",
    "    if f_x is None:\n",
    "        f_x = f(x, *args)\n",
    "\n",
    "    assert df_x.T.shape == p.shape\n",
    "    assert 0 < alpha < 1, 'Invalid value of alpha in backtracking linesearch'\n",
    "    assert 0 < beta < 1, 'Invalid value of beta in backtracking linesearch'\n",
    "\n",
    "\n",
    "    phi = lambda c: f(x + (c * p), *args)\n",
    "\n",
    "    phi0 = f_x\n",
    "    derphi0 = dot(df_x, p)\n",
    "\n",
    "    assert derphi0.shape == (1, 1) or derphi0.shape == ()\n",
    "    assert derphi0 < 0, 'Attempted to linesearch uphill'\n",
    "\n",
    "    stp = 1.0\n",
    "\n",
    "    #Loop until Armijo condition is satisfied\n",
    "    while phi(stp) > phi0 + alpha * stp * derphi0:\n",
    "        #Quadratic interpolant\n",
    "        stp_q = -derphi0 * alpha ** 2 / (2 * phi(stp) - phi0 - derphi0 * stp)\n",
    "        if phi(stp_q) > f_x + alpha * stp_q * derphi0:\n",
    "            return stp_q\n",
    "\n",
    "        #Quadratic interpolant bad, use cubic interpolant\n",
    "        A = zeros((2, 2))\n",
    "        b = zeros((2,))\n",
    "\n",
    "        A[0, 0] = stp * stp\n",
    "        A[0, 1] = -stp_q ** 2\n",
    "        A[1, 0] = -stp ** 3\n",
    "        A[1, 1] = stp_q ** 3\n",
    "\n",
    "        b[0] = phi(stp_q) - phi0 - derphi0 * stp_q\n",
    "        b[1] = phi(stp) - phi0 - derphi0 * stp\n",
    "\n",
    "        xx = dot(A, b) / ((stp * stp_q) ** 2 * (stp_q - stp))\n",
    "\n",
    "        stp_c = (-xx[1] + (xx[1] ** 2 - 3 * xx[0] * derphi0) ** 0.5) / (3 * xx[0])\n",
    "\n",
    "        #Safeguard: if new alpha too close to predecessor or too small\n",
    "        if abs(stp_c - stp) < 0.001 or stp_c < 0.001:\n",
    "            stp *= beta\n",
    "        else:\n",
    "            stp = stp_c\n",
    "        #print stp\n",
    "\n",
    "    #print stp\n",
    "    return stp\n",
    "\n",
    "\n",
    "\n",
    "def BacktrackingLineSearch(f, df, x, p, df_x = None, f_x = None, args = (),\n",
    "        alpha = 0.0001, beta = 0.9, eps = _epsilon, Verbose = False):\n",
    "    \"\"\"\n",
    "    Backtracking linesearch\n",
    "    f: function\n",
    "    x: current point\n",
    "    p: direction of search\n",
    "    df_x: gradient at x\n",
    "    f_x = f(x) (Optional)\n",
    "    args: optional arguments to f (optional)\n",
    "    alpha, beta: backtracking parameters\n",
    "    eps: (Optional) quit if norm of step produced is less than this\n",
    "    Verbose: (Optional) Print lots of info about progress\n",
    "    \n",
    "    Reference: Nocedal and Wright 2/e (2006), p. 37\n",
    "    \n",
    "    Usage notes:\n",
    "    -----------\n",
    "    Recommended for Newton methods; less appropriate for quasi-Newton or conjugate gradients\n",
    "    \"\"\"\n",
    "\n",
    "    if f_x is None:\n",
    "        f_x = f(x, *args)\n",
    "    if df_x is None:\n",
    "        df_x = df(x, *args)\n",
    "\n",
    "    assert df_x.T.shape == p.shape\n",
    "    assert 0 < alpha < 1, 'Invalid value of alpha in backtracking linesearch'\n",
    "    assert 0 < beta < 1, 'Invalid value of beta in backtracking linesearch'\n",
    "\n",
    "    derphi = dot(df_x, p)\n",
    "\n",
    "    assert derphi.shape == (1, 1) or derphi.shape == ()\n",
    "    assert derphi < 0, 'Attempted to linesearch uphill'\n",
    "\n",
    "    stp = 1.0\n",
    "    fc = 0\n",
    "    len_p = norm(p)\n",
    "\n",
    "\n",
    "    #Loop until Armijo condition is satisfied\n",
    "    while f(x + stp * p, *args) > f_x + alpha * stp * derphi:\n",
    "        stp *= beta\n",
    "        fc += 1\n",
    "        if Verbose: print ('linesearch iteration', fc, ':', stp, f(x + stp * p, *args), f_x + alpha * stp * derphi)\n",
    "        if stp * len_p < eps:\n",
    "            print( 'Step is  too small, stop')\n",
    "            break\n",
    "    #if Verbose: print 'linesearch iteration 0 :', stp, f_x, f_x\n",
    "\n",
    "    if Verbose: print( 'linesearch done')\n",
    "    #print fc, 'iterations in linesearch'\n",
    "    return stp\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Trust region models #\n",
    "#######################\n",
    "\n",
    "class DogLeg:\n",
    "    def __init__(self, f, df, B, x, g, f_x = None, df_x = None, TrustRadius = 1.0):\n",
    "        self.TrustRadius = TrustRadius\n",
    "        self.f = f   #Function\n",
    "        self.df = df #Gradient function\n",
    "        self.B = B #Approximate Hessian\n",
    "        self.x = x #Current point\n",
    "        self.g = g #Search direction\n",
    "\n",
    "    def solve(self):\n",
    "        g = self.g\n",
    "        B = self.B\n",
    "        #Step 1: Find unconstrained solution\n",
    "        pB = g\n",
    "        #Step 2: Find Cauchy point\n",
    "        pU = -dot(g, g) / dot(g, dot(B, g)) * g\n",
    "        #INCOMPLETE\n",
    "\n",
    "\n",
    "################\n",
    "# Extrapolator #\n",
    "################\n",
    "\n",
    "class DIISExtrapolator:\n",
    "    def __init__(self, max = 300):\n",
    "        self.maxvectors = max\n",
    "        self.Reset()\n",
    "\n",
    "    def Reset(self):\n",
    "        self.errorvectors = []\n",
    "        self.coordvectors = []\n",
    "\n",
    "    def AddData(self, error, coord):\n",
    "        #Check max\n",
    "        while self.GetNumVectors() - self.maxvectors >= 0:\n",
    "            self.errorvectors.pop()\n",
    "            self.coordvectors.pop()\n",
    "\n",
    "        from numpy import asarray\n",
    "        #print asarray(error).flatten().shape, asarray(coord).flatten().shape\n",
    "\n",
    "        self.errorvectors.append(deepcopy(asarray(error).flatten()))\n",
    "        self.coordvectors.append(deepcopy(asarray(coord).flatten()))\n",
    "\n",
    "    def GetNumVectors(self):\n",
    "        assert len(self.errorvectors) == len(self.coordvectors)\n",
    "        return len(self.errorvectors)\n",
    "\n",
    "    def Extrapolate(self):\n",
    "        #Construct Overlap matrix (aka Gram matrix)\n",
    "        N = self.GetNumVectors()\n",
    "        if N == 0: return None\n",
    "\n",
    "        B = zeros((N + 1, N + 1))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                B[i, j] = dot(self.errorvectors[i], self.errorvectors[j])\n",
    "\n",
    "        B[N, :] = -1\n",
    "        B[:, N] = -1\n",
    "        B[N, N] = 0\n",
    "\n",
    "        v = zeros(N + 1)\n",
    "        v[N] = -1\n",
    "\n",
    "        #Solve for linear combination\n",
    "        xex = self.coordvectors[0] * 0\n",
    "        try:\n",
    "            c = solve(B, v)\n",
    "            assert c.shape == v.shape\n",
    "\n",
    "            #Generate interpolated coordinate\n",
    "            for i in range(N):\n",
    "                xex += c[i] * self.coordvectors[i]\n",
    "        except: #Linear dependency detected; trigger automatic reset\n",
    "            c = dot(pinv(B), v)\n",
    "            #Generate interpolated coordinate\n",
    "            for i in range(N):\n",
    "                xex += c[i] * self.coordvectors[i]\n",
    "            print('Linear dependency detected in DIIS; resetting')\n",
    "            self.Reset()\n",
    "\n",
    "        return xex\n",
    "\n",
    "\n",
    "#######################\n",
    "# Quasi-Newton driver #\n",
    "#######################\n",
    "\n",
    "\n",
    "class ApproxHessianBase:\n",
    "    def __init__(self, N = None):\n",
    "        if N is None:\n",
    "            self.M = None\n",
    "        else:\n",
    "            self.M = eye(N)\n",
    "\n",
    "    def Reset(self):\n",
    "        self.SetToIdentity()\n",
    "\n",
    "    def SetToIdentity(self):\n",
    "        self.M = eye(self.M.shape[0])\n",
    "\n",
    "    def SetToScaledIdentity(self, c = 1):\n",
    "        self.M = c * eye(self.M.shape[0])\n",
    "\n",
    "    def SetToMatrix(self, K):\n",
    "        self.M = K\n",
    "\n",
    "    def GetHessian(self):\n",
    "        return self.M\n",
    "\n",
    "    def Operate(self, g):\n",
    "        assert False, 'Should never run this'\n",
    "\n",
    "    def Update(self, *args):\n",
    "        assert False, 'Should never run this'\n",
    "\n",
    "    def __rshift__(self, g): #Overload >> operator\n",
    "        \"\"\"If self.B is defined, solves for x in B x = -g\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dx = self.Operate(g)\n",
    "        except ValueError as e:\n",
    "            print( e)\n",
    "            print( g.shape, self.M.shape)\n",
    "            exit()\n",
    "\n",
    "        try:\n",
    "            return asarray_chkfinite(dx)\n",
    "        except (LinAlgError, ValueError):\n",
    "            print( 'Restarting approximate Hessian\\n',\n",
    "            'Trigger restart, fall back to steepest descent')\n",
    "            self.SetToIdentity()\n",
    "            return g\n",
    "\n",
    "\n",
    "\n",
    "class ApproxHessian(ApproxHessianBase): \n",
    "    def __init__(self, N = None):\n",
    "        ApproxHessianBase.__init__(self, N)\n",
    "\n",
    "    def SetToNumericalHessian(self, df, x, step = 0.001, UseStencil = None):\n",
    "        if UseStencil is None:\n",
    "            import Stencil\n",
    "            UseStencil = Stencil.FirstOrderCentralDifferenceStencil()\n",
    "        #Initialize Hessian as numerical Hessian\n",
    "        self.M = UseStencil.ApplyToFunction(df, x, step)\n",
    "\n",
    "    def Operate(self, g):\n",
    "        \"\"\"If self.B is defined, solves for x in B x = -g\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dx = -solve(self.M, g)\n",
    "        except LinAlgError:\n",
    "            print( 'Warning, using pseudoinverse')\n",
    "            dx = -pinv(self.M, g)\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "class ApproxInvHessian(ApproxHessianBase):\n",
    "    def __init__(self, N = None):\n",
    "        ApproxHessianBase.__init__(self, N)\n",
    "\n",
    "    def SetToNumericalHessian(self, df, x, step = 0.001, UseStencil = None):\n",
    "        if UseStencil is None:\n",
    "            import Stencil\n",
    "            UseStencil = Stencil.FirstOrderCentralDifferenceStencil()\n",
    "        #Initialize Hessian as numerical Hessian\n",
    "        K = UseStencil.ApplyToFunction(df, x, step)\n",
    "        try:\n",
    "            self.M = inv(K)\n",
    "        except LinAlgError:\n",
    "            self.M = pinv(K)\n",
    "\n",
    "    def GetHessian(self):\n",
    "        try:\n",
    "            return inv(self.M)\n",
    "        except LinAlgError:\n",
    "            print( 'Warning, using pseudoinverse')\n",
    "            return pinv(self.M)\n",
    "\n",
    "    def Operate(self, g):\n",
    "        \"\"\"If self.H is defined, returns x = H g\n",
    "        \"\"\"\n",
    "        return - dot(self.M, g)\n",
    "\n",
    "\n",
    "\n",
    "class BFGSInvHessian(ApproxInvHessian):\n",
    "    def Update(self, s, y, s_dot_y = None):\n",
    "            if s_dot_y is None: s_dot_y = dot(s, y)\n",
    "            rhok = 1.0 / s_dot_y\n",
    "            I = eye(self.M.shape[0])\n",
    "            A1 = I - outer(s, y) * rhok\n",
    "            A2 = I - outer(y, s) * rhok\n",
    "            self.M = dot(A1, dot(self.M, A2))\n",
    "            self.M += +rhok * outer(s, s)\n",
    "\n",
    "\n",
    "class LBFGSInvHessian(ApproxInvHessian):\n",
    "    def __init__(self, M = 20):\n",
    "        self.maxnumvecs = int(M)\n",
    "        self.rho = []\n",
    "        self.gamma = 1.0\n",
    "        self.Reset()\n",
    "\n",
    "        print( 'Initialized L-BFGS Hessian approximation with M =', self.maxnumvecs)\n",
    "\n",
    "    def Reset(self):\n",
    "        self.s = []\n",
    "        self.y = []\n",
    "        self.rho = []\n",
    "\n",
    "    def SetToIdentity(self): #Reset\n",
    "        self.Reset()\n",
    "\n",
    "    def SetToScaledIdentity(self, c = None): #Reset\n",
    "        self.Reset()\n",
    "        if c is not None: self.gamma = c\n",
    "\n",
    "    def Update(self, s, y, s_dot_y = None):\n",
    "        if len(self.s) == self.maxnumvecs:\n",
    "            self.s.pop(0)\n",
    "            self.y.pop(0)\n",
    "            self.rho.pop(0)\n",
    "\n",
    "        if s_dot_y is None: s_dot_y = dot(s, y)\n",
    "        rho = 1.0 / s_dot_y\n",
    "        if isinf(rho):\n",
    "            print( 'Warning, s . y = 0; ignoring pair')\n",
    "        else:\n",
    "            self.s.append(s)\n",
    "            self.y.append(y)\n",
    "            self.rho.append(rho)\n",
    "\n",
    "    def GetNumVecs(self):\n",
    "        assert len(self.s) == len(self.y)\n",
    "        return len(self.s)\n",
    "\n",
    "    def __rshift__(self, g): #Overload >> as multiply -H g\n",
    "        q = g\n",
    "        m = self.GetNumVecs()\n",
    "        s = self.s\n",
    "        y = self.y\n",
    "        a = zeros(m)\n",
    "        for i in range(m - 1, -1, -1):\n",
    "            a[i] = self.rho[i] * dot(s[i], q)\n",
    "            q = q - a[i] * y[i]\n",
    "\n",
    "        #r = Hguess * q\n",
    "        gamma = self.gamma\n",
    "        if m > 0: gamma = dot(s[-1], y[-1]) / dot(y[-1], y[-1])\n",
    "        #if m > 0: gamma = 0.5 * gamma + 0.5 * dot(s[-1], y[-1]) / dot(y[-1], y[-1])\n",
    "        self.gamma = gamma\n",
    "        print( 'gamma=',gamma)\n",
    "        r = gamma * q\n",
    "        for i in range(m):\n",
    "            b = self.rho[i] * dot(y[i], r)\n",
    "            r = r + s[i] * (a[i] - b)\n",
    "\n",
    "        return - r\n",
    "\n",
    "\n",
    "\n",
    "def Optimizer(f, x0, df = None, xtol = 0.1*_epsilon**0.5,\n",
    "             gtol =  0.1*_epsilon**0.5, maxstep = 1, maxiter = None,\n",
    "             line_search = BacktrackingLineSearch, DIISTrigger = 0):\n",
    "\n",
    "    #Hijack code with call to L-BFGS-B\n",
    "    #from scipy.optimize import fmin_l_bfgs_b\n",
    "    #return fmin_l_bfgs_b(f, x0, df, m = len(x0), pgtol = gtol, factr = 1.0/xtol, iprint=1)[0]\n",
    "\n",
    "    #Do LBFGS\n",
    "    #If true, will fall back on BFGS automatically\n",
    "    DoLBFGS = True\n",
    "\n",
    "    DoLineSearch = True\n",
    "    TrustRadius = 0.5\n",
    "    MaxTrustRadius = 10.0\n",
    "\n",
    "    #Cosine squared of uphill angle\n",
    "    UphillAngleThresh = 0.0\n",
    "    UphillFThresh = 0.001\n",
    "\n",
    "    #Massage x0\n",
    "    x0 = asarray_chkfinite(x0).ravel()\n",
    "    if x0.ndim == 0: x0.shape = (1,)\n",
    "\n",
    "    #Set default number of maxiters\n",
    "    if maxiter is None: maxiter = max(20000, len(x0) * 4)\n",
    "\n",
    "    ###########\n",
    "    #Initialize\n",
    "    ###########\n",
    "    x = x0\n",
    "    f_x, df_x = f(x0), df(x0)\n",
    "    norm_g = norm(df_x)\n",
    "    N = len(x0)\n",
    "    if DoLBFGS:\n",
    "        #Dimension of 3 --- 20 is normal\n",
    "        #H = LBFGSInvHessian(2 + N ** 0.2)\n",
    "        H = LBFGSInvHessian(N)\n",
    "    else:\n",
    "        H = BFGSInvHessian(N)\n",
    "        H.SetToNumericalHessian(df, x)\n",
    "\n",
    "    if DIISTrigger != 0: DIIS = DIISExtrapolator()\n",
    "\n",
    "\n",
    "\n",
    "    for k in range(maxiter):\n",
    "        #######################################################################\n",
    "        # Solve for unconstrained minimization direction p such that H p = -x #\n",
    "        #######################################################################\n",
    "        p = H >> df_x #overloaded\n",
    "        AcceptStep = True\n",
    "\n",
    "        ##########################################################\n",
    "        # Safeguard: perform linesearch or trust-region limiting #\n",
    "        #            to determine actual step s to take          #\n",
    "        ##########################################################\n",
    "        if DoLineSearch:\n",
    "            ####################\n",
    "            # Perform linesearch\n",
    "            ####################\n",
    "            alpha = line_search(f, df, x, p, df_x, f_x)\n",
    "            try:\n",
    "                alpha = line_search(f, df, x, p, df_x, f_x)\n",
    "                assert alpha > 0\n",
    "            except AssertionError:\n",
    "                print( 'Line search failed; falling back to exact line search')\n",
    "                alpha = MinpackLineSearch(f, df, x, p, df_x, f_x)\n",
    "                assert alpha > 0\n",
    "            s = alpha * p\n",
    "            norm_s = norm(s)\n",
    "        else:\n",
    "            ##############\n",
    "            # Do dog-leg #\n",
    "            ##############\n",
    "            #First do line search along steepest descent direction\n",
    "            alpha = line_search(f, df, x, -df_x, df_x, f_x)\n",
    "            #Cauchy point is x - alpha * df_x\n",
    "            #Next, move toward unconstrained solution from Cauchy point\n",
    "            dogleg = p + alpha * df_x\n",
    "            alpha2 = line_search(f, df, x - alpha * df_x, dogleg)\n",
    "            s = -alpha * df_x + alpha2 * dogleg\n",
    "            #Stupidly enforce simple trust radius\n",
    "            norm_s = norm(s)\n",
    "            while norm_s > TrustRadius:\n",
    "                alpha2 *= 0.9\n",
    "                s = -alpha * df_x + alpha2 * dogleg\n",
    "                norm_s = norm(s)\n",
    "\n",
    "        ######################################\n",
    "        # Do DIIS Extrapolation if requested #\n",
    "        ######################################\n",
    "        DidDIIS = False\n",
    "        if DIISTrigger != 0:\n",
    "            data = zeros(len(df_x) + 1)\n",
    "            data[1:] = df_x\n",
    "            data[0] = f_x\n",
    "            DIIS.AddData(error = data, coord = x)\n",
    "            if DIIS.GetNumVectors() % DIISTrigger == 0:\n",
    "                mindf = min([norm(xx[1:]) for xx in DIIS.errorvectors])\n",
    "                xdnew = DIIS.Extrapolate()\n",
    "                norm_df_xdnew = norm(df(xdnew))\n",
    "                f_xdnew = f(xdnew)\n",
    "                if norm_df_xdnew < mindf and f_xdnew < f_x:\n",
    "                    #Accept only if there is actually a reduction in |df|\n",
    "                    print( 'Accepted DIIS extrapolation')\n",
    "                    DidDIIS = True\n",
    "                    s = xdnew - x\n",
    "                    #if DIISTrigger > 1: DIISTrigger -= 1\n",
    "                    #print 'Improvement ratios', (f_x - f_xdnew) / f_x, (mindf - norm_df_xdnew) / mindf\n",
    "                    if (f_x - f_xdnew) < 0.000001 * f_x and (mindf - norm_df_xdnew) < 0.00001 * mindf :\n",
    "                        print( 'DIIS reaching diminishing returns, resetting')\n",
    "                        DIIS.Reset()\n",
    "                        #DIISTrigger += 1\n",
    "                else:\n",
    "                    print( 'DIIS wanted to go to', f_xdnew, norm_df_xdnew)\n",
    "                    print( \"Rejected DIIS extrapolation\")\n",
    "                    DIIS.Reset()\n",
    "                    DIISTrigger += 1\n",
    "                    AcceptStep = False\n",
    "            if DIISTrigger > 50:\n",
    "                print( 'Turning off DIIS')\n",
    "                DIISTrigger = 0\n",
    "\n",
    "        ####################################################\n",
    "        # Take step; compute function value and derivative #\n",
    "        ####################################################\n",
    "        xnew = x + s\n",
    "        f_xnew = f(xnew)\n",
    "        df_xnew = df(xnew)\n",
    "        y = df_xnew - df_x\n",
    "        norm_y = norm(y)\n",
    "        norm_gnew = norm(df_xnew)\n",
    "        s_y = dot(s, y)\n",
    "\n",
    "        #############################################\n",
    "        # If doing line search, update trust radius #\n",
    "        #############################################\n",
    "        if True:\n",
    "            ReductionRatio = 2 * (f_xnew - f_x) / dot(s, df_x)\n",
    "            print( '\\nReduction ratio:', ReductionRatio)\n",
    "            if ReductionRatio <= 0:\n",
    "                print( 'Iterations are producing a WORSE answer!\\nBailing.')\n",
    "                print( 'Rejecting step and restarting BFGS')\n",
    "                AcceptStep = False\n",
    "                if DoLBFGS:\n",
    "                    H.Reset()\n",
    "                    H.Update(s, y)\n",
    "                else:\n",
    "                    gamma = s_y / norm_y ** 2\n",
    "                    H.SetToScaledIdentity(gamma)\n",
    "\n",
    "        if not DoLineSearch: #ie do trust region\n",
    "            #Compute model quality?\n",
    "            ReductionRatio = 2 * (f_xnew - f_x) / dot(s, df_x)\n",
    "            print( '\\nReduction ratio:', ReductionRatio)\n",
    "            if ReductionRatio > 0.75:\n",
    "                if norm(s) == TrustRadius:\n",
    "                    TrustRadius = min (2 * TrustRadius, MaxTrustRadius)\n",
    "            elif ReductionRatio < 0.25:\n",
    "                TrustRadius *= 0.25\n",
    "\n",
    "        ###################\n",
    "        # Check convergence\n",
    "        ###################\n",
    "        if norm_g < gtol:\n",
    "            break\n",
    "\n",
    "        #################################\n",
    "        # Make sure we are going downhill\n",
    "        #################################\n",
    "        NegCosDescentAngle = s_y / (norm_s * norm_y)\n",
    "        DescentAngle = arccos(-NegCosDescentAngle)\n",
    "        DescentRatio = (f_x - f_xnew) / f_x\n",
    "        isGoingUphill = NegCosDescentAngle < UphillAngleThresh\n",
    "        isIncreasing = -DescentRatio > UphillFThresh\n",
    "        if isGoingUphill or isIncreasing:\n",
    "            if isGoingUphill:\n",
    "                print( '\\nIteration %d: WARNING: Going uphill with angle %f' % \\\n",
    "                    (k, -DescentAngle))\n",
    "\n",
    "            if isIncreasing:\n",
    "                print( '\\nIteration %d: WARNING: function increased by ratio %f' % \\\n",
    "                    (k, -DescentRatio))\n",
    "\n",
    "            print( 'Rejecting step and restarting BFGS')\n",
    "            AcceptStep = False\n",
    "            if DoLBFGS:\n",
    "                H.Reset()\n",
    "                H.Update(s, y)\n",
    "            else:\n",
    "                gamma = s_y / norm_y ** 2\n",
    "                H.SetToScaledIdentity(gamma)\n",
    "\n",
    "        else:\n",
    "            #####################################\n",
    "            # Accept step, update quasi-Hessian #\n",
    "            #####################################\n",
    "\n",
    "            if not DidDIIS:\n",
    "                H.Update(s, y)\n",
    "\n",
    "        #####################################################\n",
    "        # EXPERIMENTAL: Heuristics for switching algorithms #\n",
    "        #####################################################\n",
    "        if False and DoLineSearch and  DescentRatio < 1e-2:\n",
    "            print( '\\nTurning off linesearch')\n",
    "            DoLineSearch = False\n",
    "        if False and DescentRatio < 1e-2 and DIISTrigger==0:\n",
    "            print( 'Do DIIS')\n",
    "            DIISTrigger = 2\n",
    "            DIIS = DIISExtrapolator()\n",
    "            #print 'Switching to BFGS'\n",
    "            #H = BFGSInvHessian(N)\n",
    "            #gamma = dot(s, y) / dot(y, y)\n",
    "            #H.SetToScaledIdentity(gamma)\n",
    "\n",
    "        ###################################################\n",
    "        # Accept step: update function value and gradient #\n",
    "        ###################################################\n",
    "        if AcceptStep:\n",
    "            x = xnew\n",
    "            df_x = df_xnew\n",
    "            f_x = f_xnew\n",
    "            norm_g = norm_gnew\n",
    "\n",
    "        print( \"Iteration %d: f(x) = %f (%fx), ||f'(x)|| = %f, || dx || = %f, descent angle = %f\" \\\n",
    "            % (k, f_x, DescentRatio, norm_g, norm_s, DescentAngle))\n",
    "\n",
    "\n",
    "    if k == maxiter - 1: print( 'Maximum number of iterations reached')\n",
    "    print( 'Quasi-Newton done')\n",
    "\n",
    "    \"\"\"\n",
    "    W = ApproxHessian()\n",
    "    W.SetToNumericalHessian(df, x)\n",
    "    Hess = W.GetHessian()\n",
    "    from numpy.linalg import svd\n",
    "    for eigval in svd(Hess)[1]:\n",
    "        print eigval\n",
    "    \"\"\"\n",
    "    \n",
    "    exit()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that the value of the step size ${\\displaystyle \\gamma } $  is allowed to change at every iteration. With certain assumptions on the function ${\\displaystyle F} $ (for example, ${\\displaystyle F} $ convex and ${\\displaystyle \\nabla F} $ Lipschitz) and particular choices of ${\\displaystyle \\gamma } $  (e.g., chosen either via a line search that satisfies the Wolfe conditions or the Barzilai-Borwein method shown as following),\n",
    "\n",
    "\n",
    "$${\\displaystyle \\gamma _{n}={\\frac {(\\mathbf {x} _{n}-\\mathbf {x} _{n-1})^{T}[\\nabla F(\\mathbf {x} _{n})-\\nabla F(\\mathbf {x} _{n-1})]}{||\\nabla F(\\mathbf {x} _{n})-\\nabla F(\\mathbf {x} _{n-1})||^{2}}}}$$\n",
    "\n",
    "convergence to a local minimum can be guaranteed. When the function ${\\displaystyle F}$ is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
