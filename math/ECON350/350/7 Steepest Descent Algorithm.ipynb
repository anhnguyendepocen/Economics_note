{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dot Products, Norms, and Angles Between Vectors\n",
    "\n",
    "Suppose we have two nn-dimensional vectors xx and yy as shown below:\n",
    "\n",
    "$$x=\\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\x_n \\end{array} \\right) \\textrm{ and } y=\\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n \\end{array} \\right)$$\n",
    "\n",
    "### Dot Products\n",
    "\n",
    "We define the dot product of these two vectors, denoted $x⋅y$ in the following way\n",
    "\n",
    "$$x \\cdot y = x_1 y_1 + x_2 y_2 + x_3 y_3 + \\cdots + x_n y_n$$\n",
    "\n",
    "\n",
    "### norm of a vector\n",
    "\n",
    "We also define the norm of a vector $x$, denoted by $|x|$, by\n",
    "\n",
    "$$||x|| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$$\n",
    "\n",
    "\n",
    "This is meant to be geometrically interpreted as the length of the vector, or equivalently, the distance between the points $(0,0,...,0)$ and $(x_1,x_2,...,x_n$.\n",
    "\n",
    "Interestingly, we note that this can be written in a much shorter way by invoking the dot product:\n",
    "\n",
    "$$||x|| = \\sqrt{x \\cdot x}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### the triangle that can be formed from the vectors $x$, $y$, and $x−y$.\n",
    "\n",
    "Recall that the Law of Cosines, a generalization on the Pythagorean Theorem, gives us the relationship between the side lengths of an arbitrary triangle. Specifically, if a triangle has side lengths aa, bb, and cc, then\n",
    "$$a^2 + b^2 - 2ab\\cos \\theta = c^2$$\n",
    "\n",
    "where $θ$ is the angle between the sides of length $a$ and $b$.\n",
    "\n",
    "![](http://www.oxfordmathcenter.com/images/notes/168-00.png)\n",
    "\n",
    "Applying the Law of Cosines to this triangle, we have\n",
    "\n",
    "\n",
    "$$||x||^2 +||y||^2 - 2||x|| \\, ||y||\\cos \\theta = ||x-y||^2$$\n",
    "\n",
    "\n",
    "But this implies, using our observations about the dot product made above, that\n",
    "\n",
    "\n",
    "$$\\begin{array}{rcl} \n",
    "(x \\cdot x) + (y \\cdot y) - 2||x|| \\, ||y||\\cos \\theta & = & (x-y) \\cdot (x-y)\\\\ \n",
    "& = & x \\cdot (x-y) - y \\cdot (x-y)\\\\ \n",
    "& = & (x \\cdot x) - (x \\cdot y) - (y \\cdot x) + (y \\cdot y)\\\\ \n",
    "& = & (x \\cdot x) - (x \\cdot y) - (x \\cdot y) + (y \\cdot y)\\\\ \n",
    "& = & (x \\cdot x) - 2(x \\cdot y) + (y \\cdot y)\\\\ \n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "Subtracting the common $(x⋅x)$ and $(y⋅y)$ from both sides, we find\n",
    "\n",
    "\n",
    "$$- 2||x|| \\, ||y||\\cos \\theta = - 2(x \\cdot y)$$\n",
    "\n",
    "\n",
    "Which, solving for $cosθ$ tells us\n",
    "\n",
    "$$\\cos \\theta = \\frac{x \\cdot y}{||x|| \\, ||y||}$$\n",
    "\n",
    "\n",
    "\n",
    "ref: \n",
    "http://www.oxfordmathcenter.com/drupal7/node/168\n",
    "\n",
    "test01 2017 summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent\n",
    "\n",
    "\n",
    "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent\n",
    "\n",
    "https://www.youtube.com/watch?v=IHZwWFHWa-w&t=2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/jiahao/1561144\n",
    "\n",
    "\n",
    "MatrixCalculus provides matrix calculus for everyone. It is an online tool that computes vector and matrix derivatives (matrix calculus).\n",
    "http://www.matrixcalculus.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "import sympy as sym\n",
    "sym.init_printing()\n",
    "x1,x2,alpha = sym.symbols('x_1 x_2 alpha ', positive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sympy import series\n",
    "x, xbar = sym.symbols(\"x,xbar\")\n",
    "f = sym.Function(\"f\")\n",
    "\n",
    "sym.series(f(x), x, x0=xbar, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent method(梯度下降法)\n",
    "\n",
    "The algorithm of gradient descent method reads:\n",
    "\n",
    "1. Initialization: find a initial value of x0 in the feasible set\n",
    "\n",
    "2. Convergence Test: calculate the gradient of current value $g=\\Delta f(x)$, and know if it satisfy the convergent criteria.\n",
    "\n",
    "$$||\\delta x|| = ||\\alpha d|| <= \\epsilon$$\n",
    "\n",
    "$$x_{k+1} = x_k - \\alpha  d$$\n",
    "\n",
    "$$d = -g(x)$$\n",
    "\n",
    "$$g(x) = \\frac{\\partial f}{\\partial x}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Update: if not converge, update $x_{i+1}=x_i - \\alpha g$, where $\\alpha$ is a small number to control the step size.\n",
    "\n",
    "\n",
    "$$\\Delta x = - \\alpha  g$$\n",
    "\n",
    "at direction $d$, we find the best $\\alpha$ by line search\n",
    "$$f(x)  = f{\\left (\\bar{x} \\right )} + \\left(x - \\bar{x}\\right) g(\\bar{x}) + \\frac{1}{2} \\left(x - \\bar{x}\\right)^T H \\left(x - \\bar{x}\\right)) $$\n",
    "\n",
    "\n",
    "$$f(x)= f(\\bar x + \\Delta x) = f{\\left (\\bar{x} \\right )} + \\Delta x g(\\bar{x}) + \\frac{1}{2} \\Delta x^T H \\Delta x $$\n",
    "\n",
    "\n",
    "$$f(x)  = f{\\left (\\bar{x} \\right )}  - \\alpha  g \\cdot g(\\bar{x}) + \\frac{1}{2} (\\alpha  g(\\bar{x}))^T H \\alpha g(\\bar{x}) $$\n",
    "\n",
    "$$f(x)  = f{\\left (\\bar{x} \\right )}  - \\alpha  g \\cdot g(\\bar{x}) + \\frac{1}{2} \\alpha^2  g(\\bar{x})^T H g(\\bar{x}) $$\n",
    "\n",
    "F.O.C results in\n",
    "\n",
    "$$\\alpha = \\frac{g^T g}{g^T H g}$$\n",
    "\n",
    "ref:\n",
    "https://en.wikipedia.org/wiki/Gradient_descent\n",
    "\n",
    "\n",
    "http://www.scipy-lectures.org/advanced/mathematical_optimization/index.html#gradient-based-methods\n",
    "\n",
    "\n",
    "https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/ch04.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://zh.gluon.ai/_images/gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overshoot\n",
    "\n",
    "If the step size or learning rate is too large, it will overshoot. \n",
    "\n",
    "![](http://zh.gluon.ai/_images/overshoot.png)\n",
    "\n",
    "https://awwapp.com/#\n",
    "\n",
    "online collaborate drawing board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Chinese lecture from Dr. Lu at UVic\n",
    "\n",
    "https://www.bilibili.com/video/av5400941/index_4.html?t=1764\n",
    "\n",
    "#### Gradient descent\n",
    "start at 40min: FOTS\n",
    "\n",
    "44min:  $d = -g$\n",
    "\n",
    "46min: decide $\\alpha$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sym.Matrix([x1,x2])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = 2*x1**2- 2*x1*x2 +x2**2 +2*x1-2*x2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sympy.plotting import plot_parametric\n",
    "from sympy.plotting import plot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot3d(f, (x1, -3, 3), (x2, -3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g =sym.Matrix([f]).jacobian(x).T\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = sym.hessian(f,x)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sym.series(f, x, x0=xbar, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = (g.T*g)/(g.T*H*g)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dx = -alpha*g.T\n",
    "dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting point at (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha.subs({x1:3,x2:3}).evalf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dx.subs({x1:3,x2:3}).evalf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sympy symbolic function to numpy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.array([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_numpy = sym.lambdify(x,f,'numpy')\n",
    "\n",
    "# *x0 devide x0 to 2 items \n",
    "# generally in Python you can use the * operator to unpack array values.\n",
    "#  adding brackets around the variable in lambdify also works, such as: gradFunc = sym.lambdify([x], g, modules=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_numpy( x0[0],x0[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_numpy = sym.lambdify(x,g,'numpy')\n",
    "g_numpy(x0[0],x0[1] )\n",
    "#https://stackoverflow.com/questions/34664359/how-to-make-a-sympy-lambdifyed-function-accept-array-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_numpy = sym.lambdify(x,alpha,'numpy')\n",
    "alpha_numpy(x0[0],x0[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solution = [0,1]\n",
    "#x_solution\n",
    "\n",
    "# just for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(-1,3.5, 50), np.linspace(0,3.5, 50))\n",
    "#zz = 2*xx**2 - 2*xx*yy +yy**2 +2*xx-2*yy\n",
    "zz = f_numpy(xx,yy)\n",
    "x0 = np.array([3, 3])\n",
    "dx = - alpha_numpy(x0[0],x0[1])*g_numpy(x0[0],x0[1]).T\n",
    "dx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =x0\n",
    "x=x0 + dx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = [x0]\n",
    "x = x0\n",
    "eps = 10e-6\n",
    "i =0\n",
    "while np.linalg.norm(dx[0])> eps:\n",
    "    dx = - alpha_numpy(*x)*g_numpy(*x).T\n",
    "    x = x + dx[0]\n",
    "    steps.append(x)\n",
    "    i+=1\n",
    "    print(\"x:\",x,\"f(x):\" ,f_numpy(x[0],x[1]),\"at iteration:\", i)\n",
    "\n",
    "# for i in range(50):\n",
    "#     dx = - alpha_numpy(*x)*g_numpy(*x).T\n",
    "#     x = x + dx[0]\n",
    "#     steps.append(x)\n",
    "#     print(\"x:\",x,\"f(x):\" ,f_numpy(x[0],x[1]),\"at iteration:\", i)\n",
    "steps = np.array(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(dx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.contourf(xx, yy, zz, np.logspace(-5, 3, 60), cmap=\"YlGn_r\");\n",
    "csf = ax.contourf(xx, yy, zz,16, alpha=.75);\n",
    "# Create a simple contour plot with labels using default colors.  The\n",
    "# inline argument to clabel will control whether the labels are draw\n",
    "# over the line segments of the contour, removing the lines beneath\n",
    "# the label\n",
    "cs = ax.contour(xx, yy, zz,16)\n",
    "plt.clabel(cs, inline=1, fontsize=10)\n",
    "ax.set_xlim(-1,3.5)\n",
    "ax.set_ylim(0,3.5)\n",
    "ax.plot(steps[:,0], steps[:,1], label = \"path\")\n",
    "ax.plot((x0[0]), (x0[1]), 'o', color='y', label = \"starting point\")\n",
    "ax.plot(solution[0], solution[1], 'o', color='r', label = \"solution\");\n",
    "plt.title('Steepest Descent Algorithm')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "# Make a colorbar for the ContourSet returned by the contourf call.\n",
    "fig.colorbar(csf,ax = ax)\n",
    "# Add the contour line levels to the colorbar\n",
    "#cbar.add_lines(cs)\n",
    "ax.legend( );\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x * np.cos(np.pi * x)\n",
    "\n",
    "x = np.arange(-1.0, 2.0, 0.1)\n",
    "fig = plt.figure()\n",
    "subplt = fig.add_subplot(111)\n",
    "subplt.annotate('local minimum', xy=(-0.3, -0.2), xytext=(-0.8, -1.0),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "subplt.annotate('global minimum', xy=(1.1, -0.9), xytext=(0.7, 0.1),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.plot(x, f(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dichotomous Search\n",
    "\n",
    "1-d optimization\n",
    "\n",
    "https://www.bilibili.com/video/av5400941/?from=search&seid=201449505216783598#page=2\n",
    "start at 40min\n",
    "\n",
    "similar to Golden section search\n",
    "\n",
    "```\n",
    "%  Program: DichotomousSearch.m\n",
    "%  Title: Dichotomous Search\n",
    "%  Authors:  A. Antoniou, W.-S. Lu, A. Mehlenbacher, University of Victoria\n",
    "%  Description: Implements the dichotomous search.\n",
    "%  Input:\n",
    "%       fname: objective function defined in an m-file\n",
    "%       xL,xU: initial interval of uncertainty \n",
    "%         eps: epsilon for interval around current point\n",
    "%        conv: final range of uncertainty (convergence criterion)\n",
    "%  Output:\n",
    "%    xbar: minimum point\n",
    "%    fbar: objective function evaluated at xbar\n",
    "%       k: number of iterations at convergence\n",
    "%  Examples:\n",
    "%  [xbar,fbar,k] = DichotomousSearch('myquartic',-2,1,0.005,1e-6)\n",
    "%  [xbar,fbar,k] = DichotomousSearch('mysin',0.5*pi,2*pi,0.005,1e-6)\n",
    "%=============================================================\n",
    "function [xbar,fbar,k] = DichotomousSearch(fname,xL,xU,eps,conv)\n",
    "disp(' ')\n",
    "disp('Program DichotomousSearch.m')\n",
    "k = 0;\n",
    "rou = xU - xL;\n",
    "format short\n",
    "%Search while the interval is greater than the convergence criterion\n",
    "while rou > conv\n",
    "   k = k + 1;\n",
    "   kcol(k)=k;  \n",
    "   x1 = 0.5*(xL + xU);\n",
    "   epsi = eps*rou;\n",
    "   xa = x1 - epsi;\n",
    "   xb = x1 + epsi;\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %1: Evaluate the function at xa and xb\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   fxa = feval(fname,xa);\n",
    "   fxb = feval(fname,xb);\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %2: Create x value arrays for display\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   xLcol(k) = xL;\n",
    "   xUcol(k) = xU;\n",
    "   xacol(k) = xa;\n",
    "   x1col(k) = x1;\n",
    "   xbcol(k) = xb;\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %3: Create function arrays for display\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   fxacol(k) = fxa;\n",
    "   fxbcol(k) = fxb;\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   %4: Revise the range of uncertainty for next iteration\n",
    "   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "   if fxa < fxb\n",
    "      xU = xb;\n",
    "   elseif fxa > fxb\n",
    "      xL = xa;\n",
    "   else\n",
    "      xL = xa;\n",
    "      xU = xb;\n",
    "   end\n",
    "   rou = xU - xL; \n",
    "end\n",
    "xbar = 0.5*(xL + xU);\n",
    "fbar = feval(fname,xbar);\n",
    "%Display final results\n",
    "results = [kcol;xLcol;xUcol;xacol;x1col;xbcol;fxacol;fxbcol];\n",
    "results'\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```Matlab\n",
    "function DichotomousSearchPlot(fname,xL,xU,yvalue)\n",
    "% Author:  A. Mehlenbacher, University of Victoria\n",
    "%Example:  DichotomousSearchPlot('myquartic',-2,1,-4)\n",
    "%Example:  DichotomousSearchPlot('mysin',1.57,6.28,-1)\n",
    "%Create x values for plot\n",
    "x = linspace(xL, xU, 50);  \n",
    "%Evaluate the function at these x values\n",
    "y = feval(fname,x);\n",
    "%Plot the x and y values\n",
    "plot(x,y)\n",
    "%Hold the plot for adding points\n",
    "hold on\n",
    "%Plot points and iteration numbers on the x axis (at yvalue level)\n",
    "%k=1:  Initial XL and XU\n",
    "xvalue = xL;\n",
    "plot(xvalue, yvalue, 'r+')\n",
    "text(xvalue, yvalue, '1')\n",
    "xvalue = xU;\n",
    "plot(xvalue, yvalue, 'r+')\n",
    "text(xvalue, yvalue, '1')\n",
    "%For the following, replace xL with the new value for xL or xU\n",
    "%k=2: \n",
    "xvalue = newvalue;\n",
    "plot(xvalue, yvalue, 'r+')\n",
    "text(xvalue, yvalue, '2')\n",
    "%k=3: \n",
    "xvalue = newvalue;\n",
    "plot(xvalue, yvalue, 'r+')\n",
    "text(xvalue, yvalue, '3')\n",
    "%k=4: \n",
    "xvalue = newvalue;\n",
    "plot(xvalue, yvalue, 'r+')\n",
    "text(xvalue, yvalue, '4')\n",
    "%k=5: \n",
    "xvalue = newvalue;\n",
    "plot(xvalue, yvalue, 'r+')\n",
    "text(xvalue, yvalue, '5')\n",
    "%k=6: \n",
    "xvalue = newvalue;\n",
    "plot(xvalue, yvalue, 'r+')\n",
    "text(xvalue, yvalue, '6')\n",
    "%\n",
    "hold off;\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line search\n",
    "\n",
    "https://en.wikipedia.org/wiki/Line_search\n",
    "\n",
    "https://www.bilibili.com/video/av5400941/?from=search&seid=201449505216783598#page=2\n",
    "start at 30min\n",
    "\n",
    "\n",
    "In optimization, the line search strategy is one of two basic iterative approaches to find a local minimum ${\\displaystyle \\mathbf {x} ^{*}} $ of an objective function ${\\displaystyle f:\\mathbb {R} ^{n}\\to \\mathbb {R} } $ . \n",
    "\n",
    "\n",
    "Here is an example gradient method that uses a line search in step 4.\n",
    "\n",
    "Set iteration counter ${\\displaystyle \\displaystyle k=0} $, and make an initial guess ${\\displaystyle \\mathbf {x} _{0}} $ for the minimum\n",
    "\n",
    "Repeat:\n",
    "\n",
    "- Compute a descent direction ${\\displaystyle \\mathbf {p} _{k}} $\n",
    "\n",
    "- Choose ${\\displaystyle \\alpha _{k}} $ to 'loosely' minimize ${\\displaystyle h(\\alpha )=f(\\mathbf {x} _{k}+\\alpha \\mathbf {p} _{k})} $ over ${\\displaystyle \\alpha \\in \\mathbb {R} _{+}} $\n",
    "\n",
    "- Update ${\\displaystyle \\mathbf {x} _{k+1}=\\mathbf {x} _{k}+\\alpha _{k}\\mathbf {p} _{k}} $, and ${\\displaystyle k=k+1} $\n",
    "\n",
    "- Until ${\\displaystyle \\|\\nabla f(\\mathbf {x} _{k})\\|} < tolerance$\n",
    "\n",
    "\n",
    "\n",
    "Line search in this course refer to at the step3 to decide which $\\alpha$ is the best choice.\n",
    "\n",
    "\n",
    "The `inex_lsearch` gives us back the best $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```Matlab\n",
    "% Program: inex_lsearch_econ350.m\n",
    "% Title: Inexact Line Search\n",
    "% Description: Implements Fletcher's inexact line search \n",
    "% Authors:  A. Antoniou, W.-S. Lu, A. Mehlenbacher, University of Victoria\n",
    "% Input:\n",
    "% x: initial point\n",
    "% s: search direction\n",
    "% fname: objective function to be minimized along the direction of s\n",
    "% gname: gradient of objective function fname\n",
    "% Returns alpha\n",
    "%========================================================================\n",
    "function alpha = inex_lsearch_econ350(xk,s,fname,gname)\n",
    "k = 0;\n",
    "m = 0;\n",
    "tau = 0.1;\n",
    "chi = 0.75;\n",
    "rho = 0.1;  %For most accurate estimate\n",
    "sigma = 0.1; \n",
    "mhat = 400;\n",
    "epsilon = 1e-10;\n",
    "xk = xk(:);\n",
    "s = s(:);\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "%0: Initialize\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "f0 = feval(fname,xk);\n",
    "gk = feval(gname,xk);\n",
    "m = m+2;\n",
    "deltaf0 = f0;\n",
    "dk = s;\n",
    "aL = 0;\n",
    "aU = 1e99;\n",
    "fL = f0;\n",
    "dfL = gk'*dk;\n",
    "if abs(dfL) > epsilon\n",
    "   a0 = -2*deltaf0/dfL;\n",
    "else\n",
    "   a0 = 1;\n",
    "end\n",
    "if ((a0 <= 1e-9)||(a0 > 1))\n",
    "  a0 = 1;\n",
    "end\n",
    "%Loop until break \n",
    "while 1\n",
    "    deltak = a0*dk;\n",
    "    f0 = feval(fname,xk+deltak);\n",
    "    m = m + 1;\n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    %1: a0 to the right of aU\n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    if ((f0 > (fL + rho*(a0 - aL)*dfL)) & (abs(fL - f0) > epsilon) & (m < mhat))\n",
    "        if (a0 < aU)\n",
    "          aU = a0;\n",
    "        end\n",
    "        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        %2:  Compute new a0\n",
    "        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        a0hat = aL + ((a0 - aL)^2*dfL)/(2*(fL - f0 + (a0 - aL)*dfL));\n",
    "        a0Lhat = aL + tau*(aU - aL);\n",
    "        if (a0hat < a0Lhat)\n",
    "           a0hat = a0Lhat;\n",
    "        end\n",
    "        a0Uhat = aU - tau*(aU - aL);\n",
    "        if (a0hat > a0Uhat)\n",
    "           a0hat = a0Uhat;\n",
    "        end\n",
    "        a0 = a0hat;\n",
    "    else\n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    %3: a0 to the left of aL\n",
    "    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        gtemp = feval(gname,xk+a0*dk);\n",
    "        df0 = gtemp'*dk;\n",
    "        m = m + 1;\n",
    "        if (((df0 < sigma*dfL) & (abs(fL - f0) > epsilon) & (m < mhat) & (dfL ~= df0)))\n",
    "            deltaa0 = (a0 - aL)*df0/(dfL - df0);\n",
    "        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        %4:  Compute new a0\n",
    "        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            if (deltaa0 <= 0)\n",
    "                a0hat = 2*a0;\n",
    "            else\n",
    "                a0hat = a0 + deltaa0;\n",
    "            end\n",
    "            a0Uhat = a0 + chi*(aU - a0);\n",
    "            if (a0hat > a0Uhat)\n",
    "                a0hat = a0Uhat;\n",
    "            end\n",
    "            aL = a0;\n",
    "            a0 = a0hat;\n",
    "            fL = f0;\n",
    "            dfL = df0;\n",
    "        else\n",
    "        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        %5:  a0 between aL and aU so stop\n",
    "        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    "end % while 1\n",
    "if a0 < 1e-5,\n",
    "    alpha = 1e-5;\n",
    "else\n",
    "    alpha = a0;\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python version\n",
    "\n",
    "\n",
    "\n",
    "#% Program: inex_lsearch_econ350.m\n",
    "#% Title: Inexact Line Search\n",
    "#% Description: Implements Fletcher's inexact line search \n",
    "#% Authors:  A. Antoniou, W.-S. Lu, A. Mehlenbacher, University of Victoria\n",
    "#% Input:\n",
    "#% x: initial point\n",
    "#% s: search direction\n",
    "#% fname: objective function to be minimized along the direction of s\n",
    "#% gname: gradient of objective function fname\n",
    "#% Returns alpha\n",
    "#%========================================================================\n",
    "def linesearch_alpha(xk,s,fname,gname):\n",
    "    k = 0;\n",
    "    m = 0;\n",
    "    tau = 0.1;\n",
    "    chi = 0.75;\n",
    "    rho = 0.1;  #For most accurate estimate\n",
    "    sigma = 0.1; \n",
    "    mhat = 400;\n",
    "    epsilon = 1e-10;\n",
    "    xk = xk[:];\n",
    "    s = s[:];\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    #%0: Initialize\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    def feval(funcName, *args):\n",
    "        return eval(funcName)(*args)\n",
    "    from math import *\n",
    "    f0 = feval(fname,xk);\n",
    "    gk = feval(gname,xk);\n",
    "    m = m+2;\n",
    "    deltaf0 = f0;\n",
    "    dk = s;\n",
    "    aL = 0;\n",
    "    aU = 1e99;\n",
    "    fL = f0;\n",
    "    dfL = gk.T*dk;\n",
    "    if abs(dfL) > epsilon:\n",
    "           a0 = -2*deltaf0/dfL;\n",
    "    else:\n",
    "           a0 = 1;\n",
    "    #end\n",
    "    if ((a0 <= 1e-9) or (a0 > 1)):\n",
    "          a0 = 1;\n",
    "    #end\n",
    "    #%Loop until break \n",
    "    while 1:\n",
    "        deltak = a0*dk;\n",
    "        f0 = feval(fname,xk+deltak);\n",
    "        m = m + 1;\n",
    "        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        #%1: a0 to the right of aU\n",
    "        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        if ((f0 > (fL + rho*(a0 - aL)*dfL)) & (abs(fL - f0) > epsilon) & (m < mhat)):\n",
    "            if (a0 < aU):\n",
    "              aU = a0;\n",
    "            #end\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            #%2:  Compute new a0\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            a0hat = aL + ((a0 - aL)^2*dfL)/(2*(fL - f0 + (a0 - aL)*dfL));\n",
    "            a0Lhat = aL + tau*(aU - aL);\n",
    "            if (a0hat < a0Lhat):\n",
    "                   a0hat = a0Lhat;\n",
    "            #end\n",
    "            a0Uhat = aU - tau*(aU - aL);\n",
    "            if (a0hat > a0Uhat):\n",
    "                   a0hat = a0Uhat;\n",
    "            end\n",
    "            a0 = a0hat;\n",
    "        else\n",
    "        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        #%3: a0 to the left of aL\n",
    "        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            gtemp = feval(gname,xk+a0*dk);\n",
    "            df0 = gtemp'*dk;\n",
    "            m = m + 1;\n",
    "            if (((df0 < sigma*dfL) & (abs(fL - f0) > epsilon) & (m < mhat) & (dfL ~= df0)))\n",
    "                deltaa0 = (a0 - aL)*df0/(dfL - df0);\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            #%4:  Compute new a0\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "                if (deltaa0 <= 0):\n",
    "                    a0hat = 2*a0;\n",
    "                else:\n",
    "                    a0hat = a0 + deltaa0;\n",
    "                end\n",
    "                a0Uhat = a0 + chi*(aU - a0);\n",
    "                if (a0hat > a0Uhat)\n",
    "                    a0hat = a0Uhat;\n",
    "                end\n",
    "                aL = a0;\n",
    "                a0 = a0hat;\n",
    "                fL = f0;\n",
    "                dfL = df0;\n",
    "            else:\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            #%5:  a0 between aL and aU so stop\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "                break;\n",
    "            end\n",
    "        end\n",
    "    end #% while 1\n",
    "    if a0 < 1e-5,\n",
    "        alpha = 1e-5;\n",
    "    else\n",
    "        alpha = a0;\n",
    "    end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T05:20:23.874631Z",
     "start_time": "2017-11-05T05:20:23.857101Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1e99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Optimization - Line Search Method\n",
    "\n",
    "https://nlperic.github.io/line-search/\n",
    "\n",
    "A simple unconstrained problem: how to find the minimum of $100(x_1^2-x_2)^2+(x_2-1)^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm  #color map\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = 100*np.square(np.square(x)-y)+np.square(x-1)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d') #gca get current ax\n",
    "surf = ax.plot_surface(x, y, z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=\\\n",
    "       0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "#plt.savefig('./res/surface.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic iterative methods for optimization problems: the line search method and the trust region method. The line search approach first finds a descent direction along which the objective function ff will be reduced and then computes the step size that determines how far $x$ should move along that direction.\n",
    "\n",
    "The updating rule of line search method is $x_{k+1}=x_k+\\alpha_k*d(x_k)$, we need to compute the direction $d(x_k)$ and step size $\\alpha_k$ seperately.\n",
    "\n",
    "- direction: gradient descent, Newton’s method, etc.\n",
    "\n",
    "- step size: Golden-section method, Armijo rule, Wolfe rule, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent method(梯度下降法)\n",
    "\n",
    "The algorithm of gradient descent method reads:\n",
    "\n",
    "1. Initialization: find a initial value of x0 in the feasible set\n",
    "\n",
    "1. Convergence Test: calculate the gradient of current value $\\Delta f(x)$, and know if it satisfy the convergent criteria.\n",
    "\n",
    "1. Update: if not converge, update $xi+1=xi− \\alpha \\Delta f(x)$, where $\\alpha$ is a small number to control the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def func(x):\n",
    "    return 100*np.square(np.square(x[0])-x[1])+np.square(x[0]-1)\n",
    "    \n",
    "# first order derivatives of the function\n",
    "def dfunc(x):\n",
    "    df1 = 400*x[0]*(np.square(x[0])-x[1])+2*(x[0]-1)\n",
    "    df2 = -200*(np.square(x[0])-x[1])\n",
    "    return np.array([df1, df2])\n",
    "    \n",
    "def grad(x, max_int):\n",
    "    miter = 1\n",
    "    step = .0001/miter\n",
    "    vals = []\n",
    "    objectfs = []\n",
    "    # you can customize your own condition of convergence, here we limit the number of iterations\n",
    "    while miter <= max_int:\n",
    "        vals.append(x)\n",
    "        objectfs.append(func(x))\n",
    "        temp = x-step*dfunc(x)\n",
    "        if np.abs(func(temp)-func(x))>0.01:\n",
    "            x = temp\n",
    "        else:\n",
    "            break\n",
    "        print(x, func(x), miter)\n",
    "        miter += 1\n",
    "    return vals, objectfs, miter\n",
    "\n",
    "start = [5, 5]\n",
    "val, objectf, iters = grad(start, 50)\n",
    "\n",
    "x = np.array([i[0] for i in val])\n",
    "y = np.array([i[1] for i in val])\n",
    "z = np.array(objectf)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# surf = ax.plot_surface(x, y, z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=\\\n",
    "#        0, antialiased=False)\n",
    "\n",
    "#ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.scatter(x, y, z, label='gradient descent method')\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "ax.legend()\n",
    "#plt.savefig('./res/g-d.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.optimize\n",
    "def test_func(x):\n",
    "    return (x[0])**2+(x[1])**2\n",
    "\n",
    "def test_grad(x):\n",
    "    return [2*x[0],2*x[1]]\n",
    "\n",
    "import numpy as np\n",
    "sp.optimize.line_search(test_func,test_grad,np.array([1.8,1.7]),np.array([-1.,-1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "%  Program: inex_lsearch.m \n",
    "%  Title: Inexact Line Search \n",
    "%  Description: Implements Fletcher's inexact line search described in  \n",
    "%  Algorithm 4.6. \n",
    "%  Theory: See Practical Optimization Sec. 4.8 \n",
    "%  Input: \n",
    "%    x:  initial point \n",
    "%    s:  search direction \n",
    "%    F:  objective function to be minimized along the direction of s   \n",
    "%    G:  gradient of objective function F  \n",
    "%   p1:  internal parameters that are required for the implementation of \n",
    "%        the line search regardless of the application at hand. \n",
    "%        It is a string (e.g. 'rho = 0.1') and can be a combination several \n",
    "%        internal parameters (e.g., 'rho = 0.25; sigma=0.5'). \n",
    "%        Useful p1's include:                            default value \n",
    "%        'rho=   ' defines right bracket                      0.1 \n",
    "%        'sigma= ' defines left bracket  (sigma >= rho)       0.1 \n",
    "%        'tau= '   defines minimum step for sectioning        0.1 \n",
    "%        'chi='                                               0.75 \n",
    "%   p2:  user-defined parameter vector. Note that p2 must be a vector \n",
    "%        with all components numerically specified. The order in which \n",
    "%        the components of p2 appear must be the same as what they appear  \n",
    "%        in function F and gradient G. For example, if p2 = [a b], then  \n",
    "%        F.m and G.m must be in the form of function z = F(x,p2)and \n",
    "%        function z = G(x,p2) \n",
    "%  Output: \n",
    "%     z: acceptable value of alpha \n",
    "%  Example 1:  \n",
    "%  Perform inexact line search using the Himmelblau function \n",
    "%     f(x1,x2) = (x1^2 + x2 - 11)^2 + (x1 + x2^2 - 7)^2 \n",
    "%  starting from point xk = [6 6]' along search direction s = [-1 -1]' \n",
    "%  using the default parameter values.  \n",
    "%  Solution:  \n",
    "%  Execute the command \n",
    "%     a1 = inex_lsearch([6 6]',[-1 -1]','f_himm','g_himm') \n",
    "%  Example 2:  \n",
    "%  Perform inexact line search using the Himmelblau funtion \n",
    "%  starting from point xk = [6 6]' along search direction s = [-1 -1]' \n",
    "%  with sigma = 0.5. \n",
    "%  Solution:  \n",
    "%  Execute the command \n",
    "%     a2 = inex_lsearch([6 6]',[-1 -1]','f_himm','g_himm','sigma = 0.5') \n",
    "%  Example 3:  \n",
    "%  Perform inexact line search using the paramerized Himmelblau funtion \n",
    "%     f(x1,x2,a,b) = (x1^2 + x2 - a^2)^2 + (x1 + x2^2 - b^2)^2 \n",
    "%  starting from point xk = [6 6]' along search direction s = [-1 -1]', \n",
    "%  with parameters a = 3.2 and b = 2.6. \n",
    "%  Solution:  \n",
    "%  Execute the command \n",
    "%     a3 = inex_lsearch([6 6]',[-1 -1]','f_himm_p','g_himm_p',[3.2 2.6]) \n",
    "%  Notes: \n",
    "%  1. Command  \n",
    "%       z = inex_lsearch(xk,s,F,G,p1,p2) \n",
    "%     adds a new function inex_lsearch to MATLAB's vocabulary. \n",
    "%  2. Do not use a semicolon in commands  \n",
    "%      a1 = inex_lsearch([6 6]',[-1 -1]','f_himm','g_himm') \n",
    "%      a2 = inex_lsearch([6 6]',[-1 -1]','f_himm','g_himm','sigma = 0.5') \n",
    "%      a3 = inex_lsearch([6 6]',[-1 -1]','f_himm_p','g_himm_p',[3.2 2.6]) \n",
    "%    otherwise the acceptable value of alpha will not be displayed. \n",
    "% 3. f_himm and g_himm are user defined functions implemented in m-files   \n",
    "%    f_himm.m and g_himm.m, respectively, and are used to evaluate the  \n",
    "%    Himmelblau function and its derivative. Similarly, f_himm_p and  \n",
    "%    g_himm_p are user defined functions for the parameterized Himmelblau \n",
    "%    function and its gradient. \n",
    "% 4. If you plan to use this line search as part of an optimization  \n",
    "%    algorithm delete lines 73, 74, and 171. \n",
    "%========================================================================== \n",
    "function z = inex_lsearch(xk,s,F,G,p1,p2) \n",
    "k = 0; \n",
    "m = 0; \n",
    "tau = 0.1; \n",
    "chi = 0.75; \n",
    "rho = 0.1; \n",
    "sigma = 0.1; \n",
    "mhat = 400; \n",
    "epsilon = 1e-10; \n",
    "xk = xk(:); \n",
    "s = s(:); \n",
    "parameterstring =''; \n",
    "% evaluate given parameters: \n",
    "  if nargin > 4, \n",
    "    if isstr(p1), \n",
    "      eval([p1 ';']); \n",
    "    else \n",
    "      parameterstring = ',p1'; \n",
    "    end \n",
    "  end \n",
    "  if nargin > 5, \n",
    "    if isstr(p2), \n",
    "      eval([p2 ';']); \n",
    "    else \n",
    "      parameterstring = ',p2'; \n",
    "    end \n",
    "  end \n",
    "% compute f0 and g0 \n",
    "  eval(['f0 = ' F '(xk' parameterstring ');']); \n",
    "  eval(['gk = ' G '(xk' parameterstring ');']); \n",
    "  m = m+2; \n",
    "  deltaf0 = f0; \n",
    "% step 2 Initialize line search \n",
    "  dk = s; \n",
    "  aL = 0; \n",
    "  aU = 1e99; \n",
    "  fL = f0; \n",
    "  dfL = gk'*dk; \n",
    "  if abs(dfL) > epsilon, \n",
    "    a0 = -2*deltaf0/dfL; \n",
    "  else \n",
    "    a0 = 1; \n",
    " end \n",
    " if ((a0 <= 1e-9)|(a0 > 1)), \n",
    "    a0 = 1; \n",
    " end \n",
    "%step 3 \n",
    " while 1, \n",
    "    deltak = a0*dk; \n",
    "    eval(['f0 = ' F '(xk+deltak' parameterstring ');']); \n",
    "    m = m + 1; \n",
    "%step 4 \n",
    "    if ((f0 > (fL + rho*(a0 - aL)*dfL)) & (abs(fL - f0) > epsilon) & (m < mhat)) \n",
    "      if (a0 < aU) \n",
    "        aU = a0; \n",
    "      end \n",
    "      % compute a0hat using equation 7.65 \n",
    "      a0hat = aL + ((a0 - aL)^2*dfL)/(2*(fL - f0 + (a0 - aL)*dfL)); \n",
    "      a0Lhat = aL + tau*(aU - aL); \n",
    "      if (a0hat < a0Lhat) \n",
    "        a0hat = a0Lhat; \n",
    "      end \n",
    "      a0Uhat = aU - tau*(aU - aL); \n",
    "      if (a0hat > a0Uhat) \n",
    "        a0hat = a0Uhat; \n",
    "      end \n",
    "      a0 = a0hat; \n",
    "    else \n",
    "      eval(['gtemp =' G '(xk+a0*dk' parameterstring ');']); \n",
    "      df0 = gtemp'*dk; \n",
    "      m = m + 1; \n",
    "      % step 6 \n",
    "      if (((df0 < sigma*dfL) & (abs(fL - f0) > epsilon) & (m < mhat) & (dfL ~= df0))) \n",
    "        deltaa0 = (a0 - aL)*df0/(dfL - df0); \n",
    "        if (deltaa0 <= 0) \n",
    "          a0hat = 2*a0; \n",
    "        else \n",
    "          a0hat = a0 + deltaa0; \n",
    "        end \n",
    "        a0Uhat = a0 + chi*(aU - a0); \n",
    "        if (a0hat > a0Uhat) \n",
    "          a0hat = a0Uhat; \n",
    "        end \n",
    "        aL = a0; \n",
    "        a0 = a0hat; \n",
    "        fL = f0; \n",
    "        dfL = df0; \n",
    "      else \n",
    "        break; \n",
    "      end \n",
    "    end \n",
    " end % while 1 \n",
    " if a0 < 1e-5, \n",
    "    z = 1e-5; \n",
    " else \n",
    "    z = a0; \n",
    " end \n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that the value of the step size ${\\displaystyle \\gamma } $  is allowed to change at every iteration. With certain assumptions on the function ${\\displaystyle F} $ (for example, ${\\displaystyle F} $ convex and ${\\displaystyle \\nabla F} $ Lipschitz) and particular choices of ${\\displaystyle \\gamma } $  (e.g., chosen either via a line search that satisfies the Wolfe conditions or the Barzilai-Borwein method shown as following),\n",
    "\n",
    "\n",
    "$${\\displaystyle \\gamma _{n}={\\frac {(\\mathbf {x} _{n}-\\mathbf {x} _{n-1})^{T}[\\nabla F(\\mathbf {x} _{n})-\\nabla F(\\mathbf {x} _{n-1})]}{||\\nabla F(\\mathbf {x} _{n})-\\nabla F(\\mathbf {x} _{n-1})||^{2}}}}$$\n",
    "\n",
    "convergence to a local minimum can be guaranteed. When the function ${\\displaystyle F}$ is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
