{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref:\n",
    "\n",
    "http://patwie.com/2017/01/04/optimization_with_tf.html\n",
    "\n",
    "https://cs.stackexchange.com/questions/75090/can-deep-learning-be-applied-to-nonlinear-parameter-estimation-problems\n",
    "\n",
    "https://github.com/Mazecreator/tensorflow-hints/blob/master/maximize/Maximize%20Test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$% <![CDATA[\n",
    "f(x) = \\frac{1}{2}x^\\top \\left(\\begin{matrix}9 & 2\\\\ 2 & 10\\end{matrix}\\right)x - \\left(\\begin{matrix}5 \\\\ 6\\end{matrix}\\right)^\\top x + 42, \\quad x\\in\\mathbb{R}^2 %]]>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Steepest Descent (gradient, Hessian, alpha)\n",
    "\n",
    "$$% <![CDATA[\n",
    "f(x) = \\frac{1}{2}x^\\top \\left(\\begin{matrix}4 & -2\\\\ -2 & 2\\end{matrix}\\right)x + \\left(\\begin{matrix}2 \\\\ -2\\end{matrix}\\right)^\\top x + 0, \\quad x\\in\\mathbb{R}^2 %]]>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$% <![CDATA[\n",
    "f(x) = \\frac{1}{2}x^\\top \\left(\\begin{matrix}2 & -0.4\\\\ -0.4 & 2\\end{matrix}\\right)x + \\left(\\begin{matrix}-2.2 \\\\ 2.2\\end{matrix}\\right)^\\top x + 2.2, \\quad x\\in\\mathbb{R}^2 %]]>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$% <![CDATA[\n",
    "f(x) = \\frac{1}{2}x^\\top \\left(\\begin{matrix}9 & 2\\\\ 2 & 10\\end{matrix}\\right)x - \\left(\\begin{matrix}5 \\\\ 6\\end{matrix}\\right)^\\top x + 42, \\quad x\\in\\mathbb{R}^2 %]]>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x)=\\frac{1}{2}x^\\top Ax + b^\\top x + c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we just declare all variables:\n",
    "x = tf.Variable(np.random.rand(2,1), dtype=tf.float32, name=\"x\")\n",
    "# we already make clear, that we are not going to optimize these variables\n",
    "b = tf.Variable([[2],[-2]], dtype=tf.float32, trainable=False, name=\"b\")\n",
    "A = tf.Variable([[4,-2],[-2,2]], dtype=tf.float32, trainable=False, name=\"A\")\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 ways to get the derivative:\n",
    "\n",
    "- the elegant way: by hand (I porpose to do that)\n",
    "- the clever way: look it up in the so-called “Matrix Cookbook”\n",
    "- the modern way: use http://www.tensortree.org\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x}f(x) = Ax + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great as it directly allows us to solve the optimization (minimization problem) by obtaining a solution for the linear equation system. The latter is quite evident:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solving Ax=b for x is as easy as:\n",
    "xstar = tf.matrix_solve_ls(A, -b)\n",
    "# now we just print the solution\n",
    "print( \"x=\", sess.run(xstar))\n",
    "objective = 0.5 * tf.matmul(tf.matmul(tf.transpose(xstar), A), xstar) + tf.matmul(tf.transpose(b), xstar) + 0\n",
    "print( \"f(x)=\", sess.run(objective))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not trust tf.matrix_solve you should check the result by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test A*x - b = 0 ?\n",
    "sess.run(tf.matmul(A, xstar) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very basic but very powerful iterative method is gradient descent:\n",
    "\n",
    "$$x \\gets x - \\eta \\cdot \\frac{\\partial}{\\partial x}f(x)$$\n",
    "\n",
    "It is quite easy to verify, that the gradient is always pointing in the direction of maximum increase. Hence, if we just move our current guess $x$ in the opposite direction \n",
    "$-\\frac{\\partial}{\\partial x}f(x)$ with a tiny step  $η=0.01 $we may converge at some point which is likely to be the minimum.\n",
    "\n",
    "A short version of this optimization procedure is:\n",
    "\n",
    "### Known Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# destroy previous session and graph and create new session\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# define variables in the problem\n",
    "x = tf.Variable(np.random.rand(2,1), dtype=tf.float32, name=\"x\")\n",
    "b = tf.Variable([[2],[-2]], dtype=tf.float32, trainable=False, name=\"b\")\n",
    "A = tf.Variable([[4,-2],[-2,2]], dtype=tf.float32, trainable=False, name=\"A\")\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# define expressions\n",
    "objective = 0.5 * tf.matmul(tf.matmul(tf.transpose(x), A), x) + tf.matmul(tf.transpose(b), x) + 0\n",
    "grad = tf.matmul(A, x) + b              # this is new\n",
    "optimize_op = x.assign(x - 0.01 * grad) # this is new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start at some random point again\n",
    "sess.run(x.assign(np.random.rand(2,1)))\n",
    "# optimize\n",
    "for _ in range(300):\n",
    "    sess.run(optimize_op)\n",
    "print( \"x=\", sess.run(x))\n",
    "print( \"f(x)=\", sess.run(objective))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This look conspicuously similar to our closed-form solution from the beginning.\n",
    "\n",
    "\n",
    "## Unknown Gradient\n",
    "Let’s assume we have no clue what’s the derivative looks like. We just assume that the objective function is continuously differentiable. Using reverse-mode auto-differentiation TensorFlow can compute these gradients fully automatically for us, see tf.gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad = tf.gradients(objective, x)[0]   # get gradient from objective wrt. to x\n",
    "optimize_op = x.assign(x - 0.01 * grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we use tf.gradients to compute all necessary gradients for us. Optimization is the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start at some random point again\n",
    "sess.run(x.assign(np.random.rand(2,1)))\n",
    "# optimize\n",
    "for _ in range(300):\n",
    "    sess.run(optimize_op)\n",
    "print( \"x=\", sess.run(x))\n",
    "print( \"f(x)=\", sess.run(objective))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this entire routine is a pretty typical pattern, TensorFlow provides an easy way to run the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define expressions\n",
    "objective = 0.5 * tf.matmul(tf.matmul(tf.transpose(x), A), x) + tf.matmul(tf.transpose(b), x) + 0\n",
    "optimize_op = tf.train.GradientDescentOptimizer(0.01).minimize(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start at some random point again\n",
    "sess.run(x.assign(np.random.rand(2,1)))\n",
    "# optimize\n",
    "for _ in range(300):\n",
    "    sess.run(optimize_op)\n",
    "print( sess.run(objective))\n",
    "\n",
    "print( \"xstar=\", sess.run(x))\n",
    "print( \"fstar(x)=\", sess.run(objective))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, basically, 2 lines (define objective + get optimization step) is enough to solve such an optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is such a kind of ugly objective function (Rosenbrock function), which is supposed to be a nightmare for numerical optimization:\n",
    "\n",
    "$$f(x,y)= (1-x)^2 + 100 (y-x^2)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow ships several heuristics to adapt this $η$ and guessing an optimal step with for each update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def rosenbrock(x, y):\n",
    "    a, b = 1., 100.\n",
    "    f = (a - x)**2 + b *(y - x**2 )**2\n",
    "    x_solution = (a, a*a)\n",
    "    return  f, x_solution\n",
    "\n",
    "# just for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(-1.3, 1.3, 31), np.linspace(-0.9, 1.7, 31))\n",
    "zz, solution = rosenbrock(xx, yy)\n",
    "\n",
    "# destroy previous session and graph and create new session\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x0 = (-0.5, 0.9)\n",
    "\n",
    "x = tf.Variable(0, dtype=tf.float64, name=\"x\")\n",
    "y = tf.Variable(0, dtype=tf.float64, name=\"y\")\n",
    "objective, _ = rosenbrock(x,y)\n",
    "\n",
    "optimizer = []\n",
    "optimizer.append(tf.train.RMSPropOptimizer(0.02).minimize(objective))\n",
    "optimizer.append(tf.train.GradientDescentOptimizer(0.002).minimize(objective))\n",
    "optimizer.append(tf.train.AdamOptimizer(0.3).minimize(objective))\n",
    "optimizer.append(tf.train.MomentumOptimizer(0.002, 0.9).minimize(objective))\n",
    "optimizer.append(tf.train.AdadeltaOptimizer(0.1).minimize(objective))\n",
    "optimizer.append(tf.train.AdagradOptimizer(0.1).minimize(objective))\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.contourf(xx, yy, zz, np.logspace(-5, 3, 60), cmap=\"YlGn_r\");\n",
    "for opt_op in optimizer:\n",
    "    steps = [x0]\n",
    "    sess.run([x.assign(x0[0]), y.assign(x0[1])])\n",
    "    for i in range(100): # change epochs to get close to solution\n",
    "        sess.run(opt_op)\n",
    "        steps.append(sess.run([x, y]))\n",
    "\n",
    "    steps = np.array(steps)\n",
    "    ax.plot(steps[:,0], steps[:,1])\n",
    "\n",
    "ax.plot((x0[0]), (x0[1]), 'o', color='y', label = \"Starting Point\")\n",
    "ax.plot(solution[0], solution[1], 'o', color='r', label = \"Solution Point\");\n",
    "ax.legend(['GradDesc', 'RMSprop', 'Adam', 'Momentum', 'AdaDelta', 'AdaGrad'],\n",
    "          bbox_to_anchor=(1.4, 0.7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, they behave differently. Furthermore, it is totally up to your problem, which optimizer work best. As a rule-of-thumb: The tf.train.AdamOptimizer seems to have nice results on many practical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# destroy previous session and graph and create new session\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "#exercise611\n",
    "def f(x, y):\n",
    "    \n",
    "    f = 2*x**2 - 2*x*y +y**2 +2*x-2*y\n",
    "   \n",
    "    return  f \n",
    "x = tf.Variable(0, dtype=tf.float64, name=\"x\")\n",
    "y = tf.Variable(0, dtype=tf.float64, name=\"y\")\n",
    "\n",
    "objective = 2*x**2 - 2*x*y +y**2 +2*x-2*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Gradient from tensorflow to solve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using FOC to solve it\n",
    "# let tensorflow do the gradients\n",
    "xgrad = tf.gradients(objective, x)[0]   # get gradient from objective wrt. to x\n",
    "ygrad = tf.gradients(objective, y)[0]   # get gradient from objective wrt. to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Straightforward gradient descent with 0.1 learning rate\n",
    "x_optimize_op = x.assign(x - 0.1 * xgrad)\n",
    "y_optimize_op = y.assign(y - 0.1 * ygrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start at some random point again\n",
    "sess.run(x.assign(0))\n",
    "sess.run(y.assign(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize\n",
    "for _ in range(1000):\n",
    "    sess.run(x_optimize_op)\n",
    "    sess.run(y_optimize_op)\n",
    "print( \"x=\", sess.run(x))\n",
    "print( \"y=\", sess.run(y))\n",
    "print( \"f(x,y)=\", sess.run(objective))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now using Tensorflow to deal with gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#exercise611\n",
    "def f(x, y):\n",
    "    \n",
    "    f = 2*x**2 - 2*x*y +y**2 +2*x-2*y\n",
    "   \n",
    "    return  f \n",
    "\n",
    "solution = [0,1]\n",
    "#x_solution\n",
    "\n",
    "# just for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(-1.3, 1.3, 31), np.linspace(-0.9, 1.7, 31))\n",
    "#zz = 2*xx**2 - 2*xx*yy +yy**2 +2*xx-2*yy\n",
    "zz = f(xx,yy)\n",
    "# destroy previous session and graph and create new session\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x0 = (-0.5, 0.9)\n",
    "\n",
    "x = tf.Variable(0, dtype=tf.float64, name=\"x\")\n",
    "y = tf.Variable(0, dtype=tf.float64, name=\"y\")\n",
    "objective_value = f(x,y)\n",
    "\n",
    "optimizer = []\n",
    "optimizer.append(tf.train.RMSPropOptimizer(0.02).minimize(objective_value))\n",
    "optimizer.append(tf.train.GradientDescentOptimizer(0.002).minimize(objective_value))\n",
    "optimizer.append(tf.train.AdamOptimizer(0.3).minimize(objective_value))\n",
    "optimizer.append(tf.train.MomentumOptimizer(0.002, 0.9).minimize(objective_value))\n",
    "optimizer.append(tf.train.AdadeltaOptimizer(0.1).minimize(objective_value))\n",
    "optimizer.append(tf.train.AdagradOptimizer(0.1).minimize(objective_value))\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#ax.contourf(xx, yy, zz, np.logspace(-5, 3, 60), cmap=\"YlGn_r\");\n",
    "ax.contourf(xx, yy, zz,64, alpha=.75, cmap=\"YlGn_r\");\n",
    "ax.set_xlim(-1,0.5)\n",
    "ax.set_ylim(0,1.5)\n",
    "for opt_op in optimizer:\n",
    "    steps = [x0]\n",
    "    sess.run([x.assign(x0[0]), y.assign(x0[1])])\n",
    "    for i in range(100):\n",
    "        sess.run(opt_op)\n",
    "        steps.append(sess.run([x, y]))\n",
    "\n",
    "    steps = np.array(steps)\n",
    "    ax.plot(steps[:,0], steps[:,1])\n",
    "\n",
    "ax.plot((x0[0]), (x0[1]), 'o', color='y')\n",
    "ax.plot(solution[0], solution[1], 'o', color='r');\n",
    "ax.legend(['GradDesc', 'RMSprop', 'Adam', 'Momentum', 'AdaDelta', 'AdaGrad'],\n",
    "          bbox_to_anchor=(1.4, 0.7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
